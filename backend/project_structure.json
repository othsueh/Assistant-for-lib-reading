{
  "directory": "",
  "path": "ellama_codebase/",
  "files": [
    {
      "file_name": "ellama_codebase/eval_emotion.py",
      "type": "python",
      "imports": [
        "import csv",
        "import os",
        "import re",
        "import json",
        "import argparse",
        "from collections import defaultdict",
        "import random",
        "import numpy as np",
        "from PIL import Image",
        "from tqdm import tqdm",
        "import torch",
        "from torch.utils.data import DataLoader",
        "from minigpt4.common.config import Config",
        "from minigpt4.common.eval_utils import prepare_texts, init_model, eval_parser, computeIoU",
        "from minigpt4.conversation.conversation import CONV_VISION_minigptv2",
        "from minigpt4.common.registry import registry",
        "from minigpt4.datasets.datasets.first_face import FeatureFaceDataset",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix",
        "from minigpt4.datasets.data_utils import prepare_sample"
      ],
      "classes": [],
      "other_functions": [
        {
          "name": "list_of_str",
          "decorators": [],
          "body": "return list(map(str, arg.split(',')))"
        }
      ]
    },
    {
      "file_name": "ellama_codebase/app.py",
      "type": "python",
      "imports": [
        "import argparse",
        "import os",
        "import random",
        "from collections import defaultdict",
        "import cv2",
        "import re",
        "import numpy as np",
        "from PIL import Image",
        "import torch",
        "import html",
        "import gradio as gr",
        "import torchvision.transforms as T",
        "import torch.backends.cudnn as cudnn",
        "from minigpt4.common.config import Config",
        "from minigpt4.common.registry import registry",
        "from minigpt4.conversation.conversation import Conversation, SeparatorStyle, Chat",
        "from minigpt4.datasets.builders import *",
        "from minigpt4.models import *",
        "from minigpt4.processors import *",
        "from minigpt4.runners import *",
        "from minigpt4.tasks import *"
      ],
      "classes": [],
      "other_functions": [
        {
          "name": "parse_args",
          "decorators": [],
          "body": "parser = argparse.ArgumentParser(description='Demo')\nparser.add_argument('--cfg-path', default='eval_configs/demo.yaml', help='path to configuration file.')\nparser.add_argument('--options', nargs='+', help='override some settings in the used config, the key-value pair in xxx=yyy format will be merged into config file (deprecate), change to --cfg-options instead.')\nargs = parser.parse_args()\nreturn args"
        },
        {
          "name": "extract_substrings",
          "decorators": [],
          "body": "index = string.rfind('}')\nif index != -1:\n    string = string[:index + 1]\npattern = '<p>(.*?)\\\\}(?!<)'\nmatches = re.findall(pattern, string)\nsubstrings = [match for match in matches]\nreturn substrings"
        },
        {
          "name": "is_overlapping",
          "decorators": [],
          "body": "(x1, y1, x2, y2) = rect1\n(x3, y3, x4, y4) = rect2\nreturn not (x2 < x3 or x1 > x4 or y2 < y3 or (y1 > y4))"
        },
        {
          "name": "computeIoU",
          "decorators": [],
          "body": "(x1, y1, x2, y2) = bbox1\n(x3, y3, x4, y4) = bbox2\nintersection_x1 = max(x1, x3)\nintersection_y1 = max(y1, y3)\nintersection_x2 = min(x2, x4)\nintersection_y2 = min(y2, y4)\nintersection_area = max(0, intersection_x2 - intersection_x1 + 1) * max(0, intersection_y2 - intersection_y1 + 1)\nbbox1_area = (x2 - x1 + 1) * (y2 - y1 + 1)\nbbox2_area = (x4 - x3 + 1) * (y4 - y3 + 1)\nunion_area = bbox1_area + bbox2_area - intersection_area\niou = intersection_area / union_area\nreturn iou"
        },
        {
          "name": "save_tmp_img",
          "decorators": [],
          "body": "file_name = ''.join([str(random.randint(0, 9)) for _ in range(5)]) + '.jpg'\nfile_path = '/tmp/gradio' + file_name\nvisual_img.save(file_path)\nreturn file_path"
        },
        {
          "name": "mask2bbox",
          "decorators": [],
          "body": "if mask is None:\n    return ''\nmask = mask.resize([100, 100], resample=Image.NEAREST)\nmask = np.array(mask)[:, :, 0]\nrows = np.any(mask, axis=1)\ncols = np.any(mask, axis=0)\nif rows.sum():\n    (rmin, rmax) = np.where(rows)[0][[0, -1]]\n    (cmin, cmax) = np.where(cols)[0][[0, -1]]\n    bbox = '{{<{}><{}><{}><{}>}}'.format(cmin, rmin, cmax, rmax)\nelse:\n    bbox = ''\nreturn bbox"
        },
        {
          "name": "escape_markdown",
          "decorators": [],
          "body": "md_chars = ['<', '>']\nfor char in md_chars:\n    text = text.replace(char, '\\\\' + char)\nreturn text"
        },
        {
          "name": "reverse_escape",
          "decorators": [],
          "body": "md_chars = ['\\\\<', '\\\\>']\nfor char in md_chars:\n    text = text.replace(char, char[1:])\nreturn text"
        },
        {
          "name": "get_first_frame",
          "decorators": [],
          "body": "cap = cv2.VideoCapture(video_path)\nif not cap.isOpened():\n    print('Error: Cannot open video.')\n    return None\n(ret, frame) = cap.read()\ncap.release()\nif ret:\n    return frame\nelse:\n    print('Error: Cannot read frame from video.')\n    return None"
        },
        {
          "name": "visualize_all_bbox_together",
          "decorators": [],
          "body": "if image is None:\n    return (None, '')\nif isinstance(image, str):\n    raw_image = get_first_frame(image)\n    frame_rgb = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n    image = Image.fromarray(frame_rgb)\ngeneration = html.unescape(generation)\n(image_width, image_height) = image.size\nimage = image.resize([500, int(500 / image_width * image_height)])\n(image_width, image_height) = image.size\nstring_list = extract_substrings(generation)\nif string_list:\n    mode = 'all'\n    entities = defaultdict(list)\n    i = 0\n    j = 0\n    for string in string_list:\n        try:\n            (obj, string) = string.split('</p>')\n        except ValueError:\n            print('wrong string: ', string)\n            continue\n        bbox_list = string.split('<delim>')\n        flag = False\n        for bbox_string in bbox_list:\n            integers = re.findall('-?\\\\d+', bbox_string)\n            if len(integers) == 4:\n                (x0, y0, x1, y1) = (int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3]))\n                left = x0 / bounding_box_size * image_width\n                bottom = y0 / bounding_box_size * image_height\n                right = x1 / bounding_box_size * image_width\n                top = y1 / bounding_box_size * image_height\n                entities[obj].append([left, bottom, right, top])\n                j += 1\n                flag = True\n        if flag:\n            i += 1\nelse:\n    integers = re.findall('-?\\\\d+', generation)\n    if len(integers) == 4:\n        mode = 'single'\n        entities = list()\n        (x0, y0, x1, y1) = (int(integers[0]), int(integers[1]), int(integers[2]), int(integers[3]))\n        left = x0 / bounding_box_size * image_width\n        bottom = y0 / bounding_box_size * image_height\n        right = x1 / bounding_box_size * image_width\n        top = y1 / bounding_box_size * image_height\n        entities.append([left, bottom, right, top])\n    else:\n        return (None, '')\nif len(entities) == 0:\n    return (None, '')\nif isinstance(image, Image.Image):\n    image_h = image.height\n    image_w = image.width\n    image = np.array(image)\nelif isinstance(image, str):\n    if os.path.exists(image):\n        pil_img = Image.open(image).convert('RGB')\n        image = np.array(pil_img)[:, :, [2, 1, 0]]\n        image_h = pil_img.height\n        image_w = pil_img.width\n    else:\n        raise ValueError(f'invaild image path, {image}')\nelif isinstance(image, torch.Tensor):\n    image_tensor = image.cpu()\n    reverse_norm_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])[:, None, None]\n    reverse_norm_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])[:, None, None]\n    image_tensor = image_tensor * reverse_norm_std + reverse_norm_mean\n    pil_img = T.ToPILImage()(image_tensor)\n    image_h = pil_img.height\n    image_w = pil_img.width\n    image = np.array(pil_img)[:, :, [2, 1, 0]]\nelse:\n    raise ValueError(f'invaild image format, {type(image)} for {image}')\nindices = list(range(len(entities)))\nnew_image = image.copy()\nprevious_bboxes = []\ntext_size = 0.5\ntext_line = 1\nbox_line = 2\n((c_width, text_height), _) = cv2.getTextSize('F', cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\nbase_height = int(text_height * 0.675)\ntext_offset_original = text_height - base_height\ntext_spaces = 2\nused_colors = colors\ncolor_id = -1\nfor (entity_idx, entity_name) in enumerate(entities):\n    if mode == 'single' or mode == 'identify':\n        bboxes = entity_name\n        bboxes = [bboxes]\n    else:\n        bboxes = entities[entity_name]\n    color_id += 1\n    for (bbox_id, (x1_norm, y1_norm, x2_norm, y2_norm)) in enumerate(bboxes):\n        skip_flag = False\n        (orig_x1, orig_y1, orig_x2, orig_y2) = (int(x1_norm), int(y1_norm), int(x2_norm), int(y2_norm))\n        color = used_colors[entity_idx % len(used_colors)]\n        new_image = cv2.rectangle(new_image, (orig_x1, orig_y1), (orig_x2, orig_y2), color, box_line)\n        if mode == 'all':\n            (l_o, r_o) = (box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1)\n            x1 = orig_x1 - l_o\n            y1 = orig_y1 - l_o\n            if y1 < text_height + text_offset_original + 2 * text_spaces:\n                y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces\n                x1 = orig_x1 + r_o\n            ((text_width, text_height), _) = cv2.getTextSize(f'  {entity_name}', cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n            (text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2) = (x1, y1 - (text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1)\n            for prev_bbox in previous_bboxes:\n                if computeIoU((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox['bbox']) > 0.95 and prev_bbox['phrase'] == entity_name:\n                    skip_flag = True\n                    break\n                while is_overlapping((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox['bbox']):\n                    text_bg_y1 += text_height + text_offset_original + 2 * text_spaces\n                    text_bg_y2 += text_height + text_offset_original + 2 * text_spaces\n                    y1 += text_height + text_offset_original + 2 * text_spaces\n                    if text_bg_y2 >= image_h:\n                        text_bg_y1 = max(0, image_h - (text_height + text_offset_original + 2 * text_spaces))\n                        text_bg_y2 = image_h\n                        y1 = image_h\n                        break\n            if not skip_flag:\n                alpha = 0.5\n                for i in range(text_bg_y1, text_bg_y2):\n                    for j in range(text_bg_x1, text_bg_x2):\n                        if i < image_h and j < image_w:\n                            if j < text_bg_x1 + 1.35 * c_width:\n                                bg_color = color\n                            else:\n                                bg_color = [255, 255, 255]\n                            new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(np.uint8)\n                cv2.putText(new_image, f'  {entity_name}', (x1, y1 - text_offset_original - 1 * text_spaces), cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA)\n                previous_bboxes.append({'bbox': (text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), 'phrase': entity_name})\nif mode == 'all':\n\n    def color_iterator(colors):\n        while True:\n            for color in colors:\n                yield color\n    color_gen = color_iterator(colors)\n\n    def colored_phrases(match):\n        phrase = match.group(1)\n        color = next(color_gen)\n        return f'<span style=\"color:rgb{color}\">{phrase}</span>'\n    generation = re.sub('{<\\\\d+><\\\\d+><\\\\d+><\\\\d+>}|<delim>', '', generation)\n    generation_colored = re.sub('<p>(.*?)</p>', colored_phrases, generation)\nelse:\n    generation_colored = ''\npil_image = Image.fromarray(new_image)\nreturn (pil_image, generation_colored)"
        },
        {
          "name": "gradio_reset",
          "decorators": [],
          "body": "if chat_state is not None:\n    chat_state.messages = []\nif img_list is not None:\n    img_list = []\nreturn (None, gr.update(value=None, interactive=True), gr.update(placeholder='Upload your image and chat', interactive=True), chat_state, img_list)"
        },
        {
          "name": "image_upload_trigger",
          "decorators": [],
          "body": "upload_flag = 1\nif img_list:\n    replace_flag = 1\nreturn (upload_flag, replace_flag)"
        },
        {
          "name": "example_trigger",
          "decorators": [],
          "body": "upload_flag = 1\nif img_list or replace_flag == 1:\n    replace_flag = 1\nreturn (upload_flag, replace_flag)"
        },
        {
          "name": "gradio_ask",
          "decorators": [],
          "body": "print('+++gradio_ask+++')\nif len(user_message) == 0:\n    text_box_show = 'Input should not be empty!'\nelse:\n    text_box_show = ''\nprint('user_message:', user_message)\nprint('chatbot:', chatbot)\nprint('chat_state:', chat_state)\nif isinstance(gr_img, dict):\n    (gr_img, mask) = (gr_img['image'], gr_img['mask'])\nelse:\n    mask = None\nif '[identify]' in user_message:\n    integers = re.findall('-?\\\\d+', user_message)\n    if len(integers) != 4:\n        bbox = mask2bbox(mask)\n        user_message = user_message + bbox\nif chat_state is None:\n    chat_state = CONV_VISION.copy()\nif upload_flag:\n    if replace_flag:\n        chat_state = CONV_VISION.copy()\n        replace_flag = 0\n        chatbot = []\n    img_list = []\n    llm_message = chat.upload_img(gr_img, chat_state, img_list)\n    upload_flag = 0\nchat.ask(user_message, chat_state)\nprint('user_message: ', user_message)\nprint('chat_state: ', chat_state)\nchatbot = chatbot + [[user_message, None]]\nif '[identify]' in user_message:\n    (visual_img, _) = visualize_all_bbox_together(gr_img, user_message)\n    if visual_img is not None:\n        file_path = save_tmp_img(visual_img)\n        chatbot = chatbot + [[(file_path,), None]]\nreturn (text_box_show, chatbot, chat_state, img_list, upload_flag, replace_flag)"
        },
        {
          "name": "gradio_answer",
          "decorators": [],
          "body": "print('--gradio_answer--')\nllm_message = chat.answer(conv=chat_state, img_list=img_list, temperature=temperature, max_new_tokens=500, max_length=2000)[0]\nchatbot[-1][1] = llm_message\nprint('gradio_answer: ', llm_message)\nreturn (chatbot, chat_state)"
        },
        {
          "name": "process_english_text",
          "decorators": [],
          "body": "if len(text) < 2:\n    return text\ntext = text[0].upper() + text[1:]\nsentences = text.split('. ')\ncorrected_sentences = [s.capitalize() for s in sentences]\ntext = '. '.join(corrected_sentences)\nif text.endswith(','):\n    text = text[:-1]\nif not text.endswith('.'):\n    text += '.'\nreturn text"
        },
        {
          "name": "gradio_stream_answer",
          "decorators": [],
          "body": "print('---gradio_stream_answer---')\nif len(img_list) > 0:\n    if not isinstance(img_list[0], torch.Tensor):\n        chat.encode_img(img_list)\nprint(chat)\nstreamer = chat.stream_answer(conv=chat_state, img_list=img_list, temperature=temperature, max_new_tokens=500, max_length=2000)\noutput = ''\nprint('streamer:', streamer)\nfor new_output in streamer:\n    escapped = escape_markdown(new_output)\n    output += escapped\n    chatbot[-1][1] = output\n    chatbot[-1][1] = process_english_text(chatbot[-1][1])\n    yield (chatbot, chat_state)\nchat_state.messages[-1][1] = '</s>'\nprint('output:', output)\nreturn (chatbot, chat_state)"
        },
        {
          "name": "gradio_visualize",
          "decorators": [],
          "body": "if isinstance(gr_img, dict):\n    (gr_img, mask) = (gr_img['image'], gr_img['mask'])\nunescaped = reverse_escape(chatbot[-1][1])\n(visual_img, generation_color) = visualize_all_bbox_together(gr_img, unescaped)\nif visual_img is not None:\n    if len(generation_color):\n        chatbot[-1][1] = generation_color\n    file_path = save_tmp_img(visual_img)\n    chatbot = chatbot + [[None, (file_path,)]]\nreturn chatbot"
        },
        {
          "name": "gradio_taskselect",
          "decorators": [],
          "body": "prompt_list = ['', '[reason] ', '[emotion] ', '[visual] ', '[audio] ']\ninstruct_list = ['**Hint:** Type in whatever you want', '**Hint:** Send the command to multimodal emotion reasoning', '**Hint:** Send the command to multimodal emotion recognition', '**Hint:** Send the command to generate visual description', '**Hint:** Send the command to generate audio description']\nreturn (prompt_list[idx], instruct_list[idx])"
        }
      ]
    },
    {
      "file_name": "ellama_codebase/train.py",
      "type": "python",
      "imports": [
        "import argparse",
        "import os",
        "import random",
        "import numpy as np",
        "import torch",
        "import torch.backends.cudnn as cudnn",
        "import wandb",
        "import minigpt4.tasks as tasks",
        "from minigpt4.common.config import Config",
        "from minigpt4.common.dist_utils import get_rank, init_distributed_mode",
        "from minigpt4.common.logger import setup_logger",
        "from minigpt4.common.optims import LinearWarmupCosineLRScheduler, LinearWarmupStepLRScheduler",
        "from minigpt4.common.registry import registry",
        "from minigpt4.common.utils import now",
        "from minigpt4.datasets.builders import *",
        "from minigpt4.models import *",
        "from minigpt4.processors import *",
        "from minigpt4.runners import *",
        "from minigpt4.tasks import *"
      ],
      "classes": [],
      "other_functions": [
        {
          "name": "parse_args",
          "decorators": [],
          "body": "parser = argparse.ArgumentParser(description='Training')\nparser.add_argument('--cfg-path', required=True, help='path to configuration file.')\nparser.add_argument('--options', nargs='+', help='override some settings in the used config, the key-value pair in xxx=yyy format will be merged into config file (deprecate), change to --cfg-options instead.')\nargs = parser.parse_args()\nreturn args"
        },
        {
          "name": "setup_seeds",
          "decorators": [],
          "body": "seed = config.run_cfg.seed + get_rank()\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ncudnn.benchmark = False\ncudnn.deterministic = True"
        },
        {
          "name": "get_runner_class",
          "decorators": [],
          "body": "'\\n    Get runner class from config. Default to epoch-based runner.\\n    '\nrunner_cls = registry.get_runner_class(cfg.run_cfg.get('runner', 'runner_base'))\nreturn runner_cls"
        },
        {
          "name": "main",
          "decorators": [],
          "body": "job_id = now()\nargs = parse_args()\ncfg = Config(args)\ninit_distributed_mode(cfg.run_cfg)\nsetup_seeds(cfg)\nsetup_logger()\ncfg.pretty_print()\ntask = tasks.setup_task(cfg)\ndatasets = task.build_datasets(cfg)\nmodel = task.build_model(cfg)\nrunner = get_runner_class(cfg)(cfg=cfg, job_id=job_id, task=task, model=model, datasets=datasets)\nrunner.train()"
        }
      ]
    }
  ],
  "subdirs": [
    {
      "directory": "eval_configs",
      "path": "ellama_codebase/eval_configs",
      "files": [
        {
          "file_name": "ellama_codebase/eval_configs/eval_emotion.yaml",
          "type": "yaml",
          "content": {
            "model": {
              "arch": "minigpt_v2",
              "model_type": "pretrain",
              "max_txt_len": 500,
              "end_sym": "</s>",
              "low_resource": false,
              "prompt_template": "[INST] {} [/INST]",
              "llama_model": "/home/user/project/Emotion-LLaMA/checkpoints/Llama-2-7b-chat-hf",
              "ckpt": "/home/user/project/Emotion-LLaMA/checkpoints/save_checkpoint/stage2/checkpoint_best.pth",
              "lora_r": 64,
              "lora_alpha": 16
            },
            "datasets": {
              "feature_face_caption": {
                "batch_size": 1,
                "vis_processor": {
                  "train": {
                    "name": "blip2_image_train",
                    "image_size": 448
                  }
                },
                "text_processor": {
                  "train": {
                    "name": "blip_caption"
                  }
                },
                "sample_ratio": 30
              }
            },
            "evaluation_datasets": {
              "feature_face_caption": {
                "eval_file_path": "/home/user/selected_face/face_emotion/relative_test3_NCEV.txt",
                "img_path": "/home/user/selected_face/first_face/first_frames",
                "max_new_tokens": 500,
                "batch_size": 1
              }
            },
            "run": {
              "task": "image_text_pretrain",
              "name": "minigptv2_evaluation",
              "save_path": "/home/user/project/Emotion-LLaMA/results"
            }
          }
        },
        {
          "file_name": "ellama_codebase/eval_configs/demo.yaml",
          "type": "yaml",
          "content": {
            "model": {
              "arch": "minigpt_v2",
              "model_type": "pretrain",
              "max_txt_len": 500,
              "end_sym": "</s>",
              "low_resource": true,
              "prompt_template": "[INST] {} [/INST]",
              "ckpt": "checkpoints/save_checkpoint/Emoation_LLaMA.pth",
              "lora_r": 64,
              "lora_alpha": 16
            },
            "datasets": {
              "feature_face_caption": {
                "vis_processor": {
                  "train": {
                    "name": "blip2_image_eval",
                    "image_size": 448
                  }
                },
                "text_processor": {
                  "train": {
                    "name": "blip_caption"
                  }
                }
              }
            },
            "run": {
              "task": "image_text_pretrain"
            }
          }
        }
      ],
      "subdirs": []
    },
    {
      "directory": "train_configs",
      "path": "ellama_codebase/train_configs",
      "files": [
        {
          "file_name": "ellama_codebase/train_configs/Emotion-LLaMA_finetune.yaml",
          "type": "yaml",
          "content": {
            "model": {
              "arch": "minigpt_v2",
              "model_type": "pretrain",
              "max_txt_len": 1024,
              "image_size": 448,
              "end_sym": "</s>",
              "llama_model": "/home/user/project/Emotion-LLaMA/checkpoints/Llama-2-7b-chat-hf",
              "ckpt": "/home/user/project/Emotion-LLaMA/checkpoints/minigptv2_checkpoint.pth",
              "use_grad_checkpoint": true,
              "chat_template": true,
              "lora_r": 64,
              "lora_alpha": 16
            },
            "datasets": {
              "feature_face_caption": {
                "batch_size": 1,
                "vis_processor": {
                  "train": {
                    "name": "blip2_image_train",
                    "image_size": 448
                  }
                },
                "text_processor": {
                  "train": {
                    "name": "blip_caption"
                  }
                },
                "sample_ratio": 30
              }
            },
            "run": {
              "task": "image_text_pretrain",
              "lr_sched": "linear_warmup_cosine_lr",
              "init_lr": "1e-5",
              "min_lr": "1e-6",
              "warmup_lr": "1e-6",
              "weight_decay": 0.05,
              "max_epoch": 30,
              "num_workers": 6,
              "iters_per_epoch": 1000,
              "warmup_steps": 1000,
              "seed": 42,
              "output_dir": "/home/user/project/Emotion-LLaMA/checkpoints/save_checkpoint",
              "amp": true,
              "resume_ckpt_path": null,
              "evaluate": false,
              "train_splits": [
                "train"
              ],
              "device": "cuda",
              "world_size": 2,
              "dist_url": "env://",
              "distributed": true,
              "wandb_log": false,
              "job_name": "minigptv2_finetune"
            }
          }
        }
      ],
      "subdirs": []
    },
    {
      "directory": "minigpt4",
      "path": "ellama_codebase/minigpt4",
      "files": [
        {
          "file_name": "ellama_codebase/minigpt4/__init__.py",
          "type": "python",
          "imports": [
            "import os",
            "import sys",
            "from omegaconf import OmegaConf",
            "from minigpt4.common.registry import registry",
            "from minigpt4.datasets.builders import *",
            "from minigpt4.models import *",
            "from minigpt4.processors import *",
            "from minigpt4.tasks import *"
          ],
          "classes": [],
          "other_functions": []
        }
      ],
      "subdirs": [
        {
          "directory": "conversation",
          "path": "ellama_codebase/minigpt4/conversation",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/conversation/conversation.py",
              "type": "python",
              "imports": [
                "import argparse",
                "import os",
                "import time",
                "from threading import Thread",
                "from PIL import Image",
                "import cv2",
                "from moviepy.editor import VideoFileClip",
                "import soundfile as sf",
                "import torch",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer",
                "from transformers import StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer",
                "from transformers import Wav2Vec2FeatureExtractor",
                "import dataclasses",
                "from enum import auto, Enum",
                "from typing import List, Tuple, Any",
                "from minigpt4.common.registry import registry",
                "from transformers import HubertModel"
              ],
              "classes": [
                {
                  "name": "SeparatorStyle",
                  "decorators": [],
                  "inherits": [
                    "Enum"
                  ],
                  "methods": []
                },
                {
                  "name": "Conversation",
                  "decorators": [
                    "dataclasses.dataclass"
                  ],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "get_prompt",
                      "decorators": [],
                      "body": "if self.sep_style == SeparatorStyle.SINGLE:\n    ret = self.system + self.sep\n    for (role, message) in self.messages:\n        if message:\n            ret += role + message + self.sep\n        else:\n            ret += role\n    return ret\nelif self.sep_style == SeparatorStyle.TWO:\n    seps = [self.sep, self.sep2]\n    ret = self.system + seps[0]\n    for (i, (role, message)) in enumerate(self.messages):\n        if message:\n            ret += role + message + seps[i % 2]\n        else:\n            ret += role\n    return ret\nelse:\n    raise ValueError(f'Invalid style: {self.sep_style}')"
                    },
                    {
                      "name": "append_message",
                      "decorators": [],
                      "body": "self.messages.append([role, message])"
                    },
                    {
                      "name": "to_gradio_chatbot",
                      "decorators": [],
                      "body": "ret = []\nfor (i, (role, msg)) in enumerate(self.messages[self.offset:]):\n    if i % 2 == 0:\n        ret.append([msg, None])\n    else:\n        ret[-1][-1] = msg\nreturn ret"
                    },
                    {
                      "name": "copy",
                      "decorators": [],
                      "body": "return Conversation(system=self.system, roles=self.roles, messages=[[x, y] for (x, y) in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, conv_id=self.conv_id)"
                    },
                    {
                      "name": "dict",
                      "decorators": [],
                      "body": "return {'system': self.system, 'roles': self.roles, 'messages': self.messages, 'offset': self.offset, 'sep': self.sep, 'sep2': self.sep2, 'conv_id': self.conv_id}"
                    }
                  ]
                },
                {
                  "name": "StoppingCriteriaSub",
                  "decorators": [],
                  "inherits": [
                    "StoppingCriteria"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.stops = stops"
                    },
                    {
                      "name": "__call__",
                      "decorators": [],
                      "body": "for stop in self.stops:\n    if torch.all(input_ids[:, -len(stop):] == stop).item():\n        return True\nreturn False"
                    }
                  ]
                },
                {
                  "name": "Chat",
                  "decorators": [],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.device = device\nself.model = model\nself.vis_processor = vis_processor\nif stopping_criteria is not None:\n    self.stopping_criteria = stopping_criteria\nelse:\n    stop_words_ids = [torch.tensor([2]).to(self.device)]\n    self.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
                    },
                    {
                      "name": "ask",
                      "decorators": [],
                      "body": "if len(conv.messages) > 0 and conv.messages[-1][0] == conv.roles[0] and (conv.messages[-1][1][-6:] == '</Img>'):\n    conv.messages[-1][1] = ' '.join([conv.messages[-1][1], text])\nelse:\n    conv.append_message(conv.roles[0], text)"
                    },
                    {
                      "name": "answer_prepare",
                      "decorators": [],
                      "body": "conv.append_message(conv.roles[1], None)\nprompt = conv.get_prompt()\nprint('prompt:', prompt)\nembs = self.model.get_context_emb(prompt, img_list)\ncurrent_max_len = embs.shape[1] + max_new_tokens\nif current_max_len - max_length > 0:\n    print('Warning: The number of tokens in current conversation exceeds the max length. The model will not see the contexts outside the range.')\nbegin_idx = max(0, current_max_len - max_length)\nembs = embs[:, begin_idx:]\ngeneration_kwargs = dict(inputs_embeds=embs, max_new_tokens=max_new_tokens, stopping_criteria=self.stopping_criteria, num_beams=num_beams, do_sample=True, min_length=min_length, top_p=top_p, repetition_penalty=repetition_penalty, length_penalty=length_penalty, temperature=float(temperature))\nreturn generation_kwargs"
                    },
                    {
                      "name": "answer",
                      "decorators": [],
                      "body": "generation_dict = self.answer_prepare(conv, img_list, **kargs)\noutput_token = self.model_generate(**generation_dict)[0]\noutput_text = self.model.llama_tokenizer.decode(output_token, skip_special_tokens=True)\noutput_text = output_text.split('###')[0]\noutput_text = output_text.split('Assistant:')[-1].strip()\nconv.messages[-1][1] = output_text\nreturn (output_text, output_token.cpu().numpy())"
                    },
                    {
                      "name": "stream_answer",
                      "decorators": [],
                      "body": "print('stream_answer img shape: ', img_list[0].shape)\ngeneration_kwargs = self.answer_prepare(conv, img_list, **kargs)\nstreamer = TextIteratorStreamer(self.model.llama_tokenizer, skip_special_tokens=True)\ngeneration_kwargs['streamer'] = streamer\nthread = Thread(target=self.model_generate, kwargs=generation_kwargs)\nthread.start()\nreturn streamer"
                    },
                    {
                      "name": "model_generate",
                      "decorators": [],
                      "body": "with self.model.maybe_autocast():\n    output = self.model.llama_model.generate(*args, **kwargs)\nreturn output"
                    },
                    {
                      "name": "encode_img",
                      "decorators": [],
                      "body": "image = img_list[0]\nimg_list.pop(0)\nif isinstance(image, str):\n    print('isinstance str')\n    video_path = image\n    raw_image = get_first_frame(video_path)\n    frame_rgb = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)\n    pil_image = Image.fromarray(frame_rgb)\n    image = self.vis_processor(pil_image).unsqueeze(0).to(self.device)\n    (samples, sr) = extract_audio_from_video(video_path)\n    model_file = 'checkpoints/transformer/chinese-hubert-large'\n    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_file)\n    input_values = feature_extractor(samples, sampling_rate=sr, return_tensors='pt').input_values\n    from transformers import HubertModel\n    hubert_model = HubertModel.from_pretrained(model_file)\n    hubert_model.eval()\n    with torch.no_grad():\n        hidden_states = hubert_model(input_values, output_hidden_states=True).hidden_states\n        audio_feature = torch.stack(hidden_states)[[-1]].sum(dim=0)\n        audio_feature = audio_feature[0].detach().unsqueeze(0)\n        audio_feature = torch.mean(audio_feature, dim=1, keepdim=True)\nelif isinstance(image, Image.Image):\n    print('isinstance Image')\n    raw_image = image\n    image = self.vis_processor(raw_image).unsqueeze(0).to(self.device)\nelif isinstance(image, torch.Tensor):\n    print('isinstance Tensor')\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    image = image.to(self.device)\nvideo_features = torch.zeros([1, 2, 1024])\nvideo_features = torch.cat((video_features, audio_feature), dim=1)\nprint('audio faature shape:', audio_feature.shape)\nprint('video_features', video_features.shape)\n(image_emb, _) = self.model.encode_img(image, video_features)\nimg_list.append(image_emb)"
                    },
                    {
                      "name": "upload_img",
                      "decorators": [],
                      "body": "conv.append_message(conv.roles[0], '<video><VideoHere></video> <feature><FeatureHere></feature>')\nimg_list.append(image)\nmsg = 'Received.'\nreturn msg"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "get_first_frame",
                  "decorators": [],
                  "body": "cap = cv2.VideoCapture(video_path)\nif not cap.isOpened():\n    print('Error: Cannot open video.')\n    return None\n(ret, frame) = cap.read()\ncap.release()\nif ret:\n    return frame\nelse:\n    print('Error: Cannot read frame from video.')\n    return None"
                },
                {
                  "name": "extract_audio_from_video",
                  "decorators": [],
                  "body": "video = VideoFileClip(video_path)\naudio = video.audio\naudio_path = 'audio.wav'\naudio.write_audiofile(audio_path, fps=16000, codec='pcm_s16le', ffmpeg_params=['-ac', '1'])\n(samples, sr) = sf.read(audio_path)\nreturn (samples, sr)"
                }
              ]
            }
          ],
          "subdirs": []
        },
        {
          "directory": "tasks",
          "path": "ellama_codebase/minigpt4/tasks",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/tasks/__init__.py",
              "type": "python",
              "imports": [
                "from minigpt4.common.registry import registry",
                "from minigpt4.tasks.base_task import BaseTask",
                "from minigpt4.tasks.image_text_pretrain import ImageTextPretrainTask"
              ],
              "classes": [],
              "other_functions": [
                {
                  "name": "setup_task",
                  "decorators": [],
                  "body": "assert 'task' in cfg.run_cfg, 'Task name must be provided.'\ntask_name = cfg.run_cfg.task\ntask = registry.get_task_class(task_name).setup_task(cfg=cfg)\nassert task is not None, 'Task {} not properly registered.'.format(task_name)\nreturn task"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/tasks/image_text_pretrain.py",
              "type": "python",
              "imports": [
                "from minigpt4.common.registry import registry",
                "from minigpt4.tasks.base_task import BaseTask"
              ],
              "classes": [
                {
                  "name": "ImageTextPretrainTask",
                  "decorators": [
                    "registry.register_task('image_text_pretrain')"
                  ],
                  "inherits": [
                    "BaseTask"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()"
                    },
                    {
                      "name": "evaluation",
                      "decorators": [],
                      "body": "print('-----evaluation----')"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/tasks/base_task.py",
              "type": "python",
              "imports": [
                "import logging",
                "import os",
                "import torch",
                "import torch.distributed as dist",
                "from minigpt4.common.dist_utils import get_rank, get_world_size, is_main_process, is_dist_avail_and_initialized",
                "from minigpt4.common.logger import MetricLogger, SmoothedValue",
                "from minigpt4.common.registry import registry",
                "from minigpt4.datasets.data_utils import prepare_sample",
                "import wandb",
                "import json"
              ],
              "classes": [
                {
                  "name": "BaseTask",
                  "decorators": [],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.inst_id_key = 'instance_id'\nself.cfg = ''"
                    },
                    {
                      "name": "setup_task",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls()"
                    },
                    {
                      "name": "build_model",
                      "decorators": [],
                      "body": "self.cfg = cfg\nmodel_config = cfg.model_cfg\nmodel_cls = registry.get_model_class(model_config.arch)\nreturn model_cls.from_config(model_config)"
                    },
                    {
                      "name": "build_datasets",
                      "decorators": [],
                      "body": "\"\\n        Build a dictionary of datasets, keyed by split 'train', 'valid', 'test'.\\n        Download dataset and annotations automatically if not exist.\\n\\n        Args:\\n            cfg (common.config.Config): _description_\\n\\n        Returns:\\n            dict: Dictionary of torch.utils.data.Dataset objects by split.\\n        \"\ndatasets = dict()\ndatasets_config = cfg.datasets_cfg\nassert len(datasets_config) > 0, 'At least one dataset has to be specified.'\nfor name in datasets_config:\n    dataset_config = datasets_config[name]\n    builder = registry.get_builder_class(name)(dataset_config)\n    dataset = builder.build_datasets()\n    dataset['train'].name = name\n    if 'sample_ratio' in dataset_config:\n        dataset['train'].sample_ratio = dataset_config.sample_ratio\n    datasets[name] = dataset\nreturn datasets"
                    },
                    {
                      "name": "train_step",
                      "decorators": [],
                      "body": "outputs = model(samples)\nloss = outputs['emos_loss']\nreturn loss"
                    },
                    {
                      "name": "valid_step",
                      "decorators": [],
                      "body": "raise NotImplementedError"
                    },
                    {
                      "name": "before_evaluation",
                      "decorators": [],
                      "body": "model.before_evaluation(dataset=dataset, task_type=type(self))"
                    },
                    {
                      "name": "after_evaluation",
                      "decorators": [],
                      "body": "pass"
                    },
                    {
                      "name": "inference_step",
                      "decorators": [],
                      "body": "raise NotImplementedError"
                    },
                    {
                      "name": "evaluation",
                      "decorators": [],
                      "body": "metric_logger = MetricLogger(delimiter='  ')\nheader = 'Evaluation'\nprint_freq = 10\nresults = []\nfor samples in metric_logger.log_every(data_loader, print_freq, header):\n    samples = prepare_sample(samples, cuda_enabled=cuda_enabled)\n    eval_output = self.valid_step(model=model, samples=samples)\n    results.extend(eval_output)\nif is_dist_avail_and_initialized():\n    dist.barrier()\nreturn results"
                    },
                    {
                      "name": "train_epoch",
                      "decorators": [],
                      "body": "return self._train_inner_loop(epoch=epoch, iters_per_epoch=lr_scheduler.iters_per_epoch, model=model, data_loader=data_loader, optimizer=optimizer, scaler=scaler, lr_scheduler=lr_scheduler, log_freq=log_freq, cuda_enabled=cuda_enabled, accum_grad_iters=accum_grad_iters)"
                    },
                    {
                      "name": "train_iters",
                      "decorators": [],
                      "body": "return self._train_inner_loop(epoch=epoch, start_iters=start_iters, iters_per_epoch=iters_per_inner_epoch, model=model, data_loader=data_loader, optimizer=optimizer, scaler=scaler, lr_scheduler=lr_scheduler, log_freq=log_freq, cuda_enabled=cuda_enabled, accum_grad_iters=accum_grad_iters)"
                    },
                    {
                      "name": "_train_inner_loop",
                      "decorators": [],
                      "body": "'\\n        An inner training loop compatible with both epoch-based and iter-based training.\\n\\n        When using epoch-based, training stops after one epoch; when using iter-based,\\n        training stops after #iters_per_epoch iterations.\\n        '\nuse_amp = scaler is not None\nif not hasattr(data_loader, '__next__'):\n    data_loader = iter(data_loader)\nmetric_logger = MetricLogger(delimiter='  ')\nmetric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\nmetric_logger.add_meter('loss', SmoothedValue(window_size=1, fmt='{value:.4f}'))\nlogging.info('Start training epoch {}, {} iters per inner epoch.'.format(epoch, iters_per_epoch))\nheader = 'Train: data epoch: [{}]'.format(epoch)\nif start_iters is None:\n    inner_epoch = epoch\nelse:\n    inner_epoch = start_iters // iters_per_epoch\n    header = header + '; inner epoch [{}]'.format(inner_epoch)\nimage_list = []\ncaption_list = []\nfor i in metric_logger.log_every(range(iters_per_epoch), log_freq, header):\n    if i >= iters_per_epoch:\n        break\n    samples = next(data_loader)\n    image_list.append(samples['image_id'])\n    caption_list.append(samples['answer'])\n    samples = prepare_sample(samples, cuda_enabled=cuda_enabled)\n    samples.update({'epoch': inner_epoch, 'num_iters_per_epoch': iters_per_epoch, 'iters': i})\n    lr_scheduler.step(cur_epoch=inner_epoch, cur_step=i)\n    with torch.cuda.amp.autocast(enabled=use_amp):\n        loss = self.train_step(model=model, samples=samples)\n    if use_amp:\n        scaler.scale(loss).backward()\n    else:\n        loss.backward()\n    if (i + 1) % accum_grad_iters == 0:\n        if use_amp:\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            optimizer.step()\n        optimizer.zero_grad()\n        if self.cfg.run_cfg.wandb_log:\n            wandb.log({'epoch': inner_epoch, 'loss': loss})\n    metric_logger.update(loss=loss.item())\n    metric_logger.update(lr=optimizer.param_groups[0]['lr'])\n    for param_group in optimizer.param_groups:\n        if 'attention' in param_group.get('params', []):\n            print('Attention LR:', param_group['lr'])\nsave_dir = '/home/user/project/Emotion-LLaMA/checkpoints/run_samples'\nsave_to = os.path.join(save_dir, 'epoch_{}.txt'.format(epoch))\nwith open(save_to, 'w') as file:\n    for i in range(len(image_list)):\n        name = image_list[i]\n        caption = caption_list[i]\n        file.write(name[0] + ' ' + caption[0] + '\\n')\nmetric_logger.synchronize_between_processes()\nlogging.info('Averaged stats: ' + str(metric_logger.global_avg()))\nreturn {k: '{:.6f}'.format(meter.global_avg) for (k, meter) in metric_logger.meters.items()}"
                    },
                    {
                      "name": "save_result",
                      "decorators": [
                        "staticmethod"
                      ],
                      "body": "import json\nresult_file = os.path.join(result_dir, '%s_rank%d.json' % (filename, get_rank()))\nfinal_result_file = os.path.join(result_dir, '%s.json' % filename)\njson.dump(result, open(result_file, 'w'))\nif is_dist_avail_and_initialized():\n    dist.barrier()\nif is_main_process():\n    logging.warning('rank %d starts merging results.' % get_rank())\n    result = []\n    for rank in range(get_world_size()):\n        result_file = os.path.join(result_dir, '%s_rank%d.json' % (filename, rank))\n        res = json.load(open(result_file, 'r'))\n        result += res\n    if remove_duplicate:\n        result_new = []\n        id_list = []\n        for res in result:\n            if res[remove_duplicate] not in id_list:\n                id_list.append(res[remove_duplicate])\n                result_new.append(res)\n        result = result_new\n    json.dump(result, open(final_result_file, 'w'))\n    print('result file saved to %s' % final_result_file)\nreturn final_result_file"
                    }
                  ]
                }
              ],
              "other_functions": []
            }
          ],
          "subdirs": []
        },
        {
          "directory": "runners",
          "path": "ellama_codebase/minigpt4/runners",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/runners/__init__.py",
              "type": "python",
              "imports": [
                "from minigpt4.runners.runner_base import RunnerBase"
              ],
              "classes": [],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/runners/runner_base.py",
              "type": "python",
              "imports": [
                "import datetime",
                "import json",
                "import logging",
                "import os",
                "import time",
                "from pathlib import Path",
                "import torch",
                "import torch.distributed as dist",
                "import webdataset as wds",
                "from minigpt4.common.dist_utils import download_cached_file, get_rank, get_world_size, is_main_process, main_process",
                "from minigpt4.common.registry import registry",
                "from minigpt4.common.utils import is_url",
                "from minigpt4.datasets.data_utils import concat_datasets, reorg_datasets_by_split, ChainDataset",
                "from minigpt4.datasets.datasets.dataloader_utils import IterLoader, MultiIterLoader, PrefetchLoader",
                "from torch.nn.parallel import DistributedDataParallel as DDP",
                "from torch.utils.data import DataLoader, DistributedSampler"
              ],
              "classes": [
                {
                  "name": "RunnerBase",
                  "decorators": [
                    "registry.register_runner('runner_base')"
                  ],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.config = cfg\nself.job_id = job_id\nself.task = task\nself.datasets = datasets\nself._model = model\nself._wrapped_model = None\nself._device = None\nself._optimizer = None\nself._scaler = None\nself._dataloaders = None\nself._lr_sched = None\nself.start_epoch = 0\nself.setup_output_dir()"
                    },
                    {
                      "name": "device",
                      "decorators": [
                        "property"
                      ],
                      "body": "if self._device is None:\n    self._device = torch.device(self.config.run_cfg.device)\nreturn self._device"
                    },
                    {
                      "name": "use_distributed",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.config.run_cfg.distributed"
                    },
                    {
                      "name": "model",
                      "decorators": [
                        "property"
                      ],
                      "body": "'\\n        A property to get the DDP-wrapped model on the device.\\n        '\nif self._model.device != self.device:\n    self._model = self._model.to(self.device)\n    if self.use_distributed:\n        if self._wrapped_model is None:\n            self._wrapped_model = DDP(self._model, device_ids=[self.config.run_cfg.gpu], find_unused_parameters=True)\n    else:\n        self._wrapped_model = self._model\nreturn self._wrapped_model"
                    },
                    {
                      "name": "optimizer",
                      "decorators": [
                        "property"
                      ],
                      "body": "if self._optimizer is None:\n    num_parameters = 0\n    (p_wd, p_non_wd) = ([], [])\n    attention = []\n    for (n, p) in self.model.named_parameters():\n        if not p.requires_grad:\n            continue\n        print(n)\n        if p.ndim < 2 or 'bias' in n or 'ln' in n or ('bn' in n):\n            p_non_wd.append(p)\n        else:\n            p_wd.append(p)\n        num_parameters += p.data.nelement()\n    logging.info('number of trainable parameters: %d' % num_parameters)\n    optim_params = [{'params': p_wd, 'weight_decay': float(self.config.run_cfg.weight_decay), 'lr': float(self.config.run_cfg.init_lr)}, {'params': p_non_wd, 'weight_decay': 0, 'lr': float(self.config.run_cfg.init_lr)}]\n    beta2 = self.config.run_cfg.get('beta2', 0.999)\n    self._optimizer = torch.optim.AdamW(optim_params, lr=float(self.config.run_cfg.init_lr), weight_decay=float(self.config.run_cfg.weight_decay), betas=(0.9, beta2))\nreturn self._optimizer"
                    },
                    {
                      "name": "scaler",
                      "decorators": [
                        "property"
                      ],
                      "body": "amp = self.config.run_cfg.get('amp', False)\nif amp:\n    if self._scaler is None:\n        self._scaler = torch.cuda.amp.GradScaler()\nreturn self._scaler"
                    },
                    {
                      "name": "lr_scheduler",
                      "decorators": [
                        "property"
                      ],
                      "body": "'\\n        A property to get and create learning rate scheduler by split just in need.\\n        '\nif self._lr_sched is None:\n    lr_sched_cls = registry.get_lr_scheduler_class(self.config.run_cfg.lr_sched)\n    max_epoch = self.max_epoch\n    min_lr = self.min_lr\n    init_lr = self.init_lr\n    decay_rate = self.config.run_cfg.get('lr_decay_rate', None)\n    warmup_start_lr = self.config.run_cfg.get('warmup_lr', -1)\n    warmup_steps = self.config.run_cfg.get('warmup_steps', 0)\n    iters_per_epoch = self.config.run_cfg.get('iters_per_epoch', None)\n    if iters_per_epoch is None:\n        try:\n            iters_per_epoch = len(self.dataloaders['train'])\n        except (AttributeError, TypeError):\n            iters_per_epoch = 10000\n    self._lr_sched = lr_sched_cls(optimizer=self.optimizer, max_epoch=max_epoch, iters_per_epoch=iters_per_epoch, min_lr=min_lr, init_lr=init_lr, decay_rate=decay_rate, warmup_start_lr=warmup_start_lr, warmup_steps=warmup_steps)\nreturn self._lr_sched"
                    },
                    {
                      "name": "dataloaders",
                      "decorators": [
                        "property"
                      ],
                      "body": "'\\n        A property to get and create dataloaders by split just in need.\\n\\n        If no train_dataset_ratio is provided, concatenate map-style datasets and\\n        chain wds.DataPipe datasets separately. Training set becomes a tuple\\n        (ConcatDataset, ChainDataset), both are optional but at least one of them is\\n        required. The resultant ConcatDataset and ChainDataset will be sampled evenly.\\n\\n        If train_dataset_ratio is provided, create a MultiIterLoader to sample\\n        each dataset by ratios during training.\\n\\n        Currently do not support multiple datasets for validation and test.\\n\\n        Returns:\\n            dict: {split_name: (tuples of) dataloader}\\n        '\nif self._dataloaders is None:\n    logging.info('dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).')\n    batch_sizes = {dataset_name: getattr(self.config.datasets_cfg, dataset_name).batch_size for dataset_name in self.datasets.keys()}\n    (datasets, batch_sizes) = reorg_datasets_by_split(self.datasets, batch_sizes)\n    self.datasets = datasets\n    for split_name in self.datasets:\n        if isinstance(self.datasets[split_name], tuple) or isinstance(self.datasets[split_name], list):\n            num_records = sum([len(d) if not type(d) in [wds.DataPipeline, ChainDataset] else 0 for d in self.datasets[split_name]])\n        elif hasattr(self.datasets[split_name], '__len__'):\n            num_records = len(self.datasets[split_name])\n        else:\n            num_records = -1\n            logging.info('Only a single wds.DataPipeline dataset, no __len__ attribute.')\n        if num_records >= 0:\n            logging.info('Loaded {} records for {} split from the dataset.'.format(num_records, split_name))\n    split_names = sorted(self.datasets.keys())\n    datasets = [self.datasets[split] for split in split_names]\n    batch_sizes = [batch_sizes[split] for split in split_names]\n    is_trains = [split in self.train_splits for split in split_names]\n    print('batch sizes', batch_sizes)\n    collate_fns = []\n    for dataset in datasets:\n        if isinstance(dataset, tuple) or isinstance(dataset, list):\n            collate_fns.append([getattr(d, 'collater', None) for d in dataset])\n        else:\n            collate_fns.append(getattr(dataset, 'collater', None))\n    dataloaders = self.create_loaders(datasets=datasets, num_workers=self.config.run_cfg.num_workers, batch_sizes=batch_sizes, is_trains=is_trains, collate_fns=collate_fns)\n    self._dataloaders = {k: v for (k, v) in zip(split_names, dataloaders)}\nreturn self._dataloaders"
                    },
                    {
                      "name": "cuda_enabled",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.device.type == 'cuda'"
                    },
                    {
                      "name": "max_epoch",
                      "decorators": [
                        "property"
                      ],
                      "body": "return int(self.config.run_cfg.max_epoch)"
                    },
                    {
                      "name": "log_freq",
                      "decorators": [
                        "property"
                      ],
                      "body": "log_freq = self.config.run_cfg.get('log_freq', 50)\nreturn int(log_freq)"
                    },
                    {
                      "name": "init_lr",
                      "decorators": [
                        "property"
                      ],
                      "body": "return float(self.config.run_cfg.init_lr)"
                    },
                    {
                      "name": "min_lr",
                      "decorators": [
                        "property"
                      ],
                      "body": "return float(self.config.run_cfg.min_lr)"
                    },
                    {
                      "name": "accum_grad_iters",
                      "decorators": [
                        "property"
                      ],
                      "body": "return int(self.config.run_cfg.get('accum_grad_iters', 1))"
                    },
                    {
                      "name": "valid_splits",
                      "decorators": [
                        "property"
                      ],
                      "body": "valid_splits = self.config.run_cfg.get('valid_splits', [])\nif len(valid_splits) == 0:\n    logging.info('No validation splits found.')\nreturn valid_splits"
                    },
                    {
                      "name": "test_splits",
                      "decorators": [
                        "property"
                      ],
                      "body": "test_splits = self.config.run_cfg.get('test_splits', [])\nreturn test_splits"
                    },
                    {
                      "name": "train_splits",
                      "decorators": [
                        "property"
                      ],
                      "body": "train_splits = self.config.run_cfg.get('train_splits', [])\nif len(train_splits) == 0:\n    logging.info('Empty train splits.')\nreturn train_splits"
                    },
                    {
                      "name": "evaluate_only",
                      "decorators": [
                        "property"
                      ],
                      "body": "'\\n        Set to True to skip training.\\n        '\nreturn self.config.run_cfg.evaluate"
                    },
                    {
                      "name": "use_dist_eval_sampler",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.config.run_cfg.get('use_dist_eval_sampler', True)"
                    },
                    {
                      "name": "resume_ckpt_path",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.config.run_cfg.get('resume_ckpt_path', None)"
                    },
                    {
                      "name": "train_loader",
                      "decorators": [
                        "property"
                      ],
                      "body": "train_dataloader = self.dataloaders['train']\nreturn train_dataloader"
                    },
                    {
                      "name": "setup_output_dir",
                      "decorators": [],
                      "body": "lib_root = Path(registry.get_path('library_root'))\noutput_dir = lib_root / self.config.run_cfg.output_dir / self.job_id\nresult_dir = output_dir / 'result'\noutput_dir.mkdir(parents=True, exist_ok=True)\nresult_dir.mkdir(parents=True, exist_ok=True)\nregistry.register_path('result_dir', str(result_dir))\nregistry.register_path('output_dir', str(output_dir))\nself.result_dir = result_dir\nself.output_dir = output_dir"
                    },
                    {
                      "name": "train",
                      "decorators": [],
                      "body": "start_time = time.time()\nbest_agg_metric = 0\nbest_epoch = 0\nself.log_config()\nif not self.evaluate_only and self.resume_ckpt_path is not None:\n    self._load_checkpoint(self.resume_ckpt_path)\nfor cur_epoch in range(self.start_epoch, self.max_epoch):\n    if not self.evaluate_only:\n        logging.info('Start training')\n        train_stats = self.train_epoch(cur_epoch)\n        self.log_stats(split_name='train', stats=train_stats)\n    if len(self.valid_splits) > 0:\n        for split_name in self.valid_splits:\n            logging.info('Evaluating on {}.'.format(split_name))\n            val_log = self.eval_epoch(split_name=split_name, cur_epoch=cur_epoch)\n            if val_log is not None:\n                if is_main_process():\n                    assert 'agg_metrics' in val_log, 'No agg_metrics found in validation log.'\n                    agg_metrics = val_log['agg_metrics']\n                    if agg_metrics > best_agg_metric and split_name == 'val':\n                        (best_epoch, best_agg_metric) = (cur_epoch, agg_metrics)\n                        self._save_checkpoint(cur_epoch, is_best=True)\n                    val_log.update({'best_epoch': best_epoch})\n                    self.log_stats(val_log, split_name)\n    elif not self.evaluate_only:\n        self._save_checkpoint(cur_epoch, is_best=False)\n    if self.evaluate_only:\n        break\n    if self.config.run_cfg.distributed:\n        dist.barrier()\ntest_epoch = 'best' if len(self.valid_splits) > 0 else cur_epoch\nself.evaluate(cur_epoch=test_epoch, skip_reload=self.evaluate_only)\ntotal_time = time.time() - start_time\ntotal_time_str = str(datetime.timedelta(seconds=int(total_time)))\nlogging.info('Training time {}'.format(total_time_str))"
                    },
                    {
                      "name": "evaluate",
                      "decorators": [],
                      "body": "test_logs = dict()\nif len(self.test_splits) > 0:\n    for split_name in self.test_splits:\n        test_logs[split_name] = self.eval_epoch(split_name=split_name, cur_epoch=cur_epoch, skip_reload=skip_reload)\n    return test_logs"
                    },
                    {
                      "name": "train_epoch",
                      "decorators": [],
                      "body": "self.model.train()\nreturn self.task.train_epoch(epoch=epoch, model=self.model, data_loader=self.train_loader, optimizer=self.optimizer, scaler=self.scaler, lr_scheduler=self.lr_scheduler, cuda_enabled=self.cuda_enabled, log_freq=self.log_freq, accum_grad_iters=self.accum_grad_iters)"
                    },
                    {
                      "name": "eval_epoch",
                      "decorators": [
                        "torch.no_grad()"
                      ],
                      "body": "'\\n        Evaluate the model on a given split.\\n\\n        Args:\\n            split_name (str): name of the split to evaluate on.\\n            cur_epoch (int): current epoch.\\n            skip_reload_best (bool): whether to skip reloading the best checkpoint.\\n                During training, we will reload the best checkpoint for validation.\\n                During testing, we will use provided weights and skip reloading the best checkpoint .\\n        '\ndata_loader = self.dataloaders.get(split_name, None)\nassert data_loader, 'data_loader for split {} is None.'.format(split_name)\nmodel = self.unwrap_dist_model(self.model)\nif not skip_reload and cur_epoch == 'best':\n    model = self._reload_best_model(model)\nmodel.eval()\nself.task.before_evaluation(model=model, dataset=self.datasets[split_name])\nresults = self.task.evaluation(model, data_loader)\nif results is not None:\n    return self.task.after_evaluation(val_result=results, split_name=split_name, epoch=cur_epoch)"
                    },
                    {
                      "name": "unwrap_dist_model",
                      "decorators": [],
                      "body": "if self.use_distributed:\n    return model.module\nelse:\n    return model"
                    },
                    {
                      "name": "create_loaders",
                      "decorators": [],
                      "body": "'\\n        Create dataloaders for training and validation.\\n        '\n\ndef _create_loader(dataset, num_workers, bsz, is_train, collate_fn):\n    if isinstance(dataset, ChainDataset) or isinstance(dataset, wds.DataPipeline):\n        loader = iter(DataLoader(dataset, batch_size=bsz, num_workers=num_workers, pin_memory=True))\n    else:\n        if self.use_distributed:\n            sampler = DistributedSampler(dataset, shuffle=is_train, num_replicas=get_world_size(), rank=get_rank())\n            if not self.use_dist_eval_sampler:\n                sampler = sampler if is_train else None\n        else:\n            sampler = None\n        loader = DataLoader(dataset, batch_size=bsz, num_workers=num_workers, pin_memory=True, sampler=sampler, shuffle=sampler is None and is_train, collate_fn=collate_fn, drop_last=True if is_train else False)\n        loader = PrefetchLoader(loader)\n        if is_train:\n            loader = IterLoader(loader, use_distributed=self.use_distributed)\n    return loader\nloaders = []\nfor (dataset, bsz, is_train, collate_fn) in zip(datasets, batch_sizes, is_trains, collate_fns):\n    if isinstance(dataset, list) or isinstance(dataset, tuple):\n        if hasattr(dataset[0], 'sample_ratio') and dataset_ratios is None:\n            dataset_ratios = [d.sample_ratio for d in dataset]\n        loader = MultiIterLoader(loaders=[_create_loader(d, num_workers, bsz[i], is_train, collate_fn[i]) for (i, d) in enumerate(dataset)], ratios=dataset_ratios)\n    else:\n        loader = _create_loader(dataset, num_workers, bsz, is_train, collate_fn)\n    loaders.append(loader)\nreturn loaders"
                    },
                    {
                      "name": "_save_checkpoint",
                      "decorators": [
                        "main_process"
                      ],
                      "body": "'\\n        Save the checkpoint at the current epoch.\\n        '\nmodel_no_ddp = self.unwrap_dist_model(self.model)\nparam_grad_dic = {k: v.requires_grad for (k, v) in model_no_ddp.named_parameters()}\nstate_dict = model_no_ddp.state_dict()\nfor k in list(state_dict.keys()):\n    if k in param_grad_dic.keys() and (not param_grad_dic[k]):\n        del state_dict[k]\nsave_obj = {'model': state_dict, 'optimizer': self.optimizer.state_dict(), 'config': self.config.to_dict(), 'scaler': self.scaler.state_dict() if self.scaler else None, 'epoch': cur_epoch}\nsave_to = os.path.join(self.output_dir, 'checkpoint_{}.pth'.format('best' if is_best else cur_epoch))\nlogging.info('Saving checkpoint at epoch {} to {}.'.format(cur_epoch, save_to))\ntorch.save(save_obj, save_to)"
                    },
                    {
                      "name": "_reload_best_model",
                      "decorators": [],
                      "body": "'\\n        Load the best checkpoint for evaluation.\\n        '\ncheckpoint_path = os.path.join(self.output_dir, 'checkpoint_best.pth')\nlogging.info('Loading checkpoint from {}.'.format(checkpoint_path))\ncheckpoint = torch.load(checkpoint_path, map_location='cpu')\ntry:\n    model.load_state_dict(checkpoint['model'])\nexcept RuntimeError as e:\n    logging.warning('\\n                Key mismatch when loading checkpoint. This is expected if only part of the model is saved.\\n                Trying to load the model with strict=False.\\n                ')\n    model.load_state_dict(checkpoint['model'], strict=False)\nreturn model"
                    },
                    {
                      "name": "_load_checkpoint",
                      "decorators": [],
                      "body": "'\\n        Resume from a checkpoint.\\n        '\nif is_url(url_or_filename):\n    cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n    checkpoint = torch.load(cached_file, map_location=self.device)\nelif os.path.isfile(url_or_filename):\n    checkpoint = torch.load(url_or_filename, map_location=self.device)\nelse:\n    raise RuntimeError('checkpoint url or path is invalid')\nstate_dict = checkpoint['model']\nmessage = self.unwrap_dist_model(self.model).load_state_dict(state_dict, strict=False)\nself.optimizer.load_state_dict(checkpoint['optimizer'])\nif self.scaler and 'scaler' in checkpoint:\n    self.scaler.load_state_dict(checkpoint['scaler'])\nself.start_epoch = checkpoint['epoch'] + 1\nprint('resume the checkpoint')\nlogging.info('Resume checkpoint from {}'.format(url_or_filename))"
                    },
                    {
                      "name": "log_stats",
                      "decorators": [
                        "main_process"
                      ],
                      "body": "if isinstance(stats, dict):\n    log_stats = {**{f'{split_name}_{k}': v for (k, v) in stats.items()}}\n    with open(os.path.join(self.output_dir, 'log.txt'), 'a') as f:\n        f.write(json.dumps(log_stats) + '\\n')\nelif isinstance(stats, list):\n    pass"
                    },
                    {
                      "name": "log_config",
                      "decorators": [
                        "main_process"
                      ],
                      "body": "with open(os.path.join(self.output_dir, 'log.txt'), 'a') as f:\n    f.write(json.dumps(self.config.to_dict(), indent=4) + '\\n')"
                    }
                  ]
                }
              ],
              "other_functions": []
            }
          ],
          "subdirs": []
        },
        {
          "directory": "common",
          "path": "ellama_codebase/minigpt4/common",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/common/config.py",
              "type": "python",
              "imports": [
                "import logging",
                "import json",
                "from typing import Dict",
                "from omegaconf import OmegaConf",
                "from minigpt4.common.registry import registry"
              ],
              "classes": [
                {
                  "name": "Config",
                  "decorators": [],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.config = {}\nself.args = args\nregistry.register('configuration', self)\nuser_config = self._build_opt_list(self.args.options)\nconfig = OmegaConf.load(self.args.cfg_path)\nrunner_config = self.build_runner_config(config)\nmodel_config = self.build_model_config(config, **user_config)\ndataset_config = self.build_dataset_config(config)\nevaluation_dataset_config = self.build_evaluation_dataset_config(config)\nself.config = OmegaConf.merge(runner_config, model_config, dataset_config, evaluation_dataset_config, user_config)"
                    },
                    {
                      "name": "_validate_runner_config",
                      "decorators": [],
                      "body": "'\\n        This method validates the configuration, such that\\n            1) all the user specified options are valid;\\n            2) no type mismatches between the user specified options and the config.\\n        '\nrunner_config_validator = create_runner_config_validator()\nrunner_config_validator.validate(runner_config)"
                    },
                    {
                      "name": "_build_opt_list",
                      "decorators": [],
                      "body": "opts_dot_list = self._convert_to_dot_list(opts)\nreturn OmegaConf.from_dotlist(opts_dot_list)"
                    },
                    {
                      "name": "build_model_config",
                      "decorators": [
                        "staticmethod"
                      ],
                      "body": "model = config.get('model', None)\nassert model is not None, 'Missing model configuration file.'\nmodel_cls = registry.get_model_class(model.arch)\nassert model_cls is not None, f\"Model '{model.arch}' has not been registered.\"\nmodel_type = kwargs.get('model.model_type', None)\nif not model_type:\n    model_type = model.get('model_type', None)\nassert model_type is not None, 'Missing model_type.'\nmodel_config_path = model_cls.default_config_path(model_type=model_type)\nmodel_config = OmegaConf.create()\nmodel_config = OmegaConf.merge(model_config, OmegaConf.load(model_config_path), {'model': config['model']})\nreturn model_config"
                    },
                    {
                      "name": "build_runner_config",
                      "decorators": [
                        "staticmethod"
                      ],
                      "body": "return {'run': config.run}"
                    },
                    {
                      "name": "build_dataset_config",
                      "decorators": [
                        "staticmethod"
                      ],
                      "body": "datasets = config.get('datasets', None)\nif datasets is None:\n    raise KeyError(\"Expecting 'datasets' as the root key for dataset configuration.\")\ndataset_config = OmegaConf.create()\nfor dataset_name in datasets:\n    builder_cls = registry.get_builder_class(dataset_name)\n    dataset_config_type = datasets[dataset_name].get('type', 'default')\n    dataset_config_path = builder_cls.default_config_path(type=dataset_config_type)\n    dataset_config = OmegaConf.merge(dataset_config, OmegaConf.load(dataset_config_path), {'datasets': {dataset_name: config['datasets'][dataset_name]}})\nreturn dataset_config"
                    },
                    {
                      "name": "build_evaluation_dataset_config",
                      "decorators": [
                        "staticmethod"
                      ],
                      "body": "datasets = config.get('evaluation_datasets', None)\ndataset_config = OmegaConf.create()\nif datasets is not None:\n    for dataset_name in datasets:\n        builder_cls = registry.get_builder_class(dataset_name)\n        dataset_config = OmegaConf.merge(dataset_config, {'evaluation_datasets': {dataset_name: config['evaluation_datasets'][dataset_name]}})\nreturn dataset_config"
                    },
                    {
                      "name": "_convert_to_dot_list",
                      "decorators": [],
                      "body": "if opts is None:\n    opts = []\nif len(opts) == 0:\n    return opts\nhas_equal = opts[0].find('=') != -1\nif has_equal:\n    return opts\nreturn [opt + '=' + value for (opt, value) in zip(opts[0::2], opts[1::2])]"
                    },
                    {
                      "name": "get_config",
                      "decorators": [],
                      "body": "return self.config"
                    },
                    {
                      "name": "run_cfg",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.config.run"
                    },
                    {
                      "name": "datasets_cfg",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.config.datasets"
                    },
                    {
                      "name": "evaluation_datasets_cfg",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.config.evaluation_datasets"
                    },
                    {
                      "name": "model_cfg",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.config.model"
                    },
                    {
                      "name": "pretty_print",
                      "decorators": [],
                      "body": "logging.info('\\n=====  Running Parameters    =====')\nlogging.info(self._convert_node_to_json(self.config.run))\nlogging.info('\\n======  Dataset Attributes  ======')\ndatasets = self.config.datasets\nfor dataset in datasets:\n    if dataset in self.config.datasets:\n        logging.info(f'\\n======== {dataset} =======')\n        dataset_config = self.config.datasets[dataset]\n        logging.info(self._convert_node_to_json(dataset_config))\n    else:\n        logging.warning(f\"No dataset named '{dataset}' in config. Skipping\")\nlogging.info(f'\\n======  Model Attributes  ======')\nlogging.info(self._convert_node_to_json(self.config.model))"
                    },
                    {
                      "name": "_convert_node_to_json",
                      "decorators": [],
                      "body": "container = OmegaConf.to_container(node, resolve=True)\nreturn json.dumps(container, indent=4, sort_keys=True)"
                    },
                    {
                      "name": "to_dict",
                      "decorators": [],
                      "body": "return OmegaConf.to_container(self.config)"
                    }
                  ]
                },
                {
                  "name": "ConfigValidator",
                  "decorators": [],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.description = description\nself.arguments = dict()\nself.parsed_args = None"
                    },
                    {
                      "name": "__getitem__",
                      "decorators": [],
                      "body": "assert self.parsed_args is not None, 'No arguments parsed yet.'\nreturn self.parsed_args[key]"
                    },
                    {
                      "name": "__str__",
                      "decorators": [],
                      "body": "return self.format_help()"
                    },
                    {
                      "name": "add_argument",
                      "decorators": [],
                      "body": "'\\n        Assume the first argument is the name of the argument.\\n        '\nself.arguments[args[0]] = self._Argument(*args, **kwargs)"
                    },
                    {
                      "name": "validate",
                      "decorators": [],
                      "body": "'\\n        Convert yaml config (dict-like) to list, required by argparse.\\n        '\nfor (k, v) in config.items():\n    assert k in self.arguments, f'{k} is not a valid argument. Support arguments are {self.format_arguments()}.'\n    if self.arguments[k].type is not None:\n        try:\n            self.arguments[k].val = self.arguments[k].type(v)\n        except ValueError:\n            raise ValueError(f'{k} is not a valid {self.arguments[k].type}.')\n    if self.arguments[k].choices is not None:\n        assert v in self.arguments[k].choices, f'{k} must be one of {self.arguments[k].choices}.'\nreturn config"
                    },
                    {
                      "name": "format_arguments",
                      "decorators": [],
                      "body": "return str([f'{k}' for k in sorted(self.arguments.keys())])"
                    },
                    {
                      "name": "format_help",
                      "decorators": [],
                      "body": "help_msg = str(self.description)\nreturn help_msg + ', available arguments: ' + self.format_arguments()"
                    },
                    {
                      "name": "print_help",
                      "decorators": [],
                      "body": "print(self.format_help())"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "node_to_dict",
                  "decorators": [],
                  "body": "return OmegaConf.to_container(node)"
                },
                {
                  "name": "create_runner_config_validator",
                  "decorators": [],
                  "body": "validator = ConfigValidator(description='Runner configurations')\nvalidator.add_argument('runner', type=str, choices=['runner_base', 'runner_iter'], help='Runner to use. The \"runner_base\" uses epoch-based training while iter-based\\n            runner runs based on iters. Default: runner_base')\nvalidator.add_argument('train_dataset_ratios', type=Dict[str, float], help='Ratios of training dataset. This is used in iteration-based runner.\\n        Do not support for epoch-based runner because how to define an epoch becomes tricky.\\n        Default: None')\nvalidator.add_argument('max_iters', type=float, help='Maximum number of iterations to run.')\nvalidator.add_argument('max_epoch', type=int, help='Maximum number of epochs to run.')\nvalidator.add_argument('iters_per_inner_epoch', type=float, help='Number of iterations per inner epoch. This is required when runner is runner_iter.')\nlr_scheds_choices = registry.list_lr_schedulers()\nvalidator.add_argument('lr_sched', type=str, choices=lr_scheds_choices, help='Learning rate scheduler to use, from {}'.format(lr_scheds_choices))\ntask_choices = registry.list_tasks()\nvalidator.add_argument('task', type=str, choices=task_choices, help='Task to use, from {}'.format(task_choices))\nvalidator.add_argument('init_lr', type=float, help='Initial learning rate. This will be the learning rate after warmup and before decay.')\nvalidator.add_argument('min_lr', type=float, help='Minimum learning rate (after decay).')\nvalidator.add_argument('warmup_lr', type=float, help='Starting learning rate for warmup.')\nvalidator.add_argument('lr_decay_rate', type=float, help='Learning rate decay rate. Required if using a decaying learning rate scheduler.')\nvalidator.add_argument('weight_decay', type=float, help='Weight decay rate.')\nvalidator.add_argument('batch_size_train', type=int, help='Training batch size.')\nvalidator.add_argument('batch_size_eval', type=int, help='Evaluation batch size, including validation and testing.')\nvalidator.add_argument('num_workers', help='Number of workers for data loading.')\nvalidator.add_argument('warmup_steps', type=int, help='Number of warmup steps. Required if a warmup schedule is used.')\nvalidator.add_argument('seed', type=int, help='Random seed.')\nvalidator.add_argument('output_dir', type=str, help='Output directory to save checkpoints and logs.')\nvalidator.add_argument('evaluate', help='Whether to only evaluate the model. If true, training will not be performed.')\nvalidator.add_argument('train_splits', type=list, help='Splits to use for training.')\nvalidator.add_argument('valid_splits', type=list, help='Splits to use for validation. If not provided, will skip the validation.')\nvalidator.add_argument('test_splits', type=list, help='Splits to use for testing. If not provided, will skip the testing.')\nvalidator.add_argument('accum_grad_iters', type=int, help='Number of iterations to accumulate gradient for.')\nvalidator.add_argument('device', type=str, choices=['cpu', 'cuda'], help=\"Device to use. Support 'cuda' or 'cpu' as for now.\")\nvalidator.add_argument('world_size', type=int, help='Number of processes participating in the job.')\nvalidator.add_argument('dist_url', type=str)\nvalidator.add_argument('distributed', type=bool)\nvalidator.add_argument('use_dist_eval_sampler', type=bool, help='Whether to use distributed sampler during evaluation or not.')\nvalidator.add_argument('max_len', type=int, help='Maximal length of text output.')\nvalidator.add_argument('min_len', type=int, help='Minimal length of text output.')\nvalidator.add_argument('num_beams', type=int, help='Number of beams used for beam search.')\nvalidator.add_argument('num_ans_candidates', type=int, help='For ALBEF and BLIP, these models first rank answers according to likelihood to select answer candidates.')\nvalidator.add_argument('inference_method', type=str, choices=['genearte', 'rank'], help='Inference method to use for question answering. If rank, requires a answer list.')\nvalidator.add_argument('k_test', type=int, help='Number of top k most similar samples from ITC/VTC selection to be tested.')\nreturn validator"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/common/registry.py",
              "type": "python",
              "imports": [
                "from minigpt4.datasets.builders.base_dataset_builder import BaseDatasetBuilder",
                "from minigpt4.tasks.base_task import BaseTask",
                "from minigpt4.models import BaseModel",
                "from minigpt4.processors import BaseProcessor"
              ],
              "classes": [
                {
                  "name": "Registry",
                  "decorators": [],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "register_builder",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Register a dataset builder to registry with key 'name'\\n\\n        Args:\\n            name: Key with which the builder will be registered.\\n\\n        Usage:\\n\\n            from minigpt4.common.registry import registry\\n            from minigpt4.datasets.base_dataset_builder import BaseDatasetBuilder\\n        \"\n\ndef wrap(builder_cls):\n    from minigpt4.datasets.builders.base_dataset_builder import BaseDatasetBuilder\n    assert issubclass(builder_cls, BaseDatasetBuilder), 'All builders must inherit BaseDatasetBuilder class, found {}'.format(builder_cls)\n    if name in cls.mapping['builder_name_mapping']:\n        raise KeyError(\"Name '{}' already registered for {}.\".format(name, cls.mapping['builder_name_mapping'][name]))\n    cls.mapping['builder_name_mapping'][name] = builder_cls\n    return builder_cls\nreturn wrap"
                    },
                    {
                      "name": "register_task",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Register a task to registry with key 'name'\\n\\n        Args:\\n            name: Key with which the task will be registered.\\n\\n        Usage:\\n\\n            from minigpt4.common.registry import registry\\n        \"\n\ndef wrap(task_cls):\n    from minigpt4.tasks.base_task import BaseTask\n    assert issubclass(task_cls, BaseTask), 'All tasks must inherit BaseTask class'\n    if name in cls.mapping['task_name_mapping']:\n        raise KeyError(\"Name '{}' already registered for {}.\".format(name, cls.mapping['task_name_mapping'][name]))\n    cls.mapping['task_name_mapping'][name] = task_cls\n    return task_cls\nreturn wrap"
                    },
                    {
                      "name": "register_model",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Register a task to registry with key 'name'\\n\\n        Args:\\n            name: Key with which the task will be registered.\\n\\n        Usage:\\n\\n            from minigpt4.common.registry import registry\\n        \"\n\ndef wrap(model_cls):\n    from minigpt4.models import BaseModel\n    assert issubclass(model_cls, BaseModel), 'All models must inherit BaseModel class'\n    if name in cls.mapping['model_name_mapping']:\n        raise KeyError(\"Name '{}' already registered for {}.\".format(name, cls.mapping['model_name_mapping'][name]))\n    cls.mapping['model_name_mapping'][name] = model_cls\n    return model_cls\nreturn wrap"
                    },
                    {
                      "name": "register_processor",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Register a processor to registry with key 'name'\\n\\n        Args:\\n            name: Key with which the task will be registered.\\n\\n        Usage:\\n\\n            from minigpt4.common.registry import registry\\n        \"\n\ndef wrap(processor_cls):\n    from minigpt4.processors import BaseProcessor\n    assert issubclass(processor_cls, BaseProcessor), 'All processors must inherit BaseProcessor class'\n    if name in cls.mapping['processor_name_mapping']:\n        raise KeyError(\"Name '{}' already registered for {}.\".format(name, cls.mapping['processor_name_mapping'][name]))\n    cls.mapping['processor_name_mapping'][name] = processor_cls\n    return processor_cls\nreturn wrap"
                    },
                    {
                      "name": "register_lr_scheduler",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Register a model to registry with key 'name'\\n\\n        Args:\\n            name: Key with which the task will be registered.\\n\\n        Usage:\\n\\n            from minigpt4.common.registry import registry\\n        \"\n\ndef wrap(lr_sched_cls):\n    if name in cls.mapping['lr_scheduler_name_mapping']:\n        raise KeyError(\"Name '{}' already registered for {}.\".format(name, cls.mapping['lr_scheduler_name_mapping'][name]))\n    cls.mapping['lr_scheduler_name_mapping'][name] = lr_sched_cls\n    return lr_sched_cls\nreturn wrap"
                    },
                    {
                      "name": "register_runner",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Register a model to registry with key 'name'\\n\\n        Args:\\n            name: Key with which the task will be registered.\\n\\n        Usage:\\n\\n            from minigpt4.common.registry import registry\\n        \"\n\ndef wrap(runner_cls):\n    if name in cls.mapping['runner_name_mapping']:\n        raise KeyError(\"Name '{}' already registered for {}.\".format(name, cls.mapping['runner_name_mapping'][name]))\n    cls.mapping['runner_name_mapping'][name] = runner_cls\n    return runner_cls\nreturn wrap"
                    },
                    {
                      "name": "register_path",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Register a path to registry with key 'name'\\n\\n        Args:\\n            name: Key with which the path will be registered.\\n\\n        Usage:\\n\\n            from minigpt4.common.registry import registry\\n        \"\nassert isinstance(path, str), 'All path must be str.'\nif name in cls.mapping['paths']:\n    raise KeyError(\"Name '{}' already registered.\".format(name))\ncls.mapping['paths'][name] = path"
                    },
                    {
                      "name": "register",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "'Register an item to registry with key \\'name\\'\\n\\n        Args:\\n            name: Key with which the item will be registered.\\n\\n        Usage::\\n\\n            from minigpt4.common.registry import registry\\n\\n            registry.register(\"config\", {})\\n        '\npath = name.split('.')\ncurrent = cls.mapping['state']\nfor part in path[:-1]:\n    if part not in current:\n        current[part] = {}\n    current = current[part]\ncurrent[path[-1]] = obj"
                    },
                    {
                      "name": "get_builder_class",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls.mapping['builder_name_mapping'].get(name, None)"
                    },
                    {
                      "name": "get_model_class",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls.mapping['model_name_mapping'].get(name, None)"
                    },
                    {
                      "name": "get_task_class",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls.mapping['task_name_mapping'].get(name, None)"
                    },
                    {
                      "name": "get_processor_class",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls.mapping['processor_name_mapping'].get(name, None)"
                    },
                    {
                      "name": "get_lr_scheduler_class",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls.mapping['lr_scheduler_name_mapping'].get(name, None)"
                    },
                    {
                      "name": "get_runner_class",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls.mapping['runner_name_mapping'].get(name, None)"
                    },
                    {
                      "name": "list_runners",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return sorted(cls.mapping['runner_name_mapping'].keys())"
                    },
                    {
                      "name": "list_models",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return sorted(cls.mapping['model_name_mapping'].keys())"
                    },
                    {
                      "name": "list_tasks",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return sorted(cls.mapping['task_name_mapping'].keys())"
                    },
                    {
                      "name": "list_processors",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return sorted(cls.mapping['processor_name_mapping'].keys())"
                    },
                    {
                      "name": "list_lr_schedulers",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return sorted(cls.mapping['lr_scheduler_name_mapping'].keys())"
                    },
                    {
                      "name": "list_datasets",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return sorted(cls.mapping['builder_name_mapping'].keys())"
                    },
                    {
                      "name": "get_path",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls.mapping['paths'].get(name, None)"
                    },
                    {
                      "name": "get",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "\"Get an item from registry with key 'name'\\n\\n        Args:\\n            name (string): Key whose value needs to be retrieved.\\n            default: If passed and key is not in registry, default value will\\n                     be returned with a warning. Default: None\\n            no_warning (bool): If passed as True, warning when key doesn't exist\\n                               will not be generated. Useful for MMF's\\n                               internal operations. Default: False\\n        \"\noriginal_name = name\nname = name.split('.')\nvalue = cls.mapping['state']\nfor subname in name:\n    value = value.get(subname, default)\n    if value is default:\n        break\nif 'writer' in cls.mapping['state'] and value == default and (no_warning is False):\n    cls.mapping['state']['writer'].warning('Key {} is not present in registry, returning default value of {}'.format(original_name, default))\nreturn value"
                    },
                    {
                      "name": "unregister",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "'Remove an item from registry with key \\'name\\'\\n\\n        Args:\\n            name: Key which needs to be removed.\\n        Usage::\\n\\n            from mmf.common.registry import registry\\n\\n            config = registry.unregister(\"config\")\\n        '\nreturn cls.mapping['state'].pop(name, None)"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/common/optims.py",
              "type": "python",
              "imports": [
                "import math",
                "from minigpt4.common.registry import registry"
              ],
              "classes": [
                {
                  "name": "LinearWarmupStepLRScheduler",
                  "decorators": [
                    "registry.register_lr_scheduler('linear_warmup_step_lr')"
                  ],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.optimizer = optimizer\nself.max_epoch = max_epoch\nself.min_lr = min_lr\nself.decay_rate = decay_rate\nself.init_lr = init_lr\nself.warmup_steps = warmup_steps\nself.warmup_start_lr = warmup_start_lr if warmup_start_lr >= 0 else init_lr"
                    },
                    {
                      "name": "step",
                      "decorators": [],
                      "body": "if cur_epoch == 0:\n    warmup_lr_schedule(step=cur_step, optimizer=self.optimizer, max_step=self.warmup_steps, init_lr=self.warmup_start_lr, max_lr=self.init_lr)\nelse:\n    step_lr_schedule(epoch=cur_epoch, optimizer=self.optimizer, init_lr=self.init_lr, min_lr=self.min_lr, decay_rate=self.decay_rate)"
                    }
                  ]
                },
                {
                  "name": "LinearWarmupCosineLRScheduler",
                  "decorators": [
                    "registry.register_lr_scheduler('linear_warmup_cosine_lr')"
                  ],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.optimizer = optimizer\nself.max_epoch = max_epoch\nself.iters_per_epoch = iters_per_epoch\nself.min_lr = min_lr\nself.init_lr = init_lr\nself.warmup_steps = warmup_steps\nself.warmup_start_lr = warmup_start_lr if warmup_start_lr >= 0 else init_lr"
                    },
                    {
                      "name": "step",
                      "decorators": [],
                      "body": "total_cur_step = cur_epoch * self.iters_per_epoch + cur_step\nif total_cur_step < self.warmup_steps:\n    warmup_lr_schedule(step=cur_step, optimizer=self.optimizer, max_step=self.warmup_steps, init_lr=self.warmup_start_lr, max_lr=self.init_lr)\nelse:\n    cosine_lr_schedule(epoch=total_cur_step, optimizer=self.optimizer, max_epoch=self.max_epoch * self.iters_per_epoch, init_lr=self.init_lr, min_lr=self.min_lr)"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "cosine_lr_schedule",
                  "decorators": [],
                  "body": "'Decay the learning rate'\nlr = (init_lr - min_lr) * 0.5 * (1.0 + math.cos(math.pi * epoch / max_epoch)) + min_lr\nfor param_group in optimizer.param_groups:\n    param_group['lr'] = lr"
                },
                {
                  "name": "warmup_lr_schedule",
                  "decorators": [],
                  "body": "'Warmup the learning rate'\nlr = min(max_lr, init_lr + (max_lr - init_lr) * step / max(max_step, 1))\nfor param_group in optimizer.param_groups:\n    param_group['lr'] = lr"
                },
                {
                  "name": "step_lr_schedule",
                  "decorators": [],
                  "body": "'Decay the learning rate'\nlr = max(min_lr, init_lr * decay_rate ** epoch)\nfor param_group in optimizer.param_groups:\n    param_group['lr'] = lr"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/common/logger.py",
              "type": "python",
              "imports": [
                "import datetime",
                "import logging",
                "import time",
                "from collections import defaultdict, deque",
                "import torch",
                "import torch.distributed as dist",
                "from minigpt4.common import dist_utils"
              ],
              "classes": [
                {
                  "name": "SmoothedValue",
                  "decorators": [],
                  "inherits": [
                    "object"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "if fmt is None:\n    fmt = '{median:.4f} ({global_avg:.4f})'\nself.deque = deque(maxlen=window_size)\nself.total = 0.0\nself.count = 0\nself.fmt = fmt"
                    },
                    {
                      "name": "update",
                      "decorators": [],
                      "body": "self.deque.append(value)\nself.count += n\nself.total += value * n"
                    },
                    {
                      "name": "synchronize_between_processes",
                      "decorators": [],
                      "body": "'\\n        Warning: does not synchronize the deque!\\n        '\nif not dist_utils.is_dist_avail_and_initialized():\n    return\nt = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\ndist.barrier()\ndist.all_reduce(t)\nt = t.tolist()\nself.count = int(t[0])\nself.total = t[1]"
                    },
                    {
                      "name": "median",
                      "decorators": [
                        "property"
                      ],
                      "body": "d = torch.tensor(list(self.deque))\nreturn d.median().item()"
                    },
                    {
                      "name": "avg",
                      "decorators": [
                        "property"
                      ],
                      "body": "d = torch.tensor(list(self.deque), dtype=torch.float32)\nreturn d.mean().item()"
                    },
                    {
                      "name": "global_avg",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.total / self.count"
                    },
                    {
                      "name": "max",
                      "decorators": [
                        "property"
                      ],
                      "body": "return max(self.deque)"
                    },
                    {
                      "name": "value",
                      "decorators": [
                        "property"
                      ],
                      "body": "return self.deque[-1]"
                    },
                    {
                      "name": "__str__",
                      "decorators": [],
                      "body": "return self.fmt.format(median=self.median, avg=self.avg, global_avg=self.global_avg, max=self.max, value=self.value)"
                    }
                  ]
                },
                {
                  "name": "MetricLogger",
                  "decorators": [],
                  "inherits": [
                    "object"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.meters = defaultdict(SmoothedValue)\nself.delimiter = delimiter"
                    },
                    {
                      "name": "update",
                      "decorators": [],
                      "body": "for (k, v) in kwargs.items():\n    if isinstance(v, torch.Tensor):\n        v = v.item()\n    assert isinstance(v, (float, int))\n    self.meters[k].update(v)"
                    },
                    {
                      "name": "__getattr__",
                      "decorators": [],
                      "body": "if attr in self.meters:\n    return self.meters[attr]\nif attr in self.__dict__:\n    return self.__dict__[attr]\nraise AttributeError(\"'{}' object has no attribute '{}'\".format(type(self).__name__, attr))"
                    },
                    {
                      "name": "__str__",
                      "decorators": [],
                      "body": "loss_str = []\nfor (name, meter) in self.meters.items():\n    loss_str.append('{}: {}'.format(name, str(meter)))\nreturn self.delimiter.join(loss_str)"
                    },
                    {
                      "name": "global_avg",
                      "decorators": [],
                      "body": "loss_str = []\nfor (name, meter) in self.meters.items():\n    loss_str.append('{}: {:.4f}'.format(name, meter.global_avg))\nreturn self.delimiter.join(loss_str)"
                    },
                    {
                      "name": "synchronize_between_processes",
                      "decorators": [],
                      "body": "for meter in self.meters.values():\n    meter.synchronize_between_processes()"
                    },
                    {
                      "name": "add_meter",
                      "decorators": [],
                      "body": "self.meters[name] = meter"
                    },
                    {
                      "name": "log_every",
                      "decorators": [],
                      "body": "i = 0\nif not header:\n    header = ''\nstart_time = time.time()\nend = time.time()\niter_time = SmoothedValue(fmt='{avg:.4f}')\ndata_time = SmoothedValue(fmt='{avg:.4f}')\nspace_fmt = ':' + str(len(str(len(iterable)))) + 'd'\nlog_msg = [header, '[{0' + space_fmt + '}/{1}]', 'eta: {eta}', '{meters}', 'time: {time}', 'data: {data}']\nif torch.cuda.is_available():\n    log_msg.append('max mem: {memory:.0f}')\nlog_msg = self.delimiter.join(log_msg)\nMB = 1024.0 * 1024.0\nfor obj in iterable:\n    data_time.update(time.time() - end)\n    yield obj\n    iter_time.update(time.time() - end)\n    if i % print_freq == 0 or i == len(iterable) - 1:\n        eta_seconds = iter_time.global_avg * (len(iterable) - i)\n        eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n        if torch.cuda.is_available():\n            print(log_msg.format(i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time), memory=torch.cuda.max_memory_allocated() / MB))\n        else:\n            print(log_msg.format(i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)))\n    i += 1\n    end = time.time()\ntotal_time = time.time() - start_time\ntotal_time_str = str(datetime.timedelta(seconds=int(total_time)))\nprint('{} Total time: {} ({:.4f} s / it)'.format(header, total_time_str, total_time / len(iterable)))"
                    }
                  ]
                },
                {
                  "name": "AttrDict",
                  "decorators": [],
                  "inherits": [
                    "dict"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super(AttrDict, self).__init__(*args, **kwargs)\nself.__dict__ = self"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "setup_logger",
                  "decorators": [],
                  "body": "logging.basicConfig(level=logging.INFO if dist_utils.is_main_process() else logging.WARN, format='%(asctime)s [%(levelname)s] %(message)s', handlers=[logging.StreamHandler()])"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/common/utils.py",
              "type": "python",
              "imports": [
                "import io",
                "import json",
                "import logging",
                "import os",
                "import pickle",
                "import re",
                "import shutil",
                "import urllib",
                "import urllib.error",
                "import urllib.request",
                "from typing import Optional",
                "from urllib.parse import urlparse",
                "import numpy as np",
                "import pandas as pd",
                "import yaml",
                "from iopath.common.download import download",
                "from iopath.common.file_io import file_lock, g_pathmgr",
                "from minigpt4.common.registry import registry",
                "from torch.utils.model_zoo import tqdm",
                "from torchvision.datasets.utils import check_integrity, download_file_from_google_drive, extract_archive",
                "from datetime import datetime",
                "import requests",
                "import requests",
                "from tqdm import tqdm"
              ],
              "classes": [],
              "other_functions": [
                {
                  "name": "now",
                  "decorators": [],
                  "body": "from datetime import datetime\nreturn datetime.now().strftime('%Y%m%d%H%M')[:-1]"
                },
                {
                  "name": "is_url",
                  "decorators": [],
                  "body": "parsed = urlparse(url_or_filename)\nreturn parsed.scheme in ('http', 'https')"
                },
                {
                  "name": "get_cache_path",
                  "decorators": [],
                  "body": "return os.path.expanduser(os.path.join(registry.get_path('cache_root'), rel_path))"
                },
                {
                  "name": "get_abs_path",
                  "decorators": [],
                  "body": "return os.path.join(registry.get_path('library_root'), rel_path)"
                },
                {
                  "name": "load_json",
                  "decorators": [],
                  "body": "with open(filename, 'r') as f:\n    return json.load(f)"
                },
                {
                  "name": "makedir",
                  "decorators": [],
                  "body": "'\\n    Create the directory if it does not exist.\\n    '\nis_success = False\ntry:\n    if not g_pathmgr.exists(dir_path):\n        g_pathmgr.mkdirs(dir_path)\n    is_success = True\nexcept BaseException:\n    print(f'Error creating directory: {dir_path}')\nreturn is_success"
                },
                {
                  "name": "get_redirected_url",
                  "decorators": [],
                  "body": "'\\n    Given a URL, returns the URL it redirects to or the\\n    original URL in case of no indirection\\n    '\nimport requests\nwith requests.Session() as session:\n    with session.get(url, stream=True, allow_redirects=True) as response:\n        if response.history:\n            return response.url\n        else:\n            return url"
                },
                {
                  "name": "to_google_drive_download_url",
                  "decorators": [],
                  "body": "'\\n    Utility function to transform a view URL of google drive\\n    to a download URL for google drive\\n    Example input:\\n        https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view\\n    Example output:\\n        https://drive.google.com/uc?export=download&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\\n    '\nsplits = view_url.split('/')\nassert splits[-1] == 'view'\nfile_id = splits[-2]\nreturn f'https://drive.google.com/uc?export=download&id={file_id}'"
                },
                {
                  "name": "download_google_drive_url",
                  "decorators": [],
                  "body": "'\\n    Download a file from google drive\\n    Downloading an URL from google drive requires confirmation when\\n    the file of the size is too big (google drive notifies that\\n    anti-viral checks cannot be performed on such files)\\n    '\nimport requests\nwith requests.Session() as session:\n    with session.get(url, stream=True, allow_redirects=True) as response:\n        for (k, v) in response.cookies.items():\n            if k.startswith('download_warning'):\n                url = url + '&confirm=' + v\n    with session.get(url, stream=True, verify=True) as response:\n        makedir(output_path)\n        path = os.path.join(output_path, output_file_name)\n        total_size = int(response.headers.get('Content-length', 0))\n        with open(path, 'wb') as file:\n            from tqdm import tqdm\n            with tqdm(total=total_size) as progress_bar:\n                for block in response.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE):\n                    file.write(block)\n                    progress_bar.update(len(block))"
                },
                {
                  "name": "_get_google_drive_file_id",
                  "decorators": [],
                  "body": "parts = urlparse(url)\nif re.match('(drive|docs)[.]google[.]com', parts.netloc) is None:\n    return None\nmatch = re.match('/file/d/(?P<id>[^/]*)', parts.path)\nif match is None:\n    return None\nreturn match.group('id')"
                },
                {
                  "name": "_urlretrieve",
                  "decorators": [],
                  "body": "with open(filename, 'wb') as fh:\n    with urllib.request.urlopen(urllib.request.Request(url, headers={'User-Agent': 'vissl'})) as response:\n        with tqdm(total=response.length) as pbar:\n            for chunk in iter(lambda : response.read(chunk_size), ''):\n                if not chunk:\n                    break\n                pbar.update(chunk_size)\n                fh.write(chunk)"
                },
                {
                  "name": "download_url",
                  "decorators": [],
                  "body": "'Download a file from a url and place it in root.\\n    Args:\\n        url (str): URL to download file from\\n        root (str): Directory to place downloaded file in\\n        filename (str, optional): Name to save the file under.\\n                                  If None, use the basename of the URL.\\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\\n    '\nroot = os.path.expanduser(root)\nif not filename:\n    filename = os.path.basename(url)\nfpath = os.path.join(root, filename)\nmakedir(root)\nif check_integrity(fpath, md5):\n    print('Using downloaded and verified file: ' + fpath)\n    return\nurl = get_redirected_url(url)\nfile_id = _get_google_drive_file_id(url)\nif file_id is not None:\n    return download_file_from_google_drive(file_id, root, filename, md5)\ntry:\n    print('Downloading ' + url + ' to ' + fpath)\n    _urlretrieve(url, fpath)\nexcept (urllib.error.URLError, IOError) as e:\n    if url[:5] == 'https':\n        url = url.replace('https:', 'http:')\n        print('Failed download. Trying https -> http instead. Downloading ' + url + ' to ' + fpath)\n        _urlretrieve(url, fpath)\n    else:\n        raise e\nif not check_integrity(fpath, md5):\n    raise RuntimeError('File not found or corrupted.')"
                },
                {
                  "name": "download_and_extract_archive",
                  "decorators": [],
                  "body": "download_root = os.path.expanduser(download_root)\nif extract_root is None:\n    extract_root = download_root\nif not filename:\n    filename = os.path.basename(url)\ndownload_url(url, download_root, filename, md5)\narchive = os.path.join(download_root, filename)\nprint('Extracting {} to {}'.format(archive, extract_root))\nextract_archive(archive, extract_root, remove_finished)"
                },
                {
                  "name": "cache_url",
                  "decorators": [],
                  "body": "'\\n    This implementation downloads the remote resource and caches it locally.\\n    The resource will only be downloaded if not previously requested.\\n    '\nparsed_url = urlparse(url)\ndirname = os.path.join(cache_dir, os.path.dirname(parsed_url.path.lstrip('/')))\nmakedir(dirname)\nfilename = url.split('/')[-1]\ncached = os.path.join(dirname, filename)\nwith file_lock(cached):\n    if not os.path.isfile(cached):\n        logging.info(f'Downloading {url} to {cached} ...')\n        cached = download(url, dirname, filename=filename)\nlogging.info(f'URL {url} cached in {cached}')\nreturn cached"
                },
                {
                  "name": "create_file_symlink",
                  "decorators": [],
                  "body": "'\\n    Simply create the symlinks for a given file1 to file2.\\n    Useful during model checkpointing to symlinks to the\\n    latest successful checkpoint.\\n    '\ntry:\n    if g_pathmgr.exists(file2):\n        g_pathmgr.rm(file2)\n    g_pathmgr.symlink(file1, file2)\nexcept Exception as e:\n    logging.info(f'Could NOT create symlink. Error: {e}')"
                },
                {
                  "name": "save_file",
                  "decorators": [],
                  "body": "'\\n    Common i/o utility to handle saving data to various file formats.\\n    Supported:\\n        .pkl, .pickle, .npy, .json\\n    Specifically for .json, users have the option to either append (default)\\n    or rewrite by passing in Boolean value to append_to_json.\\n    '\nif verbose:\n    logging.info(f'Saving data to file: {filename}')\nfile_ext = os.path.splitext(filename)[1]\nif file_ext in ['.pkl', '.pickle']:\n    with g_pathmgr.open(filename, 'wb') as fopen:\n        pickle.dump(data, fopen, pickle.HIGHEST_PROTOCOL)\nelif file_ext == '.npy':\n    with g_pathmgr.open(filename, 'wb') as fopen:\n        np.save(fopen, data)\nelif file_ext == '.json':\n    if append_to_json:\n        with g_pathmgr.open(filename, 'a') as fopen:\n            fopen.write(json.dumps(data, sort_keys=True) + '\\n')\n            fopen.flush()\n    else:\n        with g_pathmgr.open(filename, 'w') as fopen:\n            fopen.write(json.dumps(data, sort_keys=True) + '\\n')\n            fopen.flush()\nelif file_ext == '.yaml':\n    with g_pathmgr.open(filename, 'w') as fopen:\n        dump = yaml.dump(data)\n        fopen.write(dump)\n        fopen.flush()\nelse:\n    raise Exception(f'Saving {file_ext} is not supported yet')\nif verbose:\n    logging.info(f'Saved data to file: {filename}')"
                },
                {
                  "name": "load_file",
                  "decorators": [],
                  "body": "'\\n    Common i/o utility to handle loading data from various file formats.\\n    Supported:\\n        .pkl, .pickle, .npy, .json\\n    For the npy files, we support reading the files in mmap_mode.\\n    If the mmap_mode of reading is not successful, we load data without the\\n    mmap_mode.\\n    '\nif verbose:\n    logging.info(f'Loading data from file: {filename}')\nfile_ext = os.path.splitext(filename)[1]\nif file_ext == '.txt':\n    with g_pathmgr.open(filename, 'r') as fopen:\n        data = fopen.readlines()\nelif file_ext in ['.pkl', '.pickle']:\n    with g_pathmgr.open(filename, 'rb') as fopen:\n        data = pickle.load(fopen, encoding='latin1')\nelif file_ext == '.npy':\n    if mmap_mode:\n        try:\n            with g_pathmgr.open(filename, 'rb') as fopen:\n                data = np.load(fopen, allow_pickle=allow_pickle, encoding='latin1', mmap_mode=mmap_mode)\n        except ValueError as e:\n            logging.info(f'Could not mmap {filename}: {e}. Trying without g_pathmgr')\n            data = np.load(filename, allow_pickle=allow_pickle, encoding='latin1', mmap_mode=mmap_mode)\n            logging.info('Successfully loaded without g_pathmgr')\n        except Exception:\n            logging.info('Could not mmap without g_pathmgr. Trying without mmap')\n            with g_pathmgr.open(filename, 'rb') as fopen:\n                data = np.load(fopen, allow_pickle=allow_pickle, encoding='latin1')\n    else:\n        with g_pathmgr.open(filename, 'rb') as fopen:\n            data = np.load(fopen, allow_pickle=allow_pickle, encoding='latin1')\nelif file_ext == '.json':\n    with g_pathmgr.open(filename, 'r') as fopen:\n        data = json.load(fopen)\nelif file_ext == '.yaml':\n    with g_pathmgr.open(filename, 'r') as fopen:\n        data = yaml.load(fopen, Loader=yaml.FullLoader)\nelif file_ext == '.csv':\n    with g_pathmgr.open(filename, 'r') as fopen:\n        data = pd.read_csv(fopen)\nelse:\n    raise Exception(f'Reading from {file_ext} is not supported yet')\nreturn data"
                },
                {
                  "name": "abspath",
                  "decorators": [],
                  "body": "'\\n    Make a path absolute, but take into account prefixes like\\n    \"http://\" or \"manifold://\"\\n    '\nregex = re.compile('^\\\\w+://')\nif regex.match(resource_path) is None:\n    return os.path.abspath(resource_path)\nelse:\n    return resource_path"
                },
                {
                  "name": "makedir",
                  "decorators": [],
                  "body": "'\\n    Create the directory if it does not exist.\\n    '\nis_success = False\ntry:\n    if not g_pathmgr.exists(dir_path):\n        g_pathmgr.mkdirs(dir_path)\n    is_success = True\nexcept BaseException:\n    logging.info(f'Error creating directory: {dir_path}')\nreturn is_success"
                },
                {
                  "name": "is_url",
                  "decorators": [],
                  "body": "'\\n    Check if an input string is a url. look for http(s):// and ignoring the case\\n    '\nis_url = re.match('^(?:http)s?://', input_url, re.IGNORECASE) is not None\nreturn is_url"
                },
                {
                  "name": "cleanup_dir",
                  "decorators": [],
                  "body": "'\\n    Utility for deleting a directory. Useful for cleaning the storage space\\n    that contains various training artifacts like checkpoints, data etc.\\n    '\nif os.path.exists(dir):\n    logging.info(f'Deleting directory: {dir}')\n    shutil.rmtree(dir)\nlogging.info(f'Deleted contents of directory: {dir}')"
                },
                {
                  "name": "get_file_size",
                  "decorators": [],
                  "body": "'\\n    Given a file, get the size of file in MB\\n    '\nsize_in_mb = os.path.getsize(filename) / float(1024 ** 2)\nreturn size_in_mb"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/common/gradcam.py",
              "type": "python",
              "imports": [
                "import numpy as np",
                "from matplotlib import pyplot as plt",
                "from scipy.ndimage import filters",
                "from skimage import transform as skimage_transform"
              ],
              "classes": [],
              "other_functions": [
                {
                  "name": "getAttMap",
                  "decorators": [],
                  "body": "attMap -= attMap.min()\nif attMap.max() > 0:\n    attMap /= attMap.max()\nattMap = skimage_transform.resize(attMap, img.shape[:2], order=3, mode='constant')\nif blur:\n    attMap = filters.gaussian_filter(attMap, 0.02 * max(img.shape[:2]))\n    attMap -= attMap.min()\n    attMap /= attMap.max()\ncmap = plt.get_cmap('jet')\nattMapV = cmap(attMap)\nattMapV = np.delete(attMapV, 3, 2)\nif overlap:\n    attMap = 1 * (1 - attMap ** 0.7).reshape(attMap.shape + (1,)) * img + (attMap ** 0.7).reshape(attMap.shape + (1,)) * attMapV\nreturn attMap"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/common/eval_utils.py",
              "type": "python",
              "imports": [
                "import argparse",
                "import numpy as np",
                "from nltk.translate.bleu_score import sentence_bleu",
                "from minigpt4.common.registry import registry",
                "from minigpt4.common.config import Config",
                "from minigpt4.datasets.builders import *",
                "from minigpt4.models import *",
                "from minigpt4.processors import *",
                "from minigpt4.runners import *",
                "from minigpt4.tasks import *"
              ],
              "classes": [],
              "other_functions": [
                {
                  "name": "eval_parser",
                  "decorators": [],
                  "body": "parser = argparse.ArgumentParser(description='Demo')\nparser.add_argument('--cfg-path', required=True, help='path to configuration file.')\nparser.add_argument('--name', type=str, default='A2', help='evaluation name')\nparser.add_argument('--ckpt', type=str, help='path to configuration file.')\nparser.add_argument('--eval_opt', type=str, default='all', help='path to configuration file.')\nparser.add_argument('--max_new_tokens', type=int, default=10, help='max number of generated tokens')\nparser.add_argument('--batch_size', type=int, default=32)\nparser.add_argument('--lora_r', type=int, default=64, help='lora rank of the model')\nparser.add_argument('--lora_alpha', type=int, default=16, help='lora alpha')\nparser.add_argument('--options', nargs='+', help='override some settings in the used config, the key-value pair in xxx=yyy format will be merged into config file (deprecate), change to --cfg-options instead.')\nreturn parser"
                },
                {
                  "name": "prepare_texts",
                  "decorators": [],
                  "body": "convs = [conv_temp.copy() for _ in range(len(texts))]\n[conv.append_message(conv.roles[0], '{}'.format(text)) for (conv, text) in zip(convs, texts)]\n[conv.append_message(conv.roles[1], None) for conv in convs]\ntexts = [conv.get_prompt() for conv in convs]\nreturn texts"
                },
                {
                  "name": "init_model",
                  "decorators": [],
                  "body": "print('Initialization Model')\ncfg = Config(args)\nmodel_config = cfg.model_cfg\nmodel_cls = registry.get_model_class(model_config.arch)\nmodel = model_cls.from_config(model_config).to('cuda:0')\nkey = list(cfg.datasets_cfg.keys())[0]\nvis_processor_cfg = cfg.datasets_cfg.get(key).vis_processor.train\nvis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\nprint('Initialization Finished')\nreturn (model, vis_processor)"
                },
                {
                  "name": "computeIoU",
                  "decorators": [],
                  "body": "(x1, y1, x2, y2) = bbox1\n(x3, y3, x4, y4) = bbox2\nintersection_x1 = max(x1, x3)\nintersection_y1 = max(y1, y3)\nintersection_x2 = min(x2, x4)\nintersection_y2 = min(y2, y4)\nintersection_area = max(0, intersection_x2 - intersection_x1 + 1) * max(0, intersection_y2 - intersection_y1 + 1)\nbbox1_area = (x2 - x1 + 1) * (y2 - y1 + 1)\nbbox2_area = (x4 - x3 + 1) * (y4 - y3 + 1)\nunion_area = bbox1_area + bbox2_area - intersection_area\niou = intersection_area / union_area\nreturn iou"
                }
              ]
            }
          ],
          "subdirs": [
            {
              "directory": "vqa_tools",
              "path": "ellama_codebase/minigpt4/common/vqa_tools",
              "files": [
                {
                  "file_name": "ellama_codebase/minigpt4/common/vqa_tools/__init__.py",
                  "type": "python",
                  "imports": [],
                  "classes": [],
                  "other_functions": []
                },
                {
                  "file_name": "ellama_codebase/minigpt4/common/vqa_tools/vqa.py",
                  "type": "python",
                  "imports": [
                    "import json",
                    "import datetime",
                    "import copy"
                  ],
                  "classes": [
                    {
                      "name": "VQA",
                      "decorators": [],
                      "inherits": [],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "'\\n        Constructor of VQA helper class for reading and visualizing questions and answers.\\n        :param annotation_file (str): location of VQA annotation file\\n        :return:\\n        '\nself.dataset = {}\nself.questions = {}\nself.qa = {}\nself.qqa = {}\nself.imgToQA = {}\nif not annotation_file == None and (not question_file == None):\n    print('loading VQA annotations and questions into memory...')\n    time_t = datetime.datetime.utcnow()\n    dataset = json.load(open(annotation_file, 'r'))\n    questions = json.load(open(question_file, 'r'))\n    self.dataset = dataset\n    self.questions = questions\n    self.createIndex()"
                        },
                        {
                          "name": "createIndex",
                          "decorators": [],
                          "body": "print('creating index...')\nimgToQA = {ann['image_id']: [] for ann in self.dataset['annotations']}\nqa = {ann['question_id']: [] for ann in self.dataset['annotations']}\nqqa = {ann['question_id']: [] for ann in self.dataset['annotations']}\nfor ann in self.dataset['annotations']:\n    imgToQA[ann['image_id']] += [ann]\n    qa[ann['question_id']] = ann\nfor ques in self.questions['questions']:\n    qqa[ques['question_id']] = ques\nprint('index created!')\nself.qa = qa\nself.qqa = qqa\nself.imgToQA = imgToQA"
                        },
                        {
                          "name": "info",
                          "decorators": [],
                          "body": "'\\n        Print information about the VQA annotation file.\\n        :return:\\n        '\nfor (key, value) in self.datset['info'].items():\n    print('%s: %s' % (key, value))"
                        },
                        {
                          "name": "getQuesIds",
                          "decorators": [],
                          "body": "'\\n        Get question ids that satisfy given filter conditions. default skips that filter\\n        :param \\timgIds    (int array)   : get question ids for given imgs\\n                        quesTypes (str array)   : get question ids for given question types\\n                        ansTypes  (str array)   : get question ids for given answer types\\n        :return:    ids   (int array)   : integer array of question ids\\n        '\nimgIds = imgIds if type(imgIds) == list else [imgIds]\nquesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\nansTypes = ansTypes if type(ansTypes) == list else [ansTypes]\nif len(imgIds) == len(quesTypes) == len(ansTypes) == 0:\n    anns = self.dataset['annotations']\nelse:\n    if not len(imgIds) == 0:\n        anns = sum([self.imgToQA[imgId] for imgId in imgIds if imgId in self.imgToQA], [])\n    else:\n        anns = self.dataset['annotations']\n    anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n    anns = anns if len(ansTypes) == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\nids = [ann['question_id'] for ann in anns]\nreturn ids"
                        },
                        {
                          "name": "getImgIds",
                          "decorators": [],
                          "body": "'\\n         Get image ids that satisfy given filter conditions. default skips that filter\\n         :param quesIds   (int array)   : get image ids for given question ids\\n        quesTypes (str array)   : get image ids for given question types\\n        ansTypes  (str array)   : get image ids for given answer types\\n         :return: ids     (int array)   : integer array of image ids\\n        '\nquesIds = quesIds if type(quesIds) == list else [quesIds]\nquesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\nansTypes = ansTypes if type(ansTypes) == list else [ansTypes]\nif len(quesIds) == len(quesTypes) == len(ansTypes) == 0:\n    anns = self.dataset['annotations']\nelse:\n    if not len(quesIds) == 0:\n        anns = sum([self.qa[quesId] for quesId in quesIds if quesId in self.qa], [])\n    else:\n        anns = self.dataset['annotations']\n    anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n    anns = anns if len(ansTypes) == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\nids = [ann['image_id'] for ann in anns]\nreturn ids"
                        },
                        {
                          "name": "loadQA",
                          "decorators": [],
                          "body": "'\\n        Load questions and answers with the specified question ids.\\n        :param ids (int array)       : integer ids specifying question ids\\n        :return: qa (object array)   : loaded qa objects\\n        '\nif type(ids) == list:\n    return [self.qa[id] for id in ids]\nelif type(ids) == int:\n    return [self.qa[ids]]"
                        },
                        {
                          "name": "showQA",
                          "decorators": [],
                          "body": "'\\n        Display the specified annotations.\\n        :param anns (array of object): annotations to display\\n        :return: None\\n        '\nif len(anns) == 0:\n    return 0\nfor ann in anns:\n    quesId = ann['question_id']\n    print('Question: %s' % self.qqa[quesId]['question'])\n    for ans in ann['answers']:\n        print('Answer %d: %s' % (ans['answer_id'], ans['answer']))"
                        },
                        {
                          "name": "loadRes",
                          "decorators": [],
                          "body": "'\\n        Load result file and return a result object.\\n        :param   resFile (str)     : file name of result file\\n        :return: res (obj)         : result api object\\n        '\nres = VQA()\nres.questions = json.load(open(quesFile))\nres.dataset['info'] = copy.deepcopy(self.questions['info'])\nres.dataset['task_type'] = copy.deepcopy(self.questions['task_type'])\nres.dataset['data_type'] = copy.deepcopy(self.questions['data_type'])\nres.dataset['data_subtype'] = copy.deepcopy(self.questions['data_subtype'])\nres.dataset['license'] = copy.deepcopy(self.questions['license'])\nprint('Loading and preparing results...     ')\ntime_t = datetime.datetime.utcnow()\nanns = json.load(open(resFile))\nassert type(anns) == list, 'results is not an array of objects'\nannsQuesIds = [ann['question_id'] for ann in anns]\nassert set(annsQuesIds) == set(self.getQuesIds()), 'Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.'\nfor ann in anns:\n    quesId = ann['question_id']\n    if res.dataset['task_type'] == 'Multiple Choice':\n        assert ann['answer'] in self.qqa[quesId]['multiple_choices'], 'predicted answer is not one of the multiple choices'\n    qaAnn = self.qa[quesId]\n    ann['image_id'] = qaAnn['image_id']\n    ann['question_type'] = qaAnn['question_type']\n    ann['answer_type'] = qaAnn['answer_type']\nprint('DONE (t=%0.2fs)' % (datetime.datetime.utcnow() - time_t).total_seconds())\nres.dataset['annotations'] = anns\nres.createIndex()\nreturn res"
                        }
                      ]
                    }
                  ],
                  "other_functions": []
                },
                {
                  "file_name": "ellama_codebase/minigpt4/common/vqa_tools/vqa_eval.py",
                  "type": "python",
                  "imports": [
                    "import sys",
                    "import re"
                  ],
                  "classes": [
                    {
                      "name": "VQAEval",
                      "decorators": [],
                      "inherits": [],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "self.n = n\nself.accuracy = {}\nself.evalQA = {}\nself.evalQuesType = {}\nself.evalAnsType = {}\nself.vqa = vqa\nself.vqaRes = vqaRes\nif vqa is not None:\n    self.params = {'question_id': vqa.getQuesIds()}\nself.contractions = {'aint': \"ain't\", 'arent': \"aren't\", 'cant': \"can't\", 'couldve': \"could've\", 'couldnt': \"couldn't\", \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", 'didnt': \"didn't\", 'doesnt': \"doesn't\", 'dont': \"don't\", 'hadnt': \"hadn't\", \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", 'hasnt': \"hasn't\", 'havent': \"haven't\", 'hed': \"he'd\", \"hed've\": \"he'd've\", \"he'dve\": \"he'd've\", 'hes': \"he's\", 'howd': \"how'd\", 'howll': \"how'll\", 'hows': \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", 'Im': \"I'm\", 'Ive': \"I've\", 'isnt': \"isn't\", 'itd': \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", 'itll': \"it'll\", \"let's\": \"let's\", 'maam': \"ma'am\", 'mightnt': \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", 'mightve': \"might've\", 'mustnt': \"mustn't\", 'mustve': \"must've\", 'neednt': \"needn't\", 'notve': \"not've\", 'oclock': \"o'clock\", 'oughtnt': \"oughtn't\", \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", 'shant': \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \"she's\": \"she's\", 'shouldve': \"should've\", 'shouldnt': \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \"somebody'd\": 'somebodyd', \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", 'somebodyll': \"somebody'll\", 'somebodys': \"somebody's\", 'someoned': \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", 'someonell': \"someone'll\", 'someones': \"someone's\", 'somethingd': \"something'd\", \"somethingd've\": \"something'd've\", \"something'dve\": \"something'd've\", 'somethingll': \"something'll\", 'thats': \"that's\", 'thered': \"there'd\", \"thered've\": \"there'd've\", \"there'dve\": \"there'd've\", 'therere': \"there're\", 'theres': \"there's\", 'theyd': \"they'd\", \"theyd've\": \"they'd've\", \"they'dve\": \"they'd've\", 'theyll': \"they'll\", 'theyre': \"they're\", 'theyve': \"they've\", 'twas': \"'twas\", 'wasnt': \"wasn't\", \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", 'weve': \"we've\", 'werent': \"weren't\", 'whatll': \"what'll\", 'whatre': \"what're\", 'whats': \"what's\", 'whatve': \"what've\", 'whens': \"when's\", 'whered': \"where'd\", 'wheres': \"where's\", 'whereve': \"where've\", 'whod': \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", 'wholl': \"who'll\", 'whos': \"who's\", 'whove': \"who've\", 'whyll': \"why'll\", 'whyre': \"why're\", 'whys': \"why's\", 'wont': \"won't\", 'wouldve': \"would've\", 'wouldnt': \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \"wouldn'tve\": \"wouldn't've\", 'yall': \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", 'youd': \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", 'youll': \"you'll\", 'youre': \"you're\", 'youve': \"you've\"}\nself.manualMap = {'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\nself.articles = ['a', 'an', 'the']\nself.periodStrip = re.compile('(?!<=\\\\d)(\\\\.)(?!\\\\d)')\nself.commaStrip = re.compile('(\\\\d)(,)(\\\\d)')\nself.punct = [';', '/', '[', ']', '\"', '{', '}', '(', ')', '=', '+', '\\\\', '_', '-', '>', '<', '@', '`', ',', '?', '!']"
                        },
                        {
                          "name": "evaluate",
                          "decorators": [],
                          "body": "if quesIds == None:\n    quesIds = [quesId for quesId in self.params['question_id']]\ngts = {}\nres = {}\nfor quesId in quesIds:\n    gts[quesId] = self.vqa.qa[quesId]\n    res[quesId] = self.vqaRes.qa[quesId]\naccQA = []\naccQuesType = {}\naccAnsType = {}\nprint('computing accuracy')\nstep = 0\nfor quesId in quesIds:\n    resAns = res[quesId]['answer']\n    resAns = resAns.replace('\\n', ' ')\n    resAns = resAns.replace('\\t', ' ')\n    resAns = resAns.strip()\n    resAns = self.processPunctuation(resAns)\n    resAns = self.processDigitArticle(resAns)\n    gtAcc = []\n    gtAnswers = [ans['answer'] for ans in gts[quesId]['answers']]\n    if len(set(gtAnswers)) > 1:\n        for ansDic in gts[quesId]['answers']:\n            ansDic['answer'] = self.processPunctuation(ansDic['answer'])\n    for gtAnsDatum in gts[quesId]['answers']:\n        otherGTAns = [item for item in gts[quesId]['answers'] if item != gtAnsDatum]\n        matchingAns = [item for item in otherGTAns if item['answer'] == resAns]\n        acc = min(1, float(len(matchingAns)) / 3)\n        gtAcc.append(acc)\n    quesType = gts[quesId]['question_type']\n    ansType = gts[quesId]['answer_type']\n    avgGTAcc = float(sum(gtAcc)) / len(gtAcc)\n    accQA.append(avgGTAcc)\n    if quesType not in accQuesType:\n        accQuesType[quesType] = []\n    accQuesType[quesType].append(avgGTAcc)\n    if ansType not in accAnsType:\n        accAnsType[ansType] = []\n    accAnsType[ansType].append(avgGTAcc)\n    self.setEvalQA(quesId, avgGTAcc)\n    self.setEvalQuesType(quesId, quesType, avgGTAcc)\n    self.setEvalAnsType(quesId, ansType, avgGTAcc)\n    if step % 100 == 0:\n        self.updateProgress(step / float(len(quesIds)))\n    step = step + 1\nself.setAccuracy(accQA, accQuesType, accAnsType)\nprint('Done computing accuracy')"
                        },
                        {
                          "name": "processPunctuation",
                          "decorators": [],
                          "body": "outText = inText\nfor p in self.punct:\n    if (p + ' ' in inText or ' ' + p in inText) or re.search(self.commaStrip, inText) != None:\n        outText = outText.replace(p, '')\n    else:\n        outText = outText.replace(p, ' ')\noutText = self.periodStrip.sub('', outText, re.UNICODE)\nreturn outText"
                        },
                        {
                          "name": "processDigitArticle",
                          "decorators": [],
                          "body": "outText = []\ntempText = inText.lower().split()\nfor word in tempText:\n    word = self.manualMap.setdefault(word, word)\n    if word not in self.articles:\n        outText.append(word)\n    else:\n        pass\nfor (wordId, word) in enumerate(outText):\n    if word in self.contractions:\n        outText[wordId] = self.contractions[word]\noutText = ' '.join(outText)\nreturn outText"
                        },
                        {
                          "name": "setAccuracy",
                          "decorators": [],
                          "body": "self.accuracy['overall'] = round(100 * float(sum(accQA)) / len(accQA), self.n)\nself.accuracy['perQuestionType'] = {quesType: round(100 * float(sum(accQuesType[quesType])) / len(accQuesType[quesType]), self.n) for quesType in accQuesType}\nself.accuracy['perAnswerType'] = {ansType: round(100 * float(sum(accAnsType[ansType])) / len(accAnsType[ansType]), self.n) for ansType in accAnsType}"
                        },
                        {
                          "name": "setEvalQA",
                          "decorators": [],
                          "body": "self.evalQA[quesId] = round(100 * acc, self.n)"
                        },
                        {
                          "name": "setEvalQuesType",
                          "decorators": [],
                          "body": "if quesType not in self.evalQuesType:\n    self.evalQuesType[quesType] = {}\nself.evalQuesType[quesType][quesId] = round(100 * acc, self.n)"
                        },
                        {
                          "name": "setEvalAnsType",
                          "decorators": [],
                          "body": "if ansType not in self.evalAnsType:\n    self.evalAnsType[ansType] = {}\nself.evalAnsType[ansType][quesId] = round(100 * acc, self.n)"
                        },
                        {
                          "name": "updateProgress",
                          "decorators": [],
                          "body": "barLength = 20\nstatus = ''\nif isinstance(progress, int):\n    progress = float(progress)\nif not isinstance(progress, float):\n    progress = 0\n    status = 'error: progress var must be float\\r\\n'\nif progress < 0:\n    progress = 0\n    status = 'Halt...\\r\\n'\nif progress >= 1:\n    progress = 1\n    status = 'Done...\\r\\n'\nblock = int(round(barLength * progress))\ntext = '\\rFinshed Percent: [{0}] {1}% {2}'.format('#' * block + '-' * (barLength - block), int(progress * 100), status)\nsys.stdout.write(text)\nsys.stdout.flush()"
                        }
                      ]
                    }
                  ],
                  "other_functions": []
                }
              ],
              "subdirs": [
                {
                  "directory": "VQA",
                  "path": "ellama_codebase/minigpt4/common/vqa_tools/VQA",
                  "files": [],
                  "subdirs": [
                    {
                      "directory": "PythonHelperTools",
                      "path": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonHelperTools",
                      "files": [
                        {
                          "file_name": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonHelperTools/vqaDemo.py",
                          "type": "python",
                          "imports": [
                            "from vqaTools.vqa import VQA",
                            "import random",
                            "import skimage.io as io",
                            "import matplotlib.pyplot as plt",
                            "import os"
                          ],
                          "classes": [],
                          "other_functions": []
                        }
                      ],
                      "subdirs": [
                        {
                          "directory": "vqaTools",
                          "path": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonHelperTools/vqaTools",
                          "files": [
                            {
                              "file_name": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonHelperTools/vqaTools/__init__.py",
                              "type": "python",
                              "imports": [],
                              "classes": [],
                              "other_functions": []
                            },
                            {
                              "file_name": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonHelperTools/vqaTools/vqa.py",
                              "type": "python",
                              "imports": [
                                "import json",
                                "import datetime",
                                "import copy"
                              ],
                              "classes": [
                                {
                                  "name": "VQA",
                                  "decorators": [],
                                  "inherits": [],
                                  "methods": [
                                    {
                                      "name": "__init__",
                                      "decorators": [],
                                      "body": "'\\n           Constructor of VQA helper class for reading and visualizing questions and answers.\\n        :param annotation_file (str): location of VQA annotation file\\n        :return:\\n        '\nself.dataset = {}\nself.questions = {}\nself.qa = {}\nself.qqa = {}\nself.imgToQA = {}\nif not annotation_file == None and (not question_file == None):\n    time_t = datetime.datetime.utcnow()\n    dataset = json.load(open(annotation_file, 'r'))\n    questions = json.load(open(question_file, 'r'))\n    self.dataset = dataset\n    self.questions = questions\n    self.createIndex()"
                                    },
                                    {
                                      "name": "createIndex",
                                      "decorators": [],
                                      "body": "imgToQA = {ann['image_id']: [] for ann in self.dataset['annotations']}\nqa = {ann['question_id']: [] for ann in self.dataset['annotations']}\nqqa = {ann['question_id']: [] for ann in self.dataset['annotations']}\nfor ann in self.dataset['annotations']:\n    imgToQA[ann['image_id']] += [ann]\n    qa[ann['question_id']] = ann\nfor ques in self.questions['questions']:\n    qqa[ques['question_id']] = ques\nself.qa = qa\nself.qqa = qqa\nself.imgToQA = imgToQA"
                                    },
                                    {
                                      "name": "info",
                                      "decorators": [],
                                      "body": "'\\n        Print information about the VQA annotation file.\\n        :return:\\n        '"
                                    },
                                    {
                                      "name": "getQuesIds",
                                      "decorators": [],
                                      "body": "'\\n        Get question ids that satisfy given filter conditions. default skips that filter\\n        :param \\timgIds    (int array)   : get question ids for given imgs\\n                quesTypes (str array)   : get question ids for given question types\\n                ansTypes  (str array)   : get question ids for given answer types\\n        :return:    ids   (int array)   : integer array of question ids\\n        '\nimgIds = imgIds if type(imgIds) == list else [imgIds]\nquesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\nansTypes = ansTypes if type(ansTypes) == list else [ansTypes]\nif len(imgIds) == len(quesTypes) == len(ansTypes) == 0:\n    anns = self.dataset['annotations']\nelse:\n    if not len(imgIds) == 0:\n        anns = sum([self.imgToQA[imgId] for imgId in imgIds if imgId in self.imgToQA], [])\n    else:\n        anns = self.dataset['annotations']\n    anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n    anns = anns if len(ansTypes) == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\nids = [ann['question_id'] for ann in anns]\nreturn ids"
                                    },
                                    {
                                      "name": "getImgIds",
                                      "decorators": [],
                                      "body": "'\\n        Get image ids that satisfy given filter conditions. default skips that filter\\n        :param quesIds   (int array)   : get image ids for given question ids\\n               quesTypes (str array)   : get image ids for given question types\\n               ansTypes  (str array)   : get image ids for given answer types\\n        :return: ids     (int array)   : integer array of image ids\\n        '\nquesIds = quesIds if type(quesIds) == list else [quesIds]\nquesTypes = quesTypes if type(quesTypes) == list else [quesTypes]\nansTypes = ansTypes if type(ansTypes) == list else [ansTypes]\nif len(quesIds) == len(quesTypes) == len(ansTypes) == 0:\n    anns = self.dataset['annotations']\nelse:\n    if not len(quesIds) == 0:\n        anns = sum([self.qa[quesId] for quesId in quesIds if quesId in self.qa], [])\n    else:\n        anns = self.dataset['annotations']\n    anns = anns if len(quesTypes) == 0 else [ann for ann in anns if ann['question_type'] in quesTypes]\n    anns = anns if len(ansTypes) == 0 else [ann for ann in anns if ann['answer_type'] in ansTypes]\nids = [ann['image_id'] for ann in anns]\nreturn ids"
                                    },
                                    {
                                      "name": "loadQA",
                                      "decorators": [],
                                      "body": "'\\n        Load questions and answers with the specified question ids.\\n        :param ids (int array)       : integer ids specifying question ids\\n        :return: qa (object array)   : loaded qa objects\\n        '\nif type(ids) == list:\n    return [self.qa[id] for id in ids]\nelif type(ids) == int:\n    return [self.qa[ids]]"
                                    },
                                    {
                                      "name": "showQA",
                                      "decorators": [],
                                      "body": "'\\n        Display the specified annotations.\\n        :param anns (array of object): annotations to display\\n        :return: None\\n        '\nif len(anns) == 0:\n    return 0\nfor ann in anns:\n    quesId = ann['question_id']\n    print('Question: %s' % self.qqa[quesId]['question'])\n    for ans in ann['answers']:\n        print('Answer %d: %s' % (ans['answer_id'], ans['answer']))"
                                    },
                                    {
                                      "name": "loadRes",
                                      "decorators": [],
                                      "body": "'\\n        Load result file and return a result object.\\n        :param   resFile (str)     : file name of result file\\n        :return: res (obj)         : result api object\\n        '\nres = VQA()\nres.questions = json.load(open(quesFile))\nres.dataset['info'] = copy.deepcopy(self.questions['info'])\nres.dataset['task_type'] = copy.deepcopy(self.questions['task_type'])\nres.dataset['data_type'] = copy.deepcopy(self.questions['data_type'])\nres.dataset['data_subtype'] = copy.deepcopy(self.questions['data_subtype'])\nres.dataset['license'] = copy.deepcopy(self.questions['license'])\ntime_t = datetime.datetime.utcnow()\nanns = json.load(open(resFile))\nassert type(anns) == list, 'results is not an array of objects'\nannsQuesIds = [ann['question_id'] for ann in anns]\nassert set(annsQuesIds) == set(self.getQuesIds()), 'Results do not correspond to current VQA set. Either the results do not have predictions for all question ids in annotation file or there is atleast one question id that does not belong to the question ids in the annotation file.'\nfor ann in anns:\n    quesId = ann['question_id']\n    if res.dataset['task_type'] == 'Multiple Choice':\n        assert ann['answer'] in self.qqa[quesId]['multiple_choices'], 'predicted answer is not one of the multiple choices'\n    qaAnn = self.qa[quesId]\n    ann['image_id'] = qaAnn['image_id']\n    ann['question_type'] = qaAnn['question_type']\n    ann['answer_type'] = qaAnn['answer_type']\nres.dataset['annotations'] = anns\nres.createIndex()\nreturn res"
                                    }
                                  ]
                                }
                              ],
                              "other_functions": []
                            }
                          ],
                          "subdirs": []
                        }
                      ]
                    },
                    {
                      "directory": "PythonEvaluationTools",
                      "path": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonEvaluationTools",
                      "files": [],
                      "subdirs": [
                        {
                          "directory": "vqaEvaluation",
                          "path": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonEvaluationTools/vqaEvaluation",
                          "files": [
                            {
                              "file_name": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonEvaluationTools/vqaEvaluation/vqaEval.py",
                              "type": "python",
                              "imports": [
                                "import re",
                                "import sys"
                              ],
                              "classes": [
                                {
                                  "name": "VQAEval",
                                  "decorators": [],
                                  "inherits": [],
                                  "methods": [
                                    {
                                      "name": "__init__",
                                      "decorators": [],
                                      "body": "self.n = n\nself.accuracy = {}\nself.evalQA = {}\nself.evalQuesType = {}\nself.evalAnsType = {}\nself.vqa = vqa\nself.vqaRes = vqaRes\nself.params = {'question_id': vqa.getQuesIds()}\nself.contractions = {'aint': \"ain't\", 'arent': \"aren't\", 'cant': \"can't\", 'couldve': \"could've\", 'couldnt': \"couldn't\", \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", 'didnt': \"didn't\", 'doesnt': \"doesn't\", 'dont': \"don't\", 'hadnt': \"hadn't\", \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", 'hasnt': \"hasn't\", 'havent': \"haven't\", 'hed': \"he'd\", \"hed've\": \"he'd've\", \"he'dve\": \"he'd've\", 'hes': \"he's\", 'howd': \"how'd\", 'howll': \"how'll\", 'hows': \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", 'Im': \"I'm\", 'Ive': \"I've\", 'isnt': \"isn't\", 'itd': \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", 'itll': \"it'll\", \"let's\": \"let's\", 'maam': \"ma'am\", 'mightnt': \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", 'mightve': \"might've\", 'mustnt': \"mustn't\", 'mustve': \"must've\", 'neednt': \"needn't\", 'notve': \"not've\", 'oclock': \"o'clock\", 'oughtnt': \"oughtn't\", \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", 'shant': \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \"she's\": \"she's\", 'shouldve': \"should've\", 'shouldnt': \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \"somebody'd\": 'somebodyd', \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", 'somebodyll': \"somebody'll\", 'somebodys': \"somebody's\", 'someoned': \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", 'someonell': \"someone'll\", 'someones': \"someone's\", 'somethingd': \"something'd\", \"somethingd've\": \"something'd've\", \"something'dve\": \"something'd've\", 'somethingll': \"something'll\", 'thats': \"that's\", 'thered': \"there'd\", \"thered've\": \"there'd've\", \"there'dve\": \"there'd've\", 'therere': \"there're\", 'theres': \"there's\", 'theyd': \"they'd\", \"theyd've\": \"they'd've\", \"they'dve\": \"they'd've\", 'theyll': \"they'll\", 'theyre': \"they're\", 'theyve': \"they've\", 'twas': \"'twas\", 'wasnt': \"wasn't\", \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", 'weve': \"we've\", 'werent': \"weren't\", 'whatll': \"what'll\", 'whatre': \"what're\", 'whats': \"what's\", 'whatve': \"what've\", 'whens': \"when's\", 'whered': \"where'd\", 'wheres': \"where's\", 'whereve': \"where've\", 'whod': \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", 'wholl': \"who'll\", 'whos': \"who's\", 'whove': \"who've\", 'whyll': \"why'll\", 'whyre': \"why're\", 'whys': \"why's\", 'wont': \"won't\", 'wouldve': \"would've\", 'wouldnt': \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \"wouldn'tve\": \"wouldn't've\", 'yall': \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", 'youd': \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", 'youll': \"you'll\", 'youre': \"you're\", 'youve': \"you've\"}\nself.manualMap = {'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\nself.articles = ['a', 'an', 'the']\nself.periodStrip = re.compile('(?!<=\\\\d)(\\\\.)(?!\\\\d)')\nself.commaStrip = re.compile('(\\\\d)(\\\\,)(\\\\d)')\nself.punct = [';', '/', '[', ']', '\"', '{', '}', '(', ')', '=', '+', '\\\\', '_', '-', '>', '<', '@', '`', ',', '?', '!']"
                                    },
                                    {
                                      "name": "evaluate",
                                      "decorators": [],
                                      "body": "if quesIds == None:\n    quesIds = [quesId for quesId in self.params['question_id']]\ngts = {}\nres = {}\nfor quesId in quesIds:\n    gts[quesId] = self.vqa.qa[quesId]\n    res[quesId] = self.vqaRes.qa[quesId]\naccQA = []\naccQuesType = {}\naccAnsType = {}\nstep = 0\nfor quesId in quesIds:\n    for ansDic in gts[quesId]['answers']:\n        ansDic['answer'] = ansDic['answer'].replace('\\n', ' ')\n        ansDic['answer'] = ansDic['answer'].replace('\\t', ' ')\n        ansDic['answer'] = ansDic['answer'].strip()\n    resAns = res[quesId]['answer']\n    resAns = resAns.replace('\\n', ' ')\n    resAns = resAns.replace('\\t', ' ')\n    resAns = resAns.strip()\n    gtAcc = []\n    gtAnswers = [ans['answer'] for ans in gts[quesId]['answers']]\n    if len(set(gtAnswers)) > 1:\n        for ansDic in gts[quesId]['answers']:\n            ansDic['answer'] = self.processPunctuation(ansDic['answer'])\n            ansDic['answer'] = self.processDigitArticle(ansDic['answer'])\n        resAns = self.processPunctuation(resAns)\n        resAns = self.processDigitArticle(resAns)\n    for gtAnsDatum in gts[quesId]['answers']:\n        otherGTAns = [item for item in gts[quesId]['answers'] if item != gtAnsDatum]\n        matchingAns = [item for item in otherGTAns if item['answer'].lower() == resAns.lower()]\n        acc = min(1, float(len(matchingAns)) / 3)\n        gtAcc.append(acc)\n    quesType = gts[quesId]['question_type']\n    ansType = gts[quesId]['answer_type']\n    avgGTAcc = float(sum(gtAcc)) / len(gtAcc)\n    accQA.append(avgGTAcc)\n    if quesType not in accQuesType:\n        accQuesType[quesType] = []\n    accQuesType[quesType].append(avgGTAcc)\n    if ansType not in accAnsType:\n        accAnsType[ansType] = []\n    accAnsType[ansType].append(avgGTAcc)\n    self.setEvalQA(quesId, avgGTAcc)\n    self.setEvalQuesType(quesId, quesType, avgGTAcc)\n    self.setEvalAnsType(quesId, ansType, avgGTAcc)\n    if step % 100 == 0:\n        self.updateProgress(step / float(len(quesIds)))\n    step = step + 1\nself.setAccuracy(accQA, accQuesType, accAnsType)"
                                    },
                                    {
                                      "name": "processPunctuation",
                                      "decorators": [],
                                      "body": "outText = inText\nfor p in self.punct:\n    if (p + ' ' in inText or ' ' + p in inText) or re.search(self.commaStrip, inText) != None:\n        outText = outText.replace(p, '')\n    else:\n        outText = outText.replace(p, ' ')\noutText = self.periodStrip.sub('', outText, re.UNICODE)\nreturn outText"
                                    },
                                    {
                                      "name": "processDigitArticle",
                                      "decorators": [],
                                      "body": "outText = []\ntempText = inText.lower().split()\nfor word in tempText:\n    word = self.manualMap.setdefault(word, word)\n    if word not in self.articles:\n        outText.append(word)\n    else:\n        pass\nfor (wordId, word) in enumerate(outText):\n    if word in self.contractions:\n        outText[wordId] = self.contractions[word]\noutText = ' '.join(outText)\nreturn outText"
                                    },
                                    {
                                      "name": "setAccuracy",
                                      "decorators": [],
                                      "body": "self.accuracy['overall'] = round(100 * float(sum(accQA)) / len(accQA), self.n)\nself.accuracy['perQuestionType'] = {quesType: round(100 * float(sum(accQuesType[quesType])) / len(accQuesType[quesType]), self.n) for quesType in accQuesType}\nself.accuracy['perAnswerType'] = {ansType: round(100 * float(sum(accAnsType[ansType])) / len(accAnsType[ansType]), self.n) for ansType in accAnsType}"
                                    },
                                    {
                                      "name": "setEvalQA",
                                      "decorators": [],
                                      "body": "self.evalQA[quesId] = round(100 * acc, self.n)"
                                    },
                                    {
                                      "name": "setEvalQuesType",
                                      "decorators": [],
                                      "body": "if quesType not in self.evalQuesType:\n    self.evalQuesType[quesType] = {}\nself.evalQuesType[quesType][quesId] = round(100 * acc, self.n)"
                                    },
                                    {
                                      "name": "setEvalAnsType",
                                      "decorators": [],
                                      "body": "if ansType not in self.evalAnsType:\n    self.evalAnsType[ansType] = {}\nself.evalAnsType[ansType][quesId] = round(100 * acc, self.n)"
                                    },
                                    {
                                      "name": "updateProgress",
                                      "decorators": [],
                                      "body": "barLength = 20\nstatus = ''\nif isinstance(progress, int):\n    progress = float(progress)\nif not isinstance(progress, float):\n    progress = 0\n    status = 'error: progress var must be float\\r\\n'\nif progress < 0:\n    progress = 0\n    status = 'Halt...\\r\\n'\nif progress >= 1:\n    progress = 1\n    status = 'Done...\\r\\n'\nblock = int(round(barLength * progress))\ntext = '\\rFinshed Percent: [{0}] {1}% {2}'.format('#' * block + '-' * (barLength - block), int(progress * 100), status)\nsys.stdout.write(text)\nsys.stdout.flush()"
                                    }
                                  ]
                                }
                              ],
                              "other_functions": []
                            },
                            {
                              "file_name": "ellama_codebase/minigpt4/common/vqa_tools/VQA/PythonEvaluationTools/vqaEvaluation/__init__.py",
                              "type": "python",
                              "imports": [],
                              "classes": [],
                              "other_functions": []
                            }
                          ],
                          "subdirs": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "directory": "configs",
          "path": "ellama_codebase/minigpt4/configs",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/configs/default.yaml",
              "type": "yaml",
              "content": {
                "env": {
                  "cache_root": "/export/home/.cache/minigpt4"
                }
              }
            }
          ],
          "subdirs": [
            {
              "directory": "models",
              "path": "ellama_codebase/minigpt4/configs/models",
              "files": [
                {
                  "file_name": "ellama_codebase/minigpt4/configs/models/minigpt_v2.yaml",
                  "type": "yaml",
                  "content": {
                    "model": {
                      "arch": "minigpt_v2",
                      "image_size": 448,
                      "drop_path_rate": 0,
                      "use_grad_checkpoint": false,
                      "vit_precision": "fp16",
                      "freeze_vit": true,
                      "prompt": "",
                      "llama_model": "checkpoints/Llama-2-7b-chat-hf",
                      "lora_r": 64,
                      "lora_alpha": 16
                    },
                    "preprocess": {
                      "vis_processor": {
                        "train": {
                          "name": "blip2_image_train",
                          "image_size": 448
                        },
                        "eval": {
                          "name": "blip2_image_eval",
                          "image_size": 448
                        }
                      },
                      "text_processor": {
                        "train": {
                          "name": "blip_caption"
                        },
                        "eval": {
                          "name": "blip_caption"
                        }
                      }
                    }
                  }
                },
                {
                  "file_name": "ellama_codebase/minigpt4/configs/models/minigpt4_vicuna0.yaml",
                  "type": "yaml",
                  "content": {
                    "model": {
                      "arch": "minigpt4",
                      "image_size": 224,
                      "drop_path_rate": 0,
                      "use_grad_checkpoint": false,
                      "vit_precision": "fp16",
                      "freeze_vit": true,
                      "freeze_qformer": true,
                      "num_query_token": 32,
                      "prompt": "",
                      "llama_model": "please set this value to the path of vicuna model"
                    },
                    "preprocess": {
                      "vis_processor": {
                        "train": {
                          "name": "blip2_image_train",
                          "image_size": 224
                        },
                        "eval": {
                          "name": "blip2_image_eval",
                          "image_size": 224
                        }
                      },
                      "text_processor": {
                        "train": {
                          "name": "blip_caption"
                        },
                        "eval": {
                          "name": "blip_caption"
                        }
                      }
                    }
                  }
                },
                {
                  "file_name": "ellama_codebase/minigpt4/configs/models/minigpt4_llama2.yaml",
                  "type": "yaml",
                  "content": {
                    "model": {
                      "arch": "minigpt4",
                      "image_size": 224,
                      "drop_path_rate": 0,
                      "use_grad_checkpoint": false,
                      "vit_precision": "fp16",
                      "freeze_vit": true,
                      "has_qformer": false,
                      "prompt": "",
                      "llama_model": "please set this value to the path of llama2-chat-7b"
                    },
                    "preprocess": {
                      "vis_processor": {
                        "train": {
                          "name": "blip2_image_train",
                          "image_size": 224
                        },
                        "eval": {
                          "name": "blip2_image_eval",
                          "image_size": 224
                        }
                      },
                      "text_processor": {
                        "train": {
                          "name": "blip_caption"
                        },
                        "eval": {
                          "name": "blip_caption"
                        }
                      }
                    }
                  }
                }
              ],
              "subdirs": []
            },
            {
              "directory": "datasets",
              "path": "ellama_codebase/minigpt4/configs/datasets",
              "files": [],
              "subdirs": [
                {
                  "directory": "firstface",
                  "path": "ellama_codebase/minigpt4/configs/datasets/firstface",
                  "files": [
                    {
                      "file_name": "ellama_codebase/minigpt4/configs/datasets/firstface/featureface.yaml",
                      "type": "yaml",
                      "content": {
                        "datasets": {
                          "feature_face_caption": {
                            "data_type": "images",
                            "build_info": {
                              "image_path": "/home/user/selected_face/first_face/first_frames",
                              "ann_path": "/home/user/selected_face/face_emotion/relative_train_NCEV.txt"
                            }
                          }
                        }
                      }
                    }
                  ],
                  "subdirs": []
                }
              ]
            }
          ]
        },
        {
          "directory": "models",
          "path": "ellama_codebase/minigpt4/models",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/models/Qformer.py",
              "type": "python",
              "imports": [
                "import math",
                "import os",
                "import warnings",
                "from dataclasses import dataclass",
                "from typing import Optional, Tuple, Dict, Any",
                "import torch",
                "from torch import Tensor, device, dtype, nn",
                "import torch.utils.checkpoint",
                "from torch import nn",
                "from torch.nn import CrossEntropyLoss",
                "import torch.nn.functional as F",
                "from transformers.activations import ACT2FN",
                "from transformers.file_utils import ModelOutput",
                "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions, CausalLMOutputWithCrossAttentions, MaskedLMOutput, MultipleChoiceModelOutput, NextSentencePredictorOutput, QuestionAnsweringModelOutput, SequenceClassifierOutput, TokenClassifierOutput",
                "from transformers.modeling_utils import PreTrainedModel, apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer",
                "from transformers.utils import logging",
                "from transformers.models.bert.configuration_bert import BertConfig"
              ],
              "classes": [
                {
                  "name": "BertEmbeddings",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\nself.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\nself.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\nself.dropout = nn.Dropout(config.hidden_dropout_prob)\nself.register_buffer('position_ids', torch.arange(config.max_position_embeddings).expand((1, -1)))\nself.position_embedding_type = getattr(config, 'position_embedding_type', 'absolute')\nself.config = config"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "if input_ids is not None:\n    seq_length = input_ids.size()[1]\nelse:\n    seq_length = 0\nif position_ids is None:\n    position_ids = self.position_ids[:, past_key_values_length:seq_length + past_key_values_length].clone()\nif input_ids is not None:\n    embeddings = self.word_embeddings(input_ids)\n    if self.position_embedding_type == 'absolute':\n        position_embeddings = self.position_embeddings(position_ids)\n        embeddings = embeddings + position_embeddings\n    if query_embeds is not None:\n        embeddings = torch.cat((query_embeds, embeddings), dim=1)\nelse:\n    embeddings = query_embeds\nembeddings = self.LayerNorm(embeddings)\nembeddings = self.dropout(embeddings)\nreturn embeddings"
                    }
                  ]
                },
                {
                  "name": "BertSelfAttention",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.config = config\nif config.hidden_size % config.num_attention_heads != 0 and (not hasattr(config, 'embedding_size')):\n    raise ValueError('The hidden size (%d) is not a multiple of the number of attention heads (%d)' % (config.hidden_size, config.num_attention_heads))\nself.num_attention_heads = config.num_attention_heads\nself.attention_head_size = int(config.hidden_size / config.num_attention_heads)\nself.all_head_size = self.num_attention_heads * self.attention_head_size\nself.query = nn.Linear(config.hidden_size, self.all_head_size)\nif is_cross_attention:\n    self.key = nn.Linear(config.encoder_width, self.all_head_size)\n    self.value = nn.Linear(config.encoder_width, self.all_head_size)\nelse:\n    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n    self.value = nn.Linear(config.hidden_size, self.all_head_size)\nself.dropout = nn.Dropout(config.attention_probs_dropout_prob)\nself.position_embedding_type = getattr(config, 'position_embedding_type', 'absolute')\nif self.position_embedding_type == 'relative_key' or self.position_embedding_type == 'relative_key_query':\n    self.max_position_embeddings = config.max_position_embeddings\n    self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\nself.save_attention = False"
                    },
                    {
                      "name": "save_attn_gradients",
                      "decorators": [],
                      "body": "self.attn_gradients = attn_gradients"
                    },
                    {
                      "name": "get_attn_gradients",
                      "decorators": [],
                      "body": "return self.attn_gradients"
                    },
                    {
                      "name": "save_attention_map",
                      "decorators": [],
                      "body": "self.attention_map = attention_map"
                    },
                    {
                      "name": "get_attention_map",
                      "decorators": [],
                      "body": "return self.attention_map"
                    },
                    {
                      "name": "transpose_for_scores",
                      "decorators": [],
                      "body": "new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\nx = x.view(*new_x_shape)\nreturn x.permute(0, 2, 1, 3)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "is_cross_attention = encoder_hidden_states is not None\nif is_cross_attention:\n    key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n    value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n    attention_mask = encoder_attention_mask\nelif past_key_value is not None:\n    key_layer = self.transpose_for_scores(self.key(hidden_states))\n    value_layer = self.transpose_for_scores(self.value(hidden_states))\n    key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n    value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\nelse:\n    key_layer = self.transpose_for_scores(self.key(hidden_states))\n    value_layer = self.transpose_for_scores(self.value(hidden_states))\nmixed_query_layer = self.query(hidden_states)\nquery_layer = self.transpose_for_scores(mixed_query_layer)\npast_key_value = (key_layer, value_layer)\nattention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\nif self.position_embedding_type == 'relative_key' or self.position_embedding_type == 'relative_key_query':\n    seq_length = hidden_states.size()[1]\n    position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n    position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n    distance = position_ids_l - position_ids_r\n    positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n    positional_embedding = positional_embedding.to(dtype=query_layer.dtype)\n    if self.position_embedding_type == 'relative_key':\n        relative_position_scores = torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)\n        attention_scores = attention_scores + relative_position_scores\n    elif self.position_embedding_type == 'relative_key_query':\n        relative_position_scores_query = torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)\n        relative_position_scores_key = torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)\n        attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\nattention_scores = attention_scores / math.sqrt(self.attention_head_size)\nif attention_mask is not None:\n    attention_scores = attention_scores + attention_mask\nattention_probs = nn.Softmax(dim=-1)(attention_scores)\nif is_cross_attention and self.save_attention:\n    self.save_attention_map(attention_probs)\n    attention_probs.register_hook(self.save_attn_gradients)\nattention_probs_dropped = self.dropout(attention_probs)\nif head_mask is not None:\n    attention_probs_dropped = attention_probs_dropped * head_mask\ncontext_layer = torch.matmul(attention_probs_dropped, value_layer)\ncontext_layer = context_layer.permute(0, 2, 1, 3).contiguous()\nnew_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\ncontext_layer = context_layer.view(*new_context_layer_shape)\noutputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\noutputs = outputs + (past_key_value,)\nreturn outputs"
                    }
                  ]
                },
                {
                  "name": "BertSelfOutput",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.dense = nn.Linear(config.hidden_size, config.hidden_size)\nself.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\nself.dropout = nn.Dropout(config.hidden_dropout_prob)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "hidden_states = self.dense(hidden_states)\nhidden_states = self.dropout(hidden_states)\nhidden_states = self.LayerNorm(hidden_states + input_tensor)\nreturn hidden_states"
                    }
                  ]
                },
                {
                  "name": "BertAttention",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.self = BertSelfAttention(config, is_cross_attention)\nself.output = BertSelfOutput(config)\nself.pruned_heads = set()"
                    },
                    {
                      "name": "prune_heads",
                      "decorators": [],
                      "body": "if len(heads) == 0:\n    return\n(heads, index) = find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)\nself.self.query = prune_linear_layer(self.self.query, index)\nself.self.key = prune_linear_layer(self.self.key, index)\nself.self.value = prune_linear_layer(self.self.value, index)\nself.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\nself.self.num_attention_heads = self.self.num_attention_heads - len(heads)\nself.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\nself.pruned_heads = self.pruned_heads.union(heads)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "self_outputs = self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\nattention_output = self.output(self_outputs[0], hidden_states)\noutputs = (attention_output,) + self_outputs[1:]\nreturn outputs"
                    }
                  ]
                },
                {
                  "name": "BertIntermediate",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.dense = nn.Linear(config.hidden_size, config.intermediate_size)\nif isinstance(config.hidden_act, str):\n    self.intermediate_act_fn = ACT2FN[config.hidden_act]\nelse:\n    self.intermediate_act_fn = config.hidden_act"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "hidden_states = self.dense(hidden_states)\nhidden_states = self.intermediate_act_fn(hidden_states)\nreturn hidden_states"
                    }
                  ]
                },
                {
                  "name": "BertOutput",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.dense = nn.Linear(config.intermediate_size, config.hidden_size)\nself.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\nself.dropout = nn.Dropout(config.hidden_dropout_prob)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "hidden_states = self.dense(hidden_states)\nhidden_states = self.dropout(hidden_states)\nhidden_states = self.LayerNorm(hidden_states + input_tensor)\nreturn hidden_states"
                    }
                  ]
                },
                {
                  "name": "BertLayer",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.config = config\nself.chunk_size_feed_forward = config.chunk_size_feed_forward\nself.seq_len_dim = 1\nself.attention = BertAttention(config)\nself.layer_num = layer_num\nif self.config.add_cross_attention and layer_num % self.config.cross_attention_freq == 0:\n    self.crossattention = BertAttention(config, is_cross_attention=self.config.add_cross_attention)\n    self.has_cross_attention = True\nelse:\n    self.has_cross_attention = False\nself.intermediate = BertIntermediate(config)\nself.output = BertOutput(config)\nself.intermediate_query = BertIntermediate(config)\nself.output_query = BertOutput(config)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\nself_attention_outputs = self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)\nattention_output = self_attention_outputs[0]\noutputs = self_attention_outputs[1:-1]\npresent_key_value = self_attention_outputs[-1]\nif query_length > 0:\n    query_attention_output = attention_output[:, :query_length, :]\n    if self.has_cross_attention:\n        assert encoder_hidden_states is not None, 'encoder_hidden_states must be given for cross-attention layers'\n        cross_attention_outputs = self.crossattention(query_attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions=output_attentions)\n        query_attention_output = cross_attention_outputs[0]\n        outputs = outputs + cross_attention_outputs[1:-1]\n    layer_output = apply_chunking_to_forward(self.feed_forward_chunk_query, self.chunk_size_feed_forward, self.seq_len_dim, query_attention_output)\n    if attention_output.shape[1] > query_length:\n        layer_output_text = apply_chunking_to_forward(self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output[:, query_length:, :])\n        layer_output = torch.cat([layer_output, layer_output_text], dim=1)\nelse:\n    layer_output = apply_chunking_to_forward(self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output)\noutputs = (layer_output,) + outputs\noutputs = outputs + (present_key_value,)\nreturn outputs"
                    },
                    {
                      "name": "feed_forward_chunk",
                      "decorators": [],
                      "body": "intermediate_output = self.intermediate(attention_output)\nlayer_output = self.output(intermediate_output, attention_output)\nreturn layer_output"
                    },
                    {
                      "name": "feed_forward_chunk_query",
                      "decorators": [],
                      "body": "intermediate_output = self.intermediate_query(attention_output)\nlayer_output = self.output_query(intermediate_output, attention_output)\nreturn layer_output"
                    }
                  ]
                },
                {
                  "name": "BertEncoder",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.config = config\nself.layer = nn.ModuleList([BertLayer(config, i) for i in range(config.num_hidden_layers)])"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "all_hidden_states = () if output_hidden_states else None\nall_self_attentions = () if output_attentions else None\nall_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\nnext_decoder_cache = () if use_cache else None\nfor i in range(self.config.num_hidden_layers):\n    layer_module = self.layer[i]\n    if output_hidden_states:\n        all_hidden_states = all_hidden_states + (hidden_states,)\n    layer_head_mask = head_mask[i] if head_mask is not None else None\n    past_key_value = past_key_values[i] if past_key_values is not None else None\n    if getattr(self.config, 'gradient_checkpointing', False) and self.training:\n        if use_cache:\n            logger.warn('`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...')\n            use_cache = False\n\n        def create_custom_forward(module):\n\n            def custom_forward(*inputs):\n                return module(*inputs, past_key_value, output_attentions, query_length)\n            return custom_forward\n        layer_outputs = torch.utils.checkpoint.checkpoint(create_custom_forward(layer_module), hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask)\n    else:\n        layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, query_length)\n    hidden_states = layer_outputs[0]\n    if use_cache:\n        next_decoder_cache += (layer_outputs[-1],)\n    if output_attentions:\n        all_self_attentions = all_self_attentions + (layer_outputs[1],)\n        all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\nif output_hidden_states:\n    all_hidden_states = all_hidden_states + (hidden_states,)\nif not return_dict:\n    return tuple((v for v in [hidden_states, next_decoder_cache, all_hidden_states, all_self_attentions, all_cross_attentions] if v is not None))\nreturn BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=hidden_states, past_key_values=next_decoder_cache, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions)"
                    }
                  ]
                },
                {
                  "name": "BertPooler",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.dense = nn.Linear(config.hidden_size, config.hidden_size)\nself.activation = nn.Tanh()"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "first_token_tensor = hidden_states[:, 0]\npooled_output = self.dense(first_token_tensor)\npooled_output = self.activation(pooled_output)\nreturn pooled_output"
                    }
                  ]
                },
                {
                  "name": "BertPredictionHeadTransform",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.dense = nn.Linear(config.hidden_size, config.hidden_size)\nif isinstance(config.hidden_act, str):\n    self.transform_act_fn = ACT2FN[config.hidden_act]\nelse:\n    self.transform_act_fn = config.hidden_act\nself.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "hidden_states = self.dense(hidden_states)\nhidden_states = self.transform_act_fn(hidden_states)\nhidden_states = self.LayerNorm(hidden_states)\nreturn hidden_states"
                    }
                  ]
                },
                {
                  "name": "BertLMPredictionHead",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.transform = BertPredictionHeadTransform(config)\nself.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\nself.bias = nn.Parameter(torch.zeros(config.vocab_size))\nself.decoder.bias = self.bias"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "hidden_states = self.transform(hidden_states)\nhidden_states = self.decoder(hidden_states)\nreturn hidden_states"
                    }
                  ]
                },
                {
                  "name": "BertOnlyMLMHead",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.predictions = BertLMPredictionHead(config)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "prediction_scores = self.predictions(sequence_output)\nreturn prediction_scores"
                    }
                  ]
                },
                {
                  "name": "BertPreTrainedModel",
                  "decorators": [],
                  "inherits": [
                    "PreTrainedModel"
                  ],
                  "methods": [
                    {
                      "name": "_init_weights",
                      "decorators": [],
                      "body": "'Initialize the weights'\nif isinstance(module, (nn.Linear, nn.Embedding)):\n    module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\nelif isinstance(module, nn.LayerNorm):\n    module.bias.data.zero_()\n    module.weight.data.fill_(1.0)\nif isinstance(module, nn.Linear) and module.bias is not None:\n    module.bias.data.zero_()"
                    }
                  ]
                },
                {
                  "name": "BertModel",
                  "decorators": [],
                  "inherits": [
                    "BertPreTrainedModel"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__(config)\nself.config = config\nself.embeddings = BertEmbeddings(config)\nself.encoder = BertEncoder(config)\nself.pooler = BertPooler(config) if add_pooling_layer else None\nself.init_weights()"
                    },
                    {
                      "name": "get_input_embeddings",
                      "decorators": [],
                      "body": "return self.embeddings.word_embeddings"
                    },
                    {
                      "name": "set_input_embeddings",
                      "decorators": [],
                      "body": "self.embeddings.word_embeddings = value"
                    },
                    {
                      "name": "_prune_heads",
                      "decorators": [],
                      "body": "'\\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\\n        class PreTrainedModel\\n        '\nfor (layer, heads) in heads_to_prune.items():\n    self.encoder.layer[layer].attention.prune_heads(heads)"
                    },
                    {
                      "name": "get_extended_attention_mask",
                      "decorators": [],
                      "body": "'\\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\\n\\n        Arguments:\\n            attention_mask (:obj:`torch.Tensor`):\\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\\n            input_shape (:obj:`Tuple[int]`):\\n                The shape of the input to the model.\\n            device: (:obj:`torch.device`):\\n                The device of the input to the model.\\n\\n        Returns:\\n            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\\n        '\nif attention_mask.dim() == 3:\n    extended_attention_mask = attention_mask[:, None, :, :]\nelif attention_mask.dim() == 2:\n    if is_decoder:\n        (batch_size, seq_length) = input_shape\n        seq_ids = torch.arange(seq_length, device=device)\n        causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n        causal_mask = causal_mask.to(attention_mask.dtype)\n        if causal_mask.shape[1] < attention_mask.shape[1]:\n            prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n            if has_query:\n                causal_mask = torch.cat([torch.zeros((batch_size, prefix_seq_len, seq_length), device=device, dtype=causal_mask.dtype), causal_mask], axis=1)\n            causal_mask = torch.cat([torch.ones((batch_size, causal_mask.shape[1], prefix_seq_len), device=device, dtype=causal_mask.dtype), causal_mask], axis=-1)\n        extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n    else:\n        extended_attention_mask = attention_mask[:, None, None, :]\nelse:\n    raise ValueError('Wrong shape for input_ids (shape {}) or attention_mask (shape {})'.format(input_shape, attention_mask.shape))\nextended_attention_mask = extended_attention_mask.to(dtype=self.dtype)\nextended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\nreturn extended_attention_mask"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "\"\\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\\n            the model is configured as a decoder.\\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\\n            - 1 for tokens that are **not masked**,\\n            - 0 for tokens that are **masked**.\\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\\n        use_cache (:obj:`bool`, `optional`):\\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\\n            decoding (see :obj:`past_key_values`).\\n        \"\noutput_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\noutput_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\nreturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\nif input_ids is None:\n    assert query_embeds is not None, 'You have to specify query_embeds when input_ids is None'\npast_key_values_length = past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\nquery_length = query_embeds.shape[1] if query_embeds is not None else 0\nembedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, query_embeds=query_embeds, past_key_values_length=past_key_values_length)\ninput_shape = embedding_output.size()[:-1]\n(batch_size, seq_length) = input_shape\ndevice = embedding_output.device\nif attention_mask is None:\n    attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\nif is_decoder:\n    extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_ids.shape, device, is_decoder, has_query=query_embeds is not None)\nelse:\n    extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, device, is_decoder)\nif encoder_hidden_states is not None:\n    if type(encoder_hidden_states) == list:\n        (encoder_batch_size, encoder_sequence_length, _) = encoder_hidden_states[0].size()\n    else:\n        (encoder_batch_size, encoder_sequence_length, _) = encoder_hidden_states.size()\n    encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n    if type(encoder_attention_mask) == list:\n        encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n    elif encoder_attention_mask is None:\n        encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n    else:\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\nelse:\n    encoder_extended_attention_mask = None\nhead_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\nencoder_outputs = self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, query_length=query_length)\nsequence_output = encoder_outputs[0]\npooled_output = self.pooler(sequence_output) if self.pooler is not None else None\nif not return_dict:\n    return (sequence_output, pooled_output) + encoder_outputs[1:]\nreturn BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=sequence_output, pooler_output=pooled_output, past_key_values=encoder_outputs.past_key_values, hidden_states=encoder_outputs.hidden_states, attentions=encoder_outputs.attentions, cross_attentions=encoder_outputs.cross_attentions)"
                    }
                  ]
                },
                {
                  "name": "BertLMHeadModel",
                  "decorators": [],
                  "inherits": [
                    "BertPreTrainedModel"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__(config)\nself.bert = BertModel(config, add_pooling_layer=False)\nself.cls = BertOnlyMLMHead(config)\nself.init_weights()"
                    },
                    {
                      "name": "get_output_embeddings",
                      "decorators": [],
                      "body": "return self.cls.predictions.decoder"
                    },
                    {
                      "name": "set_output_embeddings",
                      "decorators": [],
                      "body": "self.cls.predictions.decoder = new_embeddings"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "'\\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\\n            the model is configured as a decoder.\\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\\n            - 1 for tokens that are **not masked**,\\n            - 0 for tokens that are **masked**.\\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\\n            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\\n            (those that don\\'t have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\\n        use_cache (:obj:`bool`, `optional`):\\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\\n            decoding (see :obj:`past_key_values`).\\n        Returns:\\n        Example::\\n            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\\n            >>> import torch\\n            >>> tokenizer = BertTokenizer.from_pretrained(\\'bert-base-cased\\')\\n            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\\n            >>> model = BertLMHeadModel.from_pretrained(\\'bert-base-cased\\', config=config)\\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\\n            >>> outputs = model(**inputs)\\n            >>> prediction_logits = outputs.logits\\n        '\nreturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\nif labels is not None:\n    use_cache = False\nif past_key_values is not None:\n    query_embeds = None\noutputs = self.bert(input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, query_embeds=query_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder)\nsequence_output = outputs[0]\nif query_embeds is not None:\n    sequence_output = outputs[0][:, query_embeds.shape[1]:, :]\nprediction_scores = self.cls(sequence_output)\nif return_logits:\n    return prediction_scores[:, :-1, :].contiguous()\nlm_loss = None\nif labels is not None:\n    shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n    labels = labels[:, 1:].contiguous()\n    loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1)\n    lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n    if reduction == 'none':\n        lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1)\nif not return_dict:\n    output = (prediction_scores,) + outputs[2:]\n    return (lm_loss,) + output if lm_loss is not None else output\nreturn CausalLMOutputWithCrossAttentions(loss=lm_loss, logits=prediction_scores, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, cross_attentions=outputs.cross_attentions)"
                    },
                    {
                      "name": "prepare_inputs_for_generation",
                      "decorators": [],
                      "body": "if attention_mask is None:\n    attention_mask = input_ids.new_ones(input_ids.shape)\nquery_mask = input_ids.new_ones(query_embeds.shape[:-1])\nattention_mask = torch.cat([query_mask, attention_mask], dim=-1)\nif past is not None:\n    input_ids = input_ids[:, -1:]\nreturn {'input_ids': input_ids, 'query_embeds': query_embeds, 'attention_mask': attention_mask, 'past_key_values': past, 'encoder_hidden_states': model_kwargs.get('encoder_hidden_states', None), 'encoder_attention_mask': model_kwargs.get('encoder_attention_mask', None), 'is_decoder': True}"
                    },
                    {
                      "name": "_reorder_cache",
                      "decorators": [],
                      "body": "reordered_past = ()\nfor layer_past in past:\n    reordered_past += (tuple((past_state.index_select(0, beam_idx) for past_state in layer_past)),)\nreturn reordered_past"
                    }
                  ]
                },
                {
                  "name": "BertForMaskedLM",
                  "decorators": [],
                  "inherits": [
                    "BertPreTrainedModel"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__(config)\nself.bert = BertModel(config, add_pooling_layer=False)\nself.cls = BertOnlyMLMHead(config)\nself.init_weights()"
                    },
                    {
                      "name": "get_output_embeddings",
                      "decorators": [],
                      "body": "return self.cls.predictions.decoder"
                    },
                    {
                      "name": "set_output_embeddings",
                      "decorators": [],
                      "body": "self.cls.predictions.decoder = new_embeddings"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "'\\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\\n        '\nreturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\noutputs = self.bert(input_ids, attention_mask=attention_mask, position_ids=position_ids, head_mask=head_mask, query_embeds=query_embeds, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, is_decoder=is_decoder)\nif query_embeds is not None:\n    sequence_output = outputs[0][:, query_embeds.shape[1]:, :]\nprediction_scores = self.cls(sequence_output)\nif return_logits:\n    return prediction_scores\nmasked_lm_loss = None\nif labels is not None:\n    loss_fct = CrossEntropyLoss()\n    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\nif not return_dict:\n    output = (prediction_scores,) + outputs[2:]\n    return (masked_lm_loss,) + output if masked_lm_loss is not None else output\nreturn MaskedLMOutput(loss=masked_lm_loss, logits=prediction_scores, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/models/minigpt_v2.py",
              "type": "python",
              "imports": [
                "import logging",
                "import random",
                "import torch",
                "from torch.cuda.amp import autocast as autocast",
                "import torch.nn as nn",
                "from minigpt4.common.registry import registry",
                "from minigpt4.models.base_model import disabled_train",
                "from minigpt4.models.minigpt_base import MiniGPTBase",
                "from minigpt4.models.Qformer import BertConfig, BertLMHeadModel"
              ],
              "classes": [
                {
                  "name": "MiniGPTv2",
                  "decorators": [
                    "registry.register_model('minigpt_v2')"
                  ],
                  "inherits": [
                    "MiniGPTBase"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__(vit_model=vit_model, img_size=img_size, drop_path_rate=drop_path_rate, use_grad_checkpoint=use_grad_checkpoint, vit_precision=vit_precision, freeze_vit=freeze_vit, llama_model=llama_model, max_txt_len=max_txt_len, max_context_len=max_context_len, end_sym=end_sym, prompt_template=prompt_template, low_resource=low_resource, device_8bit=device_8bit, lora_r=lora_r, lora_target_modules=lora_target_modules, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\nimg_f_dim = self.visual_encoder.num_features * 4\nself.llama_proj = nn.Linear(img_f_dim, self.llama_model.config.hidden_size)\nself.feats_llama_proj1 = nn.Linear(1024, self.llama_model.config.hidden_size)\nself.feats_llama_proj2 = nn.Linear(1024, self.llama_model.config.hidden_size)\nself.feats_llama_proj3 = nn.Linear(1024, self.llama_model.config.hidden_size)\nself.cls_tk_llama_proj = nn.Linear(1408, self.llama_model.config.hidden_size)\nself.chat_template = chat_template\nif use_grad_checkpoint_llm:\n    self.llama_model.gradient_checkpointing_enable()"
                    },
                    {
                      "name": "encode_img",
                      "decorators": [],
                      "body": "device = image.device\nif len(image.shape) > 4:\n    image = image.reshape(-1, *image.shape[-3:])\nwith self.maybe_autocast():\n    image_feats = self.visual_encoder(image)\n    image_embeds = self.ln_vision(image_feats).to(device)\n    image_cls_tk = image_embeds[:, :1, :]\n    cls_tk_feats = self.cls_tk_llama_proj(image_cls_tk)\n    image_embeds = image_embeds[:, 1:, :]\n    (bs, pn, hs) = image_embeds.shape\n    image_embeds = image_embeds.view(bs, int(pn / 4), int(hs * 4))\n    image_inputs_llama = self.llama_proj(image_embeds)\n    video_features = video_features.to(device)\n    video_features_split = torch.split(video_features, 1, dim=1)\n    output1 = self.feats_llama_proj1(video_features_split[0].squeeze(1))\n    output2 = self.feats_llama_proj2(video_features_split[1].squeeze(1))\n    output3 = self.feats_llama_proj3(video_features_split[2].squeeze(1))\n    video_feats = torch.stack([output1, output2, output3], dim=1)\n    inputs_llama = torch.cat((image_inputs_llama, video_feats, cls_tk_feats), dim=1)\n    atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(image.device)\nreturn (inputs_llama, atts_llama)"
                    },
                    {
                      "name": "from_config",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "vit_model = cfg.get('vit_model', 'eva_clip_g')\nimg_size = cfg.get('image_size')\nllama_model = cfg.get('llama_model')\ndrop_path_rate = cfg.get('drop_path_rate', 0)\nuse_grad_checkpoint = cfg.get('use_grad_checkpoint', False)\nvit_precision = cfg.get('vit_precision', 'fp16')\nfreeze_vit = cfg.get('freeze_vit', True)\nlow_resource = cfg.get('low_resource', False)\nprompt_template = cfg.get('prompt_template', '[INST] {} [/INST]')\nmax_txt_len = cfg.get('max_txt_len', 300)\nend_sym = cfg.get('end_sym', '\\n')\nlora_r = cfg.get('lora_r', 64)\nlora_alpha = cfg.get('lora_alpha', 16)\nchat_template = cfg.get('chat_template', False)\nuse_grad_checkpoint_llm = cfg.get('use_grad_checkpoint_llm', False)\nmax_context_len = cfg.get('max_context_len', 3800)\nmodel = cls(vit_model=vit_model, img_size=img_size, drop_path_rate=drop_path_rate, use_grad_checkpoint=use_grad_checkpoint, vit_precision=vit_precision, freeze_vit=freeze_vit, llama_model=llama_model, prompt_template=prompt_template, max_txt_len=max_txt_len, low_resource=low_resource, end_sym=end_sym, lora_r=lora_r, lora_alpha=lora_alpha, chat_template=chat_template, use_grad_checkpoint_llm=use_grad_checkpoint_llm, max_context_len=max_context_len)\nckpt_path = cfg.get('ckpt', '')\nif ckpt_path:\n    print('Load Minigpt-4-LLM Checkpoint: {}'.format(ckpt_path))\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    msg = model.load_state_dict(ckpt['model'], strict=False)\nreturn model"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/models/__init__.py",
              "type": "python",
              "imports": [
                "import logging",
                "import torch",
                "from omegaconf import OmegaConf",
                "from minigpt4.common.registry import registry",
                "from minigpt4.models.base_model import BaseModel",
                "from minigpt4.models.minigpt_base import MiniGPTBase",
                "from minigpt4.models.minigpt4 import MiniGPT4",
                "from minigpt4.models.minigpt_v2 import MiniGPTv2",
                "from minigpt4.processors.base_processor import BaseProcessor"
              ],
              "classes": [
                {
                  "name": "ModelZoo",
                  "decorators": [],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.model_zoo = {k: list(v.PRETRAINED_MODEL_CONFIG_DICT.keys()) for (k, v) in registry.mapping['model_name_mapping'].items()}"
                    },
                    {
                      "name": "__str__",
                      "decorators": [],
                      "body": "return '=' * 50 + '\\n' + f\"{'Architectures':<30} {'Types'}\\n\" + '=' * 50 + '\\n' + '\\n'.join([f\"{name:<30} {', '.join(types)}\" for (name, types) in self.model_zoo.items()])"
                    },
                    {
                      "name": "__iter__",
                      "decorators": [],
                      "body": "return iter(self.model_zoo.items())"
                    },
                    {
                      "name": "__len__",
                      "decorators": [],
                      "body": "return sum([len(v) for v in self.model_zoo.values()])"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "load_model",
                  "decorators": [],
                  "body": "'\\n    Load supported models.\\n\\n    To list all available models and types in registry:\\n    >>> from minigpt4.models import model_zoo\\n    >>> print(model_zoo)\\n\\n    Args:\\n        name (str): name of the model.\\n        model_type (str): type of the model.\\n        is_eval (bool): whether the model is in eval mode. Default: False.\\n        device (str): device to use. Default: \"cpu\".\\n        checkpoint (str): path or to checkpoint. Default: None.\\n            Note that expecting the checkpoint to have the same keys in state_dict as the model.\\n\\n    Returns:\\n        model (torch.nn.Module): model.\\n    '\nmodel = registry.get_model_class(name).from_pretrained(model_type=model_type)\nif checkpoint is not None:\n    model.load_checkpoint(checkpoint)\nif is_eval:\n    model.eval()\nif device == 'cpu':\n    model = model.float()\nreturn model.to(device)"
                },
                {
                  "name": "load_preprocess",
                  "decorators": [],
                  "body": "'\\n    Load preprocessor configs and construct preprocessors.\\n\\n    If no preprocessor is specified, return BaseProcessor, which does not do any preprocessing.\\n\\n    Args:\\n        config (dict): preprocessor configs.\\n\\n    Returns:\\n        vis_processors (dict): preprocessors for visual inputs.\\n        txt_processors (dict): preprocessors for text inputs.\\n\\n        Key is \"train\" or \"eval\" for processors used in training and evaluation respectively.\\n    '\n\ndef _build_proc_from_cfg(cfg):\n    return registry.get_processor_class(cfg.name).from_config(cfg) if cfg is not None else BaseProcessor()\nvis_processors = dict()\ntxt_processors = dict()\nvis_proc_cfg = config.get('vis_processor')\ntxt_proc_cfg = config.get('text_processor')\nif vis_proc_cfg is not None:\n    vis_train_cfg = vis_proc_cfg.get('train')\n    vis_eval_cfg = vis_proc_cfg.get('eval')\nelse:\n    vis_train_cfg = None\n    vis_eval_cfg = None\nvis_processors['train'] = _build_proc_from_cfg(vis_train_cfg)\nvis_processors['eval'] = _build_proc_from_cfg(vis_eval_cfg)\nif txt_proc_cfg is not None:\n    txt_train_cfg = txt_proc_cfg.get('train')\n    txt_eval_cfg = txt_proc_cfg.get('eval')\nelse:\n    txt_train_cfg = None\n    txt_eval_cfg = None\ntxt_processors['train'] = _build_proc_from_cfg(txt_train_cfg)\ntxt_processors['eval'] = _build_proc_from_cfg(txt_eval_cfg)\nreturn (vis_processors, txt_processors)"
                },
                {
                  "name": "load_model_and_preprocess",
                  "decorators": [],
                  "body": "'\\n    Load model and its related preprocessors.\\n\\n    List all available models and types in registry:\\n    >>> from minigpt4.models import model_zoo\\n    >>> print(model_zoo)\\n\\n    Args:\\n        name (str): name of the model.\\n        model_type (str): type of the model.\\n        is_eval (bool): whether the model is in eval mode. Default: False.\\n        device (str): device to use. Default: \"cpu\".\\n\\n    Returns:\\n        model (torch.nn.Module): model.\\n        vis_processors (dict): preprocessors for visual inputs.\\n        txt_processors (dict): preprocessors for text inputs.\\n    '\nmodel_cls = registry.get_model_class(name)\nmodel = model_cls.from_pretrained(model_type=model_type)\nif is_eval:\n    model.eval()\ncfg = OmegaConf.load(model_cls.default_config_path(model_type))\nif cfg is not None:\n    preprocess_cfg = cfg.preprocess\n    (vis_processors, txt_processors) = load_preprocess(preprocess_cfg)\nelse:\n    (vis_processors, txt_processors) = (None, None)\n    logging.info(f'No default preprocess for model {name} ({model_type}).\\n                This can happen if the model is not finetuned on downstream datasets,\\n                or it is not intended for direct use without finetuning.\\n            ')\nif device == 'cpu' or device == torch.device('cpu'):\n    model = model.float()\nreturn (model.to(device), vis_processors, txt_processors)"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/models/minigpt_base.py",
              "type": "python",
              "imports": [
                "import logging",
                "import random",
                "import torch",
                "from torch.cuda.amp import autocast as autocast",
                "import torch.nn as nn",
                "from minigpt4.common.registry import registry",
                "from minigpt4.models.base_model import BaseModel",
                "from transformers import StoppingCriteria, StoppingCriteriaList",
                "from minigpt4.conversation.conversation import StoppingCriteriaSub"
              ],
              "classes": [
                {
                  "name": "Attention",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super(Attention, self).__init__()\ndims = [1024, 1024, 1024, 4096, 4096, 4096, 4096]\nself.features_count = 7\nself.feats_prep = nn.ModuleList()\nfor i in range(len(dims)):\n    self.feats_prep.append(self.MLP(dims[i], layers, dropout))\nlayers_list = list(map(lambda x: int(x), layers.split(',')))\nhiddendim = layers_list[-1] * self.features_count\nself.attention_mlp = self.MLP(hiddendim, layers, dropout)\nself.fc_att = nn.Linear(layers_list[-1], self.features_count)\nself.fc_out = nn.Linear(layers_list[-1], output_dim)\nself.softmax = nn.Softmax(dim=1)"
                    },
                    {
                      "name": "MLP",
                      "decorators": [],
                      "body": "all_layers = []\nlayers = list(map(lambda x: int(x), layers.split(',')))\nfor i in range(0, len(layers)):\n    all_layers.append(nn.Linear(input_dim, layers[i]))\n    all_layers.append(nn.ReLU())\n    all_layers.append(nn.Dropout(dropout))\n    input_dim = layers[i]\nmodule = nn.Sequential(*all_layers)\nreturn module"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "feats_hidden = []\nfor i in range(self.features_count):\n    feats_hidden.append(self.feats_prep[i](feats[i]))\nmulti_hidden1 = torch.cat(feats_hidden, dim=1)\nmulti_hidden2 = torch.stack(feats_hidden, dim=2)\nattention = self.attention_mlp(multi_hidden1)\nattention = self.fc_att(attention)\nattention = torch.unsqueeze(attention, 2)\nfused_feat = torch.matmul(multi_hidden2, attention)\nfused_feat = fused_feat.squeeze(dim=2)\nemos_out = self.fc_out(fused_feat)\nemos_pred = self.softmax(emos_out)\nreturn emos_pred"
                    }
                  ]
                },
                {
                  "name": "MiniGPTBase",
                  "decorators": [],
                  "inherits": [
                    "BaseModel"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\n(self.llama_model, self.llama_tokenizer) = self.init_llm(llama_model_path=llama_model, low_resource=low_resource, low_res_device=device_8bit, lora_r=lora_r, lora_target_modules=lora_target_modules, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n(self.visual_encoder, self.ln_vision) = self.init_vision_encoder(vit_model, img_size, drop_path_rate, use_grad_checkpoint, vit_precision, freeze_vit)\nself.max_txt_len = max_txt_len\nself.max_context_len = max_context_len\nself.end_sym = end_sym\nself.prompt_template = prompt_template\nself.prompt_list = []\nself.attention = Attention(output_dim=6)\nself.CEloss = nn.CrossEntropyLoss()"
                    },
                    {
                      "name": "vit_to_cpu",
                      "decorators": [],
                      "body": "self.ln_vision.to('cpu')\nself.ln_vision.float()\nself.visual_encoder.to('cpu')\nself.visual_encoder.float()"
                    },
                    {
                      "name": "get_context_emb",
                      "decorators": [],
                      "body": "device = img_list[0].device\nprompt_segs = prompt.split('<VideoHere>')\nassert len(prompt_segs) == len(img_list) + 1, 'Unmatched numbers of image placeholders and images.'\nseg_tokens = [self.llama_tokenizer(seg, return_tensors='pt', add_special_tokens=i == 0).to(device).input_ids for (i, seg) in enumerate(prompt_segs)]\nseg_embs = [self.embed_tokens(seg_t) for seg_t in seg_tokens]\nmixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\nmixed_embs = torch.cat(mixed_embs, dim=1)\nreturn mixed_embs"
                    },
                    {
                      "name": "prompt_wrap",
                      "decorators": [],
                      "body": "if prompts is None or len(prompts) == 0:\n    return (img_embeds, atts_img)\nelif img_embeds is None:\n    self.llama_tokenizer.padding_side = 'right'\n    prompt_tokens = self.llama_tokenizer(prompts, return_tensors='pt', padding='longest', add_special_tokens=False).to(self.device)\n    prompt_embeds = self.embed_tokens(prompt_tokens.input_ids)\n    atts_prompt = prompt_tokens.attention_mask\n    return (prompt_embeds, atts_prompt)\nelse:\n    emb_lists = []\n    if isinstance(prompts, str):\n        prompts = [prompts] * len(img_embeds)\n    for (idx, (each_img_embed, each_prompt)) in enumerate(zip(img_embeds, prompts)):\n        each_video_feature = each_img_embed[-4:]\n        each_img_embed = each_img_embed[:-4]\n        pn = each_img_embed.shape[-2]\n        if lengths is not None:\n            each_img_embed = each_img_embed.reshape(-1, each_img_embed.shape[-1])\n            each_img_embed = each_img_embed[:lengths[idx] * pn]\n        p_segs = each_prompt.split('<VideoHere>')\n        interleave_emb = []\n        for (idx, seg) in enumerate(p_segs[:-1]):\n            p_tokens = self.llama_tokenizer(seg, return_tensors='pt', add_special_tokens=False).to(img_embeds.device)\n            p_embed = self.embed_tokens(p_tokens.input_ids)\n            interleave_emb.append(torch.cat([p_embed, each_img_embed[None][:, idx * pn:(idx + 1) * pn]], dim=1))\n        wrapped_emb = torch.cat(interleave_emb, dim=1)\n        f_segs = p_segs[-1].split('<FeatureHere>')\n        f_tokens = self.llama_tokenizer(f_segs[0], return_tensors='pt', add_special_tokens=False).to(img_embeds.device)\n        f_embed = self.embed_tokens(f_tokens.input_ids)\n        f_wrapped_emb = torch.cat([f_embed, each_video_feature[None][:]], dim=1)\n        p_tokens = self.llama_tokenizer(f_segs[-1], return_tensors='pt', add_special_tokens=False).to(img_embeds.device)\n        p_embed = self.embed_tokens(p_tokens.input_ids)\n        wrapped_emb = torch.cat([wrapped_emb, f_wrapped_emb, p_embed], dim=1)\n        emb_lists.append(wrapped_emb)\n    emb_lens = [emb.shape[1] for emb in emb_lists]\n    pad_emb = self.embed_tokens(torch.tensor(self.llama_tokenizer.pad_token_id, device=img_embeds.device))\n    max_length = max(emb_lens) if max(emb_lens) < self.max_context_len else self.max_context_len\n    wrapped_embs = pad_emb.expand(len(emb_lens), max_length, -1).clone()\n    wrapped_atts = torch.zeros([len(emb_lens), max_length], dtype=torch.int, device=img_embeds.device)\n    for (i, emb) in enumerate(emb_lists):\n        length = emb_lens[i] if emb_lens[i] < self.max_context_len else self.max_context_len\n        wrapped_embs[i, :length] = emb[:, :length]\n        wrapped_atts[i, :length] = 1\n    return (wrapped_embs, wrapped_atts)"
                    },
                    {
                      "name": "concat_emb_input_output",
                      "decorators": [],
                      "body": "'\\n        Concatenate the batched input embedding and batched output embedding together.\\n        Both the input and the output embedding should be right padded.\\n        '\ninput_lens = []\ncat_embs = []\ncat_atts = []\nfor i in range(input_embs.size(0)):\n    input_len = input_atts[i].sum()\n    input_lens.append(input_len)\n    cat_embs.append(torch.cat([input_embs[i][:input_len], output_embs[i], input_embs[i][input_len:]]))\n    cat_atts.append(torch.cat([input_atts[i][:input_len], output_atts[i], input_atts[i][input_len:]]))\ncat_embs = torch.stack(cat_embs)\ncat_atts = torch.stack(cat_atts)\nreturn (cat_embs, cat_atts, input_lens)"
                    },
                    {
                      "name": "tokenize_conversation",
                      "decorators": [],
                      "body": "'concatenate conversation and make sure the model is only trained to regress the answer'\nto_regress_token_ids_list = []\ntargets_list = []\nbatch_size = len(conv_q)\nfor batch_idx in range(batch_size):\n    (questions, answers) = (conv_q[batch_idx], conv_a[batch_idx])\n    questions = [self.llama_tokenizer(self.llama_tokenizer.bos_token + q, return_tensors='pt', add_special_tokens=False).to(self.device) for q in questions[1:]]\n    answers = [self.llama_tokenizer(a + self.end_sym, return_tensors='pt', add_special_tokens=False).to(self.device) for a in answers]\n    cur_id = []\n    cur_target = []\n    for i in range(len(questions)):\n        cur_id.append(answers[i].input_ids)\n        cur_target.append(answers[i].input_ids)\n        cur_id.append(questions[i].input_ids)\n        cur_target.append(torch.ones_like(questions[i].input_ids) * -100)\n    cur_id.append(answers[-1].input_ids)\n    cur_target.append(answers[-1].input_ids)\n    cur_id = torch.cat(cur_id, dim=1)\n    cur_target = torch.cat(cur_target, dim=1)\n    to_regress_token_ids_list.append(cur_id)\n    targets_list.append(cur_target)\nmax_len = min(max([target.shape[1] for target in targets_list]), self.max_txt_len)\nto_regress_token_ids = torch.ones([batch_size, max_len], dtype=cur_id.dtype, device=self.device) * self.llama_tokenizer.pad_token_id\ntargets = torch.ones([batch_size, max_len], dtype=cur_id.dtype, device=self.device) * -100\nfor batch_idx in range(batch_size):\n    cur_len = to_regress_token_ids_list[batch_idx].shape[1]\n    to_regress_token_ids[batch_idx, :cur_len] = to_regress_token_ids_list[batch_idx][0, :max_len]\n    targets[batch_idx, :cur_len] = targets_list[batch_idx][0, :max_len]\nto_regress_token_attn = (to_regress_token_ids != self.llama_tokenizer.pad_token_id).to(torch.int)\nreturn (to_regress_token_ids, to_regress_token_attn, targets)"
                    },
                    {
                      "name": "preparing_embedding",
                      "decorators": [],
                      "body": "if 'image' in samples:\n    (img_embeds, img_atts) = self.encode_img(samples['image'], samples['video_features'])\nelse:\n    img_embeds = img_atts = None\nif 'conv_q' in samples:\n    (conv_q, conv_a) = (samples['conv_q'], samples['conv_a'])\n    connect_sym = samples['connect_sym'][0]\n    conv_q = [q.split(connect_sym) for q in conv_q]\n    conv_a = [a.split(connect_sym) for a in conv_a]\n    conv_q = [[self.prompt_template.format(item) for item in items] for items in conv_q]\n    (cond_embeds, cond_atts) = self.prompt_wrap(img_embeds, img_atts, [q[0] for q in conv_q])\n    (regress_token_ids, regress_atts, part_targets) = self.tokenize_conversation(conv_q, conv_a)\nelse:\n    if 'instruction_input' in samples:\n        instruction = samples['instruction_input']\n    elif self.prompt_list:\n        instruction = random.choice(self.prompt_list)\n    else:\n        instruction = None\n    if hasattr(self, 'chat_template') and self.chat_template:\n        instruction = [self.prompt_template.format(instruct) for instruct in instruction]\n    if 'length' in samples:\n        (bsz, pn, hs) = img_embeds.shape\n        img_embeds = img_embeds.reshape(len(samples['image']), -1, pn, hs)\n        (cond_embeds, cond_atts) = self.prompt_wrap(img_embeds, img_atts, instruction, samples['length'])\n    else:\n        (cond_embeds, cond_atts) = self.prompt_wrap(img_embeds, img_atts, instruction)\n    self.llama_tokenizer.padding_side = 'right'\n    text = [t + self.end_sym for t in samples['answer']]\n    regress_tokens = self.llama_tokenizer(text, return_tensors='pt', padding='longest', truncation=True, max_length=self.max_txt_len, add_special_tokens=False).to(self.device)\n    regress_token_ids = regress_tokens.input_ids\n    regress_atts = regress_tokens.attention_mask\n    part_targets = regress_token_ids.masked_fill(regress_token_ids == self.llama_tokenizer.pad_token_id, -100)\nregress_embeds = self.embed_tokens(regress_token_ids)\nreturn (cond_embeds, cond_atts, regress_embeds, regress_atts, part_targets)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "(cond_embeds, cond_atts, regress_embeds, regress_atts, part_targets) = self.preparing_embedding(samples)\n(inputs_embeds, attention_mask, input_lens) = self.concat_emb_input_output(cond_embeds, cond_atts, regress_embeds, regress_atts)\nbos = torch.ones_like(part_targets[:, :1]) * self.llama_tokenizer.bos_token_id\nbos_embeds = self.embed_tokens(bos)\nbos_atts = cond_atts[:, :1]\ninputs_embeds = torch.cat([bos_embeds, inputs_embeds], dim=1)\nattention_mask = torch.cat([bos_atts, attention_mask], dim=1)\ntargets = torch.ones([inputs_embeds.shape[0], inputs_embeds.shape[1]], dtype=torch.long).to(self.device).fill_(-100)\nfor (i, target) in enumerate(part_targets):\n    targets[i, input_lens[i] + 1:input_lens[i] + len(target) + 1] = target\nwith self.maybe_autocast():\n    outputs = self.llama_model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, return_dict=True, labels=targets, reduction=reduction, output_hidden_states=True, emotion=samples['emotion'])\nloss = outputs.loss\nemos_loss = loss\nemos_pred = 0\nreturn {'loss': loss, 'emos_loss': emos_loss, 'emos_pred': emos_pred, 'emotion': samples['emotion']}"
                    },
                    {
                      "name": "embed_tokens",
                      "decorators": [],
                      "body": "if hasattr(self.llama_model.base_model, 'model'):\n    embeds = self.llama_model.base_model.model.model.embed_tokens(token_ids)\nelse:\n    embeds = self.llama_model.base_model.embed_tokens(token_ids)\nreturn embeds"
                    },
                    {
                      "name": "generate",
                      "decorators": [
                        "torch.no_grad()"
                      ],
                      "body": "'\\n            function for generate test use\\n        '\nstopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=[torch.tensor([i]).to(self.device) for i in stop_words_ids])])\n(img_embeds, atts_img) = self.encode_img(images.to(self.device), video_features.to(self.device))\nimage_lists = [[image_emb[None]] for image_emb in img_embeds]\nbatch_embs = [self.get_context_emb(text, img_list) for (text, img_list) in zip(texts, image_lists)]\nbatch_size = len(batch_embs)\nmax_len = max([emb.shape[1] for emb in batch_embs])\nemb_dim = batch_embs[0].shape[2]\ndtype = batch_embs[0].dtype\ndevice = batch_embs[0].device\nembs = torch.zeros([batch_size, max_len, emb_dim], dtype=dtype, device=device)\nattn_mask = torch.zeros([batch_size, max_len], dtype=torch.int, device=device)\nfor (i, emb) in enumerate(batch_embs):\n    emb_len = emb.shape[1]\n    embs[i, -emb_len:] = emb[0]\n    attn_mask[i, -emb_len:] = 1\nwith self.maybe_autocast():\n    outputs = self.llama_model.generate(inputs_embeds=embs, attention_mask=attn_mask, max_new_tokens=max_new_tokens, num_beams=num_beams, length_penalty=length_penalty, temperature=temperature, do_sample=do_sample, min_length=min_length, top_p=top_p, repetition_penalty=repetition_penalty)\nanswers = []\nfor output_token in outputs:\n    if output_token[0] == 0:\n        output_token = output_token[1:]\n    output_texts = self.llama_tokenizer.decode(output_token, skip_special_tokens=True)\n    output_texts = output_texts.split('</s>')[0]\n    output_texts = output_texts.replace('<s>', '')\n    output_texts = output_texts.split('[/INST]')[-1].strip()\n    answers.append(output_texts)\nreturn answers"
                    },
                    {
                      "name": "multi_select",
                      "decorators": [
                        "torch.no_grad()"
                      ],
                      "body": "all_losses = []\nfor answer in answers:\n    choice_samples = {'image': images, 'instruction_input': texts, 'answer': answer}\n    loss = self.forward(choice_samples, reduction='none')['loss'].reshape(-1, 1)\n    all_losses.append(loss)\n    torch.cuda.empty_cache()\nall_losses = torch.cat(all_losses, dim=-1)\nif num_cand is not None:\n    for i in range(all_losses.shape[0]):\n        all_losses[i, num_cand[i]:] = 9999\noutput_class_ranks = torch.argsort(all_losses, dim=-1)\nreturn output_class_ranks.tolist()"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/models/base_model.py",
              "type": "python",
              "imports": [
                "import os",
                "import logging",
                "import contextlib",
                "from omegaconf import OmegaConf",
                "import numpy as np",
                "import torch",
                "import torch.nn as nn",
                "from transformers import LlamaTokenizer",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training",
                "from minigpt4.common.dist_utils import download_cached_file",
                "from minigpt4.common.utils import get_abs_path, is_url",
                "from minigpt4.models.eva_vit import create_eva_vit_g",
                "from minigpt4.models.modeling_llama import LlamaForCausalLM"
              ],
              "classes": [
                {
                  "name": "BaseModel",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()"
                    },
                    {
                      "name": "device",
                      "decorators": [
                        "property"
                      ],
                      "body": "return list(self.parameters())[-1].device"
                    },
                    {
                      "name": "load_checkpoint",
                      "decorators": [],
                      "body": "'\\n        Load from a finetuned checkpoint.\\n\\n        This should expect no mismatch in the model keys and the checkpoint keys.\\n        '\nif is_url(url_or_filename):\n    cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n    checkpoint = torch.load(cached_file, map_location='cpu')\nelif os.path.isfile(url_or_filename):\n    checkpoint = torch.load(url_or_filename, map_location='cpu')\nelse:\n    raise RuntimeError('checkpoint url or path is invalid')\nif 'model' in checkpoint.keys():\n    state_dict = checkpoint['model']\nelse:\n    state_dict = checkpoint\nmsg = self.load_state_dict(state_dict, strict=False)\nlogging.info('Missing keys {}'.format(msg.missing_keys))\nlogging.info('load checkpoint from %s' % url_or_filename)\nreturn msg"
                    },
                    {
                      "name": "from_pretrained",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "'\\n        Build a pretrained model from default configuration file, specified by model_type.\\n\\n        Args:\\n            - model_type (str): model type, specifying architecture and checkpoints.\\n\\n        Returns:\\n            - model (nn.Module): pretrained or finetuned model, depending on the configuration.\\n        '\nmodel_cfg = OmegaConf.load(cls.default_config_path(model_type)).model\nmodel = cls.from_config(model_cfg)\nreturn model"
                    },
                    {
                      "name": "default_config_path",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "assert model_type in cls.PRETRAINED_MODEL_CONFIG_DICT, 'Unknown model type {}'.format(model_type)\nreturn get_abs_path(cls.PRETRAINED_MODEL_CONFIG_DICT[model_type])"
                    },
                    {
                      "name": "load_checkpoint_from_config",
                      "decorators": [],
                      "body": "'\\n        Load checkpoint as specified in the config file.\\n\\n        If load_finetuned is True, load the finetuned model; otherwise, load the pretrained model.\\n        When loading the pretrained model, each task-specific architecture may define their\\n        own load_from_pretrained() method.\\n        '\nload_finetuned = cfg.get('load_finetuned', True)\nif load_finetuned:\n    finetune_path = cfg.get('finetuned', None)\n    assert finetune_path is not None, 'Found load_finetuned is True, but finetune_path is None.'\n    self.load_checkpoint(url_or_filename=finetune_path)\nelse:\n    pretrain_path = cfg.get('pretrained', None)\n    assert 'Found load_finetuned is False, but pretrain_path is None.'\n    self.load_from_pretrained(url_or_filename=pretrain_path, **kwargs)"
                    },
                    {
                      "name": "before_evaluation",
                      "decorators": [],
                      "body": "pass"
                    },
                    {
                      "name": "show_n_params",
                      "decorators": [],
                      "body": "tot = 0\nfor p in self.parameters():\n    w = 1\n    for x in p.shape:\n        w *= x\n    tot += w\nif return_str:\n    if tot >= 1000000.0:\n        return '{:.1f}M'.format(tot / 1000000.0)\n    else:\n        return '{:.1f}K'.format(tot / 1000.0)\nelse:\n    return tot"
                    },
                    {
                      "name": "maybe_autocast",
                      "decorators": [],
                      "body": "enable_autocast = self.device != torch.device('cpu')\nif enable_autocast:\n    return torch.cuda.amp.autocast(dtype=dtype)\nelse:\n    return contextlib.nullcontext()"
                    },
                    {
                      "name": "init_vision_encoder",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "logging.info('Loading VIT')\nassert model_name == 'eva_clip_g', 'vit model must be eva_clip_g for current version of MiniGPT-4'\nif not freeze:\n    precision = 'fp32'\nvisual_encoder = create_eva_vit_g(img_size, drop_path_rate, use_grad_checkpoint, precision)\nln_vision = LayerNorm(visual_encoder.num_features)\nif freeze:\n    for (name, param) in visual_encoder.named_parameters():\n        param.requires_grad = False\n    visual_encoder = visual_encoder.eval()\n    visual_encoder.train = disabled_train\n    for (name, param) in ln_vision.named_parameters():\n        param.requires_grad = False\n    ln_vision = ln_vision.eval()\n    ln_vision.train = disabled_train\n    logging.info('freeze vision encoder')\nlogging.info('Loading VIT Done')\nreturn (visual_encoder, ln_vision)"
                    },
                    {
                      "name": "init_llm",
                      "decorators": [],
                      "body": "logging.info('Loading LLAMA')\nllama_tokenizer = LlamaTokenizer.from_pretrained(llama_model_path, use_fast=False)\nllama_tokenizer.pad_token = '$$'\nif low_resource:\n    llama_model = LlamaForCausalLM.from_pretrained(llama_model_path, torch_dtype=torch.float16, load_in_8bit=True, device_map={'': low_res_device})\nelse:\n    llama_model = LlamaForCausalLM.from_pretrained(llama_model_path, torch_dtype=torch.float16)\nif lora_r > 0:\n    llama_model = prepare_model_for_int8_training(llama_model)\n    loraconfig = LoraConfig(r=lora_r, bias='none', task_type='CAUSAL_LM', target_modules=lora_target_modules, **lora_kargs)\n    print('loraconfig:', loraconfig)\n    llama_model = get_peft_model(llama_model, loraconfig)\n    llama_model.print_trainable_parameters()\nelse:\n    for (name, param) in llama_model.named_parameters():\n        param.requires_grad = False\nlogging.info('Loading LLAMA Done')\nreturn (llama_model, llama_tokenizer)"
                    },
                    {
                      "name": "load_from_pretrained",
                      "decorators": [],
                      "body": "if is_url(url_or_filename):\n    cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n    checkpoint = torch.load(cached_file, map_location='cpu')\nelif os.path.isfile(url_or_filename):\n    checkpoint = torch.load(url_or_filename, map_location='cpu')\nelse:\n    raise RuntimeError('checkpoint url or path is invalid')\nstate_dict = checkpoint['model']\nmsg = self.load_state_dict(state_dict, strict=False)\nlogging.info('load checkpoint from %s' % url_or_filename)\nreturn msg"
                    }
                  ]
                },
                {
                  "name": "LayerNorm",
                  "decorators": [],
                  "inherits": [
                    "nn.LayerNorm"
                  ],
                  "methods": [
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "orig_type = x.dtype\nret = super().forward(x.type(torch.float32))\nreturn ret.type(orig_type)"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "disabled_train",
                  "decorators": [],
                  "body": "'Overwrite model.train with this function to make sure train/eval mode\\n    does not change anymore.'\nreturn self"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/models/minigpt4.py",
              "type": "python",
              "imports": [
                "import logging",
                "import random",
                "import torch",
                "from torch.cuda.amp import autocast as autocast",
                "import torch.nn as nn",
                "from minigpt4.common.registry import registry",
                "from minigpt4.models.base_model import disabled_train",
                "from minigpt4.models.minigpt_base import MiniGPTBase",
                "from minigpt4.models.Qformer import BertConfig, BertLMHeadModel"
              ],
              "classes": [
                {
                  "name": "MiniGPT4",
                  "decorators": [
                    "registry.register_model('minigpt4')"
                  ],
                  "inherits": [
                    "MiniGPTBase"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__(vit_model=vit_model, img_size=img_size, drop_path_rate=drop_path_rate, use_grad_checkpoint=use_grad_checkpoint, vit_precision=vit_precision, freeze_vit=freeze_vit, llama_model=llama_model, max_txt_len=max_txt_len, end_sym=end_sym, low_resource=low_resource, device_8bit=device_8bit)\nself.has_qformer = has_qformer\nif self.has_qformer:\n    print('Loading Q-Former')\n    (self.Qformer, self.query_tokens) = self.init_Qformer(num_query_token, self.visual_encoder.num_features, freeze_qformer)\n    self.load_from_pretrained(url_or_filename=q_former_model)\n    img_f_dim = self.Qformer.config.hidden_size\n    print('Loading Q-Former Done')\nelse:\n    img_f_dim = self.visual_encoder.num_features * 4\n    print('Do not use Q-Former here.')\nself.llama_proj = nn.Linear(img_f_dim, self.llama_model.config.hidden_size)\nif prompt_path:\n    with open(prompt_path, 'r') as f:\n        raw_prompts = f.read().splitlines()\n    filted_prompts = [raw_prompt for raw_prompt in raw_prompts if '<VideoHere>' in raw_prompt]\n    self.prompt_list = [prompt_template.format(p) for p in filted_prompts]\n    print('Load {} training prompts'.format(len(self.prompt_list)))\n    print('Prompt Example \\n{}'.format(random.choice(self.prompt_list)))\nelse:\n    self.prompt_list = []"
                    },
                    {
                      "name": "init_Qformer",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "encoder_config = BertConfig.from_pretrained('bert-base-uncased')\nencoder_config.encoder_width = vision_width\nencoder_config.add_cross_attention = True\nencoder_config.cross_attention_freq = 2\nencoder_config.query_length = num_query_token\nQformer = BertLMHeadModel(config=encoder_config)\nquery_tokens = nn.Parameter(torch.zeros(1, num_query_token, encoder_config.hidden_size))\nquery_tokens.data.normal_(mean=0.0, std=encoder_config.initializer_range)\nQformer.cls = None\nQformer.bert.embeddings.word_embeddings = None\nQformer.bert.embeddings.position_embeddings = None\nfor layer in Qformer.bert.encoder.layer:\n    layer.output = None\n    layer.intermediate = None\nif freeze:\n    for (name, param) in Qformer.named_parameters():\n        param.requires_grad = False\n    Qformer = Qformer.eval()\n    Qformer.train = disabled_train\n    query_tokens.requires_grad = False\n    logging.info('freeze Qformer')\nreturn (Qformer, query_tokens)"
                    },
                    {
                      "name": "encode_img",
                      "decorators": [],
                      "body": "device = image.device\nif len(image.shape) > 4:\n    image = image.reshape(-1, *image.shape[-3:])\nwith self.maybe_autocast():\n    image_embeds = self.ln_vision(self.visual_encoder(image)).to(device)\n    if self.has_qformer:\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(device)\n        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n        query_output = self.Qformer.bert(query_embeds=query_tokens, encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True)\n        inputs_llama = self.llama_proj(query_output.last_hidden_state)\n    else:\n        image_embeds = image_embeds[:, 1:, :]\n        (bs, pn, hs) = image_embeds.shape\n        image_embeds = image_embeds.view(bs, int(pn / 4), int(hs * 4))\n        inputs_llama = self.llama_proj(image_embeds)\n    atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(image.device)\nreturn (inputs_llama, atts_llama)"
                    },
                    {
                      "name": "from_config",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "vit_model = cfg.get('vit_model', 'eva_clip_g')\nq_former_model = cfg.get('q_former_model', 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth')\nimg_size = cfg.get('image_size')\nnum_query_token = cfg.get('num_query_token')\nllama_model = cfg.get('llama_model')\ndrop_path_rate = cfg.get('drop_path_rate', 0)\nuse_grad_checkpoint = cfg.get('use_grad_checkpoint', False)\nvit_precision = cfg.get('vit_precision', 'fp16')\nfreeze_vit = cfg.get('freeze_vit', True)\nhas_qformer = cfg.get('has_qformer', True)\nfreeze_qformer = cfg.get('freeze_qformer', True)\nlow_resource = cfg.get('low_resource', False)\ndevice_8bit = cfg.get('device_8bit', 0)\nprompt_path = cfg.get('prompt_path', '')\nprompt_template = cfg.get('prompt_template', '')\nmax_txt_len = cfg.get('max_txt_len', 32)\nend_sym = cfg.get('end_sym', '\\n')\nmodel = cls(vit_model=vit_model, q_former_model=q_former_model, img_size=img_size, drop_path_rate=drop_path_rate, use_grad_checkpoint=use_grad_checkpoint, vit_precision=vit_precision, freeze_vit=freeze_vit, has_qformer=has_qformer, freeze_qformer=freeze_qformer, num_query_token=num_query_token, llama_model=llama_model, prompt_path=prompt_path, prompt_template=prompt_template, max_txt_len=max_txt_len, end_sym=end_sym, low_resource=low_resource, device_8bit=device_8bit)\nckpt_path = cfg.get('ckpt', '')\nif ckpt_path:\n    print('Load MiniGPT-4 Checkpoint: {}'.format(ckpt_path))\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    msg = model.load_state_dict(ckpt['model'], strict=False)\nreturn model"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/models/modeling_llama.py",
              "type": "python",
              "imports": [
                "import math",
                "from typing import List, Optional, Tuple, Union",
                "import torch",
                "import torch.nn.functional as F",
                "from torch.nn import CrossEntropyLoss",
                "from transformers.utils import add_start_docstrings_to_model_forward, replace_return_docstrings",
                "from transformers.modeling_outputs import CausalLMOutputWithPast",
                "from transformers.models.llama.modeling_llama import LLAMA_INPUTS_DOCSTRING, _CONFIG_FOR_DOC",
                "from transformers.models.llama.modeling_llama import LlamaForCausalLM as LlamaForCausalLMOrig"
              ],
              "classes": [
                {
                  "name": "LlamaForCausalLM",
                  "decorators": [],
                  "inherits": [
                    "LlamaForCausalLMOrig"
                  ],
                  "methods": [
                    {
                      "name": "forward",
                      "decorators": [
                        "add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)",
                        "replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)"
                      ],
                      "body": "'\\n        Args:\\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\\n\\n        Returns:\\n\\n        Example:\\n\\n        ```python\\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\\n\\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\\n\\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\\n\\n        >>> # Generate\\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\\n        \"Hey, are you conscious? Can you talk to me?\\\\nI\\'m not conscious, but I can talk to you.\"\\n        ```'\noutput_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\noutput_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\nreturn_dict = return_dict if return_dict is not None else self.config.use_return_dict\noutputs = self.model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)\nhidden_states = outputs[0]\nif hasattr(self.config, 'pretraining_tp') and self.config.pretraining_tp > 1:\n    lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n    logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n    logits = torch.cat(logits, dim=-1)\nelse:\n    logits = self.lm_head(hidden_states)\nlogits = logits.float()\nloss = None\nif labels is not None:\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n    loss_fct = CrossEntropyLoss(reduction=reduction)\n    shift_logits = shift_logits.view(-1, self.config.vocab_size)\n    shift_labels = shift_labels.view(-1)\n    shift_labels = shift_labels.to(shift_logits.device)\n    loss = loss_fct(shift_logits, shift_labels)\n    if reduction == 'none':\n        loss = loss.view(logits.size(0), -1).mean(1)\nif not return_dict:\n    output = (logits,) + outputs[1:]\n    return (loss,) + output if loss is not None else output\nreturn CausalLMOutputWithPast(loss=loss, logits=logits, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/models/eva_vit.py",
              "type": "python",
              "imports": [
                "import math",
                "from functools import partial",
                "import torch",
                "import torch.nn as nn",
                "import torch.nn.functional as F",
                "import torch.utils.checkpoint as checkpoint",
                "from timm.models.layers import drop_path, to_2tuple, trunc_normal_",
                "from timm.models.registry import register_model",
                "from minigpt4.common.dist_utils import download_cached_file"
              ],
              "classes": [
                {
                  "name": "DropPath",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super(DropPath, self).__init__()\nself.drop_prob = drop_prob"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "return drop_path(x, self.drop_prob, self.training)"
                    },
                    {
                      "name": "extra_repr",
                      "decorators": [],
                      "body": "return 'p={}'.format(self.drop_prob)"
                    }
                  ]
                },
                {
                  "name": "Mlp",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nout_features = out_features or in_features\nhidden_features = hidden_features or in_features\nself.fc1 = nn.Linear(in_features, hidden_features)\nself.act = act_layer()\nself.fc2 = nn.Linear(hidden_features, out_features)\nself.drop = nn.Dropout(drop)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "x = self.fc1(x)\nx = self.act(x)\nx = self.fc2(x)\nx = self.drop(x)\nreturn x"
                    }
                  ]
                },
                {
                  "name": "Attention",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.num_heads = num_heads\nhead_dim = dim // num_heads\nif attn_head_dim is not None:\n    head_dim = attn_head_dim\nall_head_dim = head_dim * self.num_heads\nself.scale = qk_scale or head_dim ** (-0.5)\nself.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\nif qkv_bias:\n    self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n    self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\nelse:\n    self.q_bias = None\n    self.v_bias = None\nif window_size:\n    self.window_size = window_size\n    self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n    self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))\n    coords_h = torch.arange(window_size[0])\n    coords_w = torch.arange(window_size[1])\n    coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n    coords_flatten = torch.flatten(coords, 1)\n    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n    relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n    relative_coords[:, :, 0] += window_size[0] - 1\n    relative_coords[:, :, 1] += window_size[1] - 1\n    relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n    relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n    relative_position_index[1:, 1:] = relative_coords.sum(-1)\n    relative_position_index[0, 0:] = self.num_relative_distance - 3\n    relative_position_index[0:, 0] = self.num_relative_distance - 2\n    relative_position_index[0, 0] = self.num_relative_distance - 1\n    self.register_buffer('relative_position_index', relative_position_index)\nelse:\n    self.window_size = None\n    self.relative_position_bias_table = None\n    self.relative_position_index = None\nself.attn_drop = nn.Dropout(attn_drop)\nself.proj = nn.Linear(all_head_dim, dim)\nself.proj_drop = nn.Dropout(proj_drop)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "(B, N, C) = x.shape\nqkv_bias = None\nif self.q_bias is not None:\n    qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\nqkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\nqkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n(q, k, v) = (qkv[0], qkv[1], qkv[2])\nq = q * self.scale\nattn = q @ k.transpose(-2, -1)\nif self.relative_position_bias_table is not None:\n    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)\n    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n    attn = attn + relative_position_bias.unsqueeze(0)\nif rel_pos_bias is not None:\n    attn = attn + rel_pos_bias\nattn = attn.softmax(dim=-1)\nattn = self.attn_drop(attn)\nx = (attn @ v).transpose(1, 2).reshape(B, N, -1)\nx = self.proj(x)\nx = self.proj_drop(x)\nreturn x"
                    }
                  ]
                },
                {
                  "name": "Block",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.norm1 = norm_layer(dim)\nself.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\nself.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\nself.norm2 = norm_layer(dim)\nmlp_hidden_dim = int(dim * mlp_ratio)\nself.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\nif init_values is not None and init_values > 0:\n    self.gamma_1 = nn.Parameter(init_values * torch.ones(dim), requires_grad=True)\n    self.gamma_2 = nn.Parameter(init_values * torch.ones(dim), requires_grad=True)\nelse:\n    (self.gamma_1, self.gamma_2) = (None, None)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "if self.gamma_1 is None:\n    x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n    x = x + self.drop_path(self.mlp(self.norm2(x)))\nelse:\n    x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n    x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\nreturn x"
                    }
                  ]
                },
                {
                  "name": "PatchEmbed",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nimg_size = to_2tuple(img_size)\npatch_size = to_2tuple(patch_size)\nnum_patches = img_size[1] // patch_size[1] * (img_size[0] // patch_size[0])\nself.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\nself.img_size = img_size\nself.patch_size = patch_size\nself.num_patches = num_patches\nself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "(B, C, H, W) = x.shape\nassert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\nx = self.proj(x).flatten(2).transpose(1, 2)\nreturn x"
                    }
                  ]
                },
                {
                  "name": "RelativePositionBias",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.window_size = window_size\nself.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\nself.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))\ncoords_h = torch.arange(window_size[0])\ncoords_w = torch.arange(window_size[1])\ncoords = torch.stack(torch.meshgrid([coords_h, coords_w]))\ncoords_flatten = torch.flatten(coords, 1)\nrelative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\nrelative_coords = relative_coords.permute(1, 2, 0).contiguous()\nrelative_coords[:, :, 0] += window_size[0] - 1\nrelative_coords[:, :, 1] += window_size[1] - 1\nrelative_coords[:, :, 0] *= 2 * window_size[1] - 1\nrelative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\nrelative_position_index[1:, 1:] = relative_coords.sum(-1)\nrelative_position_index[0, 0:] = self.num_relative_distance - 3\nrelative_position_index[0:, 0] = self.num_relative_distance - 2\nrelative_position_index[0, 0] = self.num_relative_distance - 1\nself.register_buffer('relative_position_index', relative_position_index)"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)\nreturn relative_position_bias.permute(2, 0, 1).contiguous()"
                    }
                  ]
                },
                {
                  "name": "VisionTransformer",
                  "decorators": [],
                  "inherits": [
                    "nn.Module"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.image_size = img_size\nself.num_classes = num_classes\nself.num_features = self.embed_dim = embed_dim\nself.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\nnum_patches = self.patch_embed.num_patches\nself.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\nif use_abs_pos_emb:\n    self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\nelse:\n    self.pos_embed = None\nself.pos_drop = nn.Dropout(p=drop_rate)\nif use_shared_rel_pos_bias:\n    self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\nelse:\n    self.rel_pos_bias = None\nself.use_checkpoint = use_checkpoint\ndpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\nself.use_rel_pos_bias = use_rel_pos_bias\nself.blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None) for i in range(depth)])\nif self.pos_embed is not None:\n    trunc_normal_(self.pos_embed, std=0.02)\ntrunc_normal_(self.cls_token, std=0.02)\nself.apply(self._init_weights)\nself.fix_init_weight()"
                    },
                    {
                      "name": "fix_init_weight",
                      "decorators": [],
                      "body": "def rescale(param, layer_id):\n    param.div_(math.sqrt(2.0 * layer_id))\nfor (layer_id, layer) in enumerate(self.blocks):\n    rescale(layer.attn.proj.weight.data, layer_id + 1)\n    rescale(layer.mlp.fc2.weight.data, layer_id + 1)"
                    },
                    {
                      "name": "_init_weights",
                      "decorators": [],
                      "body": "if isinstance(m, nn.Linear):\n    trunc_normal_(m.weight, std=0.02)\n    if isinstance(m, nn.Linear) and m.bias is not None:\n        nn.init.constant_(m.bias, 0)\nelif isinstance(m, nn.LayerNorm):\n    nn.init.constant_(m.bias, 0)\n    nn.init.constant_(m.weight, 1.0)"
                    },
                    {
                      "name": "get_classifier",
                      "decorators": [],
                      "body": "return self.head"
                    },
                    {
                      "name": "reset_classifier",
                      "decorators": [],
                      "body": "self.num_classes = num_classes\nself.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()"
                    },
                    {
                      "name": "forward_features",
                      "decorators": [],
                      "body": "x = self.patch_embed(x)\n(batch_size, seq_len, _) = x.size()\ncls_tokens = self.cls_token.expand(batch_size, -1, -1)\nx = torch.cat((cls_tokens, x), dim=1)\nif self.pos_embed is not None:\n    x = x + self.pos_embed\nx = self.pos_drop(x)\nrel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\nfor blk in self.blocks:\n    if self.use_checkpoint:\n        x = checkpoint.checkpoint(blk, x, rel_pos_bias)\n    else:\n        x = blk(x, rel_pos_bias)\nreturn x"
                    },
                    {
                      "name": "forward",
                      "decorators": [],
                      "body": "x = self.forward_features(x)\nreturn x"
                    },
                    {
                      "name": "get_intermediate_layers",
                      "decorators": [],
                      "body": "x = self.patch_embed(x)\n(batch_size, seq_len, _) = x.size()\ncls_tokens = self.cls_token.expand(batch_size, -1, -1)\nx = torch.cat((cls_tokens, x), dim=1)\nif self.pos_embed is not None:\n    x = x + self.pos_embed\nx = self.pos_drop(x)\nfeatures = []\nrel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\nfor blk in self.blocks:\n    x = blk(x, rel_pos_bias)\n    features.append(x)\nreturn features"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "_cfg",
                  "decorators": [],
                  "body": "return {'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None, 'crop_pct': 0.9, 'interpolation': 'bicubic', 'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5), **kwargs}"
                },
                {
                  "name": "interpolate_pos_embed",
                  "decorators": [],
                  "body": "if 'pos_embed' in checkpoint_model:\n    pos_embed_checkpoint = checkpoint_model['pos_embed'].float()\n    embedding_size = pos_embed_checkpoint.shape[-1]\n    num_patches = model.patch_embed.num_patches\n    num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n    orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n    new_size = int(num_patches ** 0.5)\n    if orig_size != new_size:\n        print('Position interpolate from %dx%d to %dx%d' % (orig_size, orig_size, new_size, new_size))\n        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n        pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n        checkpoint_model['pos_embed'] = new_pos_embed"
                },
                {
                  "name": "convert_weights_to_fp16",
                  "decorators": [],
                  "body": "'Convert applicable model parameters to fp16'\n\ndef _convert_weights_to_fp16(l):\n    if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n        l.weight.data = l.weight.data.half()\n        if l.bias is not None:\n            l.bias.data = l.bias.data.half()\nmodel.apply(_convert_weights_to_fp16)"
                },
                {
                  "name": "create_eva_vit_g",
                  "decorators": [],
                  "body": "model = VisionTransformer(img_size=img_size, patch_size=14, use_mean_pooling=False, embed_dim=1408, depth=39, num_heads=1408 // 88, mlp_ratio=4.3637, qkv_bias=True, drop_path_rate=drop_path_rate, norm_layer=partial(nn.LayerNorm, eps=1e-06), use_checkpoint=use_checkpoint)\nurl = 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/eva_vit_g.pth'\ncached_file = download_cached_file(url, check_hash=False, progress=True)\nstate_dict = torch.load(cached_file, map_location='cpu')\ninterpolate_pos_embed(model, state_dict)\nincompatible_keys = model.load_state_dict(state_dict, strict=False)\nif precision == 'fp16':\n    convert_weights_to_fp16(model)\nreturn model"
                }
              ]
            }
          ],
          "subdirs": []
        },
        {
          "directory": "datasets",
          "path": "ellama_codebase/minigpt4/datasets",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/datasets/data_utils.py",
              "type": "python",
              "imports": [
                "import gzip",
                "import logging",
                "import os",
                "import random as rnd",
                "import tarfile",
                "import zipfile",
                "import random",
                "from typing import List",
                "from tqdm import tqdm",
                "import decord",
                "from decord import VideoReader",
                "import webdataset as wds",
                "import numpy as np",
                "import torch",
                "from torch.utils.data.dataset import IterableDataset",
                "from minigpt4.common.registry import registry",
                "from minigpt4.datasets.datasets.base_dataset import ConcatDataset"
              ],
              "classes": [
                {
                  "name": "ChainDataset",
                  "decorators": [],
                  "inherits": [
                    "wds.DataPipeline"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__()\nself.datasets = datasets\nself.prob = []\nself.names = []\nfor dataset in self.datasets:\n    if hasattr(dataset, 'name'):\n        self.names.append(dataset.name)\n    else:\n        self.names.append('Unknown')\n    if hasattr(dataset, 'sample_ratio'):\n        self.prob.append(dataset.sample_ratio)\n    else:\n        self.prob.append(1)\n        logging.info(\"One of the datapipeline doesn't define ratio and set to 1 automatically.\")"
                    },
                    {
                      "name": "__iter__",
                      "decorators": [],
                      "body": "datastreams = [iter(dataset) for dataset in self.datasets]\nwhile True:\n    select_datastream = random.choices(datastreams, weights=self.prob, k=1)[0]\n    yield next(select_datastream)"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "apply_to_sample",
                  "decorators": [],
                  "body": "if len(sample) == 0:\n    return {}\n\ndef _apply(x):\n    if torch.is_tensor(x):\n        return f(x)\n    elif isinstance(x, dict):\n        return {key: _apply(value) for (key, value) in x.items()}\n    elif isinstance(x, list):\n        return [_apply(x) for x in x]\n    else:\n        return x\nreturn _apply(sample)"
                },
                {
                  "name": "move_to_cuda",
                  "decorators": [],
                  "body": "def _move_to_cuda(tensor):\n    return tensor.cuda()\nreturn apply_to_sample(_move_to_cuda, sample)"
                },
                {
                  "name": "prepare_sample",
                  "decorators": [],
                  "body": "if cuda_enabled:\n    samples = move_to_cuda(samples)\nreturn samples"
                },
                {
                  "name": "reorg_datasets_by_split",
                  "decorators": [],
                  "body": "'\\n    Organizes datasets by split.\\n\\n    Args:\\n        datasets: dict of torch.utils.data.Dataset objects by name.\\n\\n    Returns:\\n        Dict of datasets by split {split_name: List[Datasets]}.\\n    '\nreorg_datasets = dict()\nreorg_batch_sizes = dict()\nfor (dataset_name, dataset) in datasets.items():\n    for (split_name, dataset_split) in dataset.items():\n        if split_name not in reorg_datasets:\n            reorg_datasets[split_name] = [dataset_split]\n            reorg_batch_sizes[split_name] = [batch_sizes[dataset_name]]\n        else:\n            reorg_datasets[split_name].append(dataset_split)\n            reorg_batch_sizes[split_name].append(batch_sizes[dataset_name])\nreturn (reorg_datasets, reorg_batch_sizes)"
                },
                {
                  "name": "concat_datasets",
                  "decorators": [],
                  "body": "'\\n    Concatenates multiple datasets into a single dataset.\\n\\n    It supports may-style datasets and DataPipeline from WebDataset. Currently, does not support\\n    generic IterableDataset because it requires creating separate samplers.\\n\\n    Now only supports conctenating training datasets and assuming validation and testing\\n    have only a single dataset. This is because metrics should not be computed on the concatenated\\n    datasets.\\n\\n    Args:\\n        datasets: dict of torch.utils.data.Dataset objects by split.\\n\\n    Returns:\\n        Dict of concatenated datasets by split, \"train\" is the concatenation of multiple datasets,\\n        \"val\" and \"test\" remain the same.\\n\\n        If the input training datasets contain both map-style and DataPipeline datasets, returns\\n        a tuple, where the first element is a concatenated map-style dataset and the second\\n        element is a chained DataPipeline dataset.\\n\\n    '\nfor split_name in datasets:\n    if split_name != 'train':\n        assert len(datasets[split_name]) == 1, 'Do not support multiple {} datasets.'.format(split_name)\n        datasets[split_name] = datasets[split_name][0]\n    else:\n        (iterable_datasets, map_datasets) = ([], [])\n        for dataset in datasets[split_name]:\n            if isinstance(dataset, wds.DataPipeline):\n                logging.info(\"Dataset {} is IterableDataset, can't be concatenated.\".format(dataset))\n                iterable_datasets.append(dataset)\n            elif isinstance(dataset, IterableDataset):\n                raise NotImplementedError('Do not support concatenation of generic IterableDataset.')\n            else:\n                map_datasets.append(dataset)\n        if len(iterable_datasets) > 1:\n            chained_datasets = ChainDataset(iterable_datasets)\n        elif len(iterable_datasets) == 1:\n            chained_datasets = iterable_datasets[0]\n        else:\n            chained_datasets = None\n        concat_datasets = ConcatDataset(map_datasets) if len(map_datasets) > 0 else None\n        train_datasets = (concat_datasets, chained_datasets)\n        train_datasets = tuple([x for x in train_datasets if x is not None])\n        train_datasets = train_datasets[0] if len(train_datasets) == 1 else train_datasets\n        datasets[split_name] = train_datasets\nreturn datasets"
                }
              ]
            }
          ],
          "subdirs": [
            {
              "directory": "builders",
              "path": "ellama_codebase/minigpt4/datasets/builders",
              "files": [
                {
                  "file_name": "ellama_codebase/minigpt4/datasets/builders/image_text_pair_builder.py",
                  "type": "python",
                  "imports": [
                    "import os",
                    "import logging",
                    "import warnings",
                    "from minigpt4.common.registry import registry",
                    "from minigpt4.datasets.builders.base_dataset_builder import BaseDatasetBuilder",
                    "from minigpt4.datasets.datasets.first_face import FeatureFaceDataset"
                  ],
                  "classes": [
                    {
                      "name": "FirstfaceCaptionBuilder",
                      "decorators": [
                        "registry.register_builder('feature_face_caption')"
                      ],
                      "inherits": [
                        "BaseDatasetBuilder"
                      ],
                      "methods": [
                        {
                          "name": "_download_ann",
                          "decorators": [],
                          "body": "pass"
                        },
                        {
                          "name": "_download_vis",
                          "decorators": [],
                          "body": "pass"
                        },
                        {
                          "name": "build",
                          "decorators": [],
                          "body": "self.build_processors()\nbuild_info = self.config.build_info\ndatasets = dict()\nsplit = 'train'\ndataset_cls = self.train_dataset_cls\ndatasets[split] = dataset_cls(vis_processor=self.vis_processors[split], text_processor=self.text_processors[split], ann_path=build_info.ann_path, vis_root=build_info.image_path)\nreturn datasets"
                        }
                      ]
                    }
                  ],
                  "other_functions": []
                },
                {
                  "file_name": "ellama_codebase/minigpt4/datasets/builders/__init__.py",
                  "type": "python",
                  "imports": [
                    "from minigpt4.datasets.builders.base_dataset_builder import load_dataset_config",
                    "from minigpt4.datasets.builders.image_text_pair_builder import FirstfaceCaptionBuilder",
                    "from minigpt4.common.registry import registry"
                  ],
                  "classes": [
                    {
                      "name": "DatasetZoo",
                      "decorators": [],
                      "inherits": [],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "self.dataset_zoo = {k: list(v.DATASET_CONFIG_DICT.keys()) for (k, v) in sorted(registry.mapping['builder_name_mapping'].items())}"
                        },
                        {
                          "name": "get_names",
                          "decorators": [],
                          "body": "return list(self.dataset_zoo.keys())"
                        }
                      ]
                    }
                  ],
                  "other_functions": [
                    {
                      "name": "load_dataset",
                      "decorators": [],
                      "body": "'\\n    Example\\n\\n    >>> dataset = load_dataset(\"coco_caption\", cfg=None)\\n    >>> splits = dataset.keys()\\n    >>> print([len(dataset[split]) for split in splits])\\n\\n    '\nif cfg_path is None:\n    cfg = None\nelse:\n    cfg = load_dataset_config(cfg_path)\ntry:\n    builder = registry.get_builder_class(name)(cfg)\nexcept TypeError:\n    print(f'Dataset {name} not found. Available datasets:\\n' + ', '.join([str(k) for k in dataset_zoo.get_names()]))\n    exit(1)\nif vis_path is not None:\n    if data_type is None:\n        data_type = builder.config.data_type\n    assert data_type in builder.config.build_info, f'Invalid data_type {data_type} for {name}.'\n    builder.config.build_info.get(data_type).storage = vis_path\ndataset = builder.build_datasets()\nreturn dataset"
                    }
                  ]
                },
                {
                  "file_name": "ellama_codebase/minigpt4/datasets/builders/base_dataset_builder.py",
                  "type": "python",
                  "imports": [
                    "import logging",
                    "import os",
                    "import shutil",
                    "import warnings",
                    "from omegaconf import OmegaConf",
                    "import torch.distributed as dist",
                    "from torchvision.datasets.utils import download_url",
                    "import minigpt4.common.utils as utils",
                    "from minigpt4.common.dist_utils import is_dist_avail_and_initialized, is_main_process",
                    "from minigpt4.common.registry import registry",
                    "from minigpt4.processors.base_processor import BaseProcessor"
                  ],
                  "classes": [
                    {
                      "name": "BaseDatasetBuilder",
                      "decorators": [],
                      "inherits": [],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "super().__init__()\nif cfg is None:\n    self.config = load_dataset_config(self.default_config_path())\nelif isinstance(cfg, str):\n    self.config = load_dataset_config(cfg)\nelse:\n    self.config = cfg\nself.data_type = self.config.data_type\nprint('BaseDatasetBuilder data type:', self.data_type)\nself.vis_processors = {'train': BaseProcessor(), 'eval': BaseProcessor()}\nself.text_processors = {'train': BaseProcessor(), 'eval': BaseProcessor()}"
                        },
                        {
                          "name": "build_datasets",
                          "decorators": [],
                          "body": "if is_main_process():\n    self._download_data()\nif is_dist_avail_and_initialized():\n    dist.barrier()\nlogging.info('Building datasets...')\ndatasets = self.build()\nreturn datasets"
                        },
                        {
                          "name": "build_processors",
                          "decorators": [],
                          "body": "vis_proc_cfg = self.config.get('vis_processor')\ntxt_proc_cfg = self.config.get('text_processor')\nif vis_proc_cfg is not None:\n    vis_train_cfg = vis_proc_cfg.get('train')\n    vis_eval_cfg = vis_proc_cfg.get('eval')\n    self.vis_processors['train'] = self._build_proc_from_cfg(vis_train_cfg)\n    self.vis_processors['eval'] = self._build_proc_from_cfg(vis_eval_cfg)\nif txt_proc_cfg is not None:\n    txt_train_cfg = txt_proc_cfg.get('train')\n    txt_eval_cfg = txt_proc_cfg.get('eval')\n    self.text_processors['train'] = self._build_proc_from_cfg(txt_train_cfg)\n    self.text_processors['eval'] = self._build_proc_from_cfg(txt_eval_cfg)"
                        },
                        {
                          "name": "_build_proc_from_cfg",
                          "decorators": [
                            "staticmethod"
                          ],
                          "body": "return registry.get_processor_class(cfg.name).from_config(cfg) if cfg is not None else None"
                        },
                        {
                          "name": "default_config_path",
                          "decorators": [
                            "classmethod"
                          ],
                          "body": "return utils.get_abs_path(cls.DATASET_CONFIG_DICT[type])"
                        },
                        {
                          "name": "_download_data",
                          "decorators": [],
                          "body": "self._download_ann()\nself._download_vis()"
                        },
                        {
                          "name": "_download_ann",
                          "decorators": [],
                          "body": "'\\n        Download annotation files if necessary.\\n        All the vision-language datasets should have annotations of unified format.\\n\\n        storage_path can be:\\n          (1) relative/absolute: will be prefixed with env.cache_root to make full path if relative.\\n          (2) basename/dirname: will be suffixed with base name of URL if dirname is provided.\\n\\n        Local annotation paths should be relative.\\n        '\nanns = self.config.build_info.annotations\nsplits = anns.keys()\ncache_root = registry.get_path('cache_root')\nfor split in splits:\n    info = anns[split]\n    (urls, storage_paths) = (info.get('url', None), info.storage)\n    if isinstance(urls, str):\n        urls = [urls]\n    if isinstance(storage_paths, str):\n        storage_paths = [storage_paths]\n    assert len(urls) == len(storage_paths)\n    for (url_or_filename, storage_path) in zip(urls, storage_paths):\n        if not os.path.isabs(storage_path):\n            storage_path = os.path.join(cache_root, storage_path)\n        dirname = os.path.dirname(storage_path)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        if os.path.isfile(url_or_filename):\n            (src, dst) = (url_or_filename, storage_path)\n            if not os.path.exists(dst):\n                shutil.copyfile(src=src, dst=dst)\n            else:\n                logging.info('Using existing file {}.'.format(dst))\n        else:\n            if os.path.isdir(storage_path):\n                raise ValueError('Expecting storage_path to be a file path, got directory {}'.format(storage_path))\n            else:\n                filename = os.path.basename(storage_path)\n            download_url(url=url_or_filename, root=dirname, filename=filename)"
                        },
                        {
                          "name": "_download_vis",
                          "decorators": [],
                          "body": "storage_path = self.config.build_info.get(self.data_type).storage\nstorage_path = utils.get_cache_path(storage_path)\nif not os.path.exists(storage_path):\n    warnings.warn(f'\\n                The specified path {storage_path} for visual inputs does not exist.\\n                Please provide a correct path to the visual inputs or\\n                refer to datasets/download_scripts/README.md for downloading instructions.\\n                ')"
                        },
                        {
                          "name": "build",
                          "decorators": [],
                          "body": "'\\n        Create by split datasets inheriting torch.utils.data.Datasets.\\n\\n        # build() can be dataset-specific. Overwrite to customize.\\n        '\nself.build_processors()\nbuild_info = self.config.build_info\nann_info = build_info.annotations\nvis_info = build_info.get(self.data_type)\ndatasets = dict()\nfor split in ann_info.keys():\n    if split not in ['train', 'val', 'test']:\n        continue\n    is_train = split == 'train'\n    vis_processor = self.vis_processors['train'] if is_train else self.vis_processors['eval']\n    text_processor = self.text_processors['train'] if is_train else self.text_processors['eval']\n    ann_paths = ann_info.get(split).storage\n    if isinstance(ann_paths, str):\n        ann_paths = [ann_paths]\n    abs_ann_paths = []\n    for ann_path in ann_paths:\n        if not os.path.isabs(ann_path):\n            ann_path = utils.get_cache_path(ann_path)\n        abs_ann_paths.append(ann_path)\n    ann_paths = abs_ann_paths\n    vis_path = os.path.join(vis_info.storage, split)\n    if not os.path.isabs(vis_path):\n        vis_path = utils.get_cache_path(vis_path)\n    if not os.path.exists(vis_path):\n        warnings.warn('storage path {} does not exist.'.format(vis_path))\n    dataset_cls = self.train_dataset_cls if is_train else self.eval_dataset_cls\n    datasets[split] = dataset_cls(vis_processor=vis_processor, text_processor=text_processor, ann_paths=ann_paths, vis_root=vis_path)\nreturn datasets"
                        }
                      ]
                    }
                  ],
                  "other_functions": [
                    {
                      "name": "load_dataset_config",
                      "decorators": [],
                      "body": "cfg = OmegaConf.load(cfg_path).datasets\ncfg = cfg[list(cfg.keys())[0]]\nreturn cfg"
                    }
                  ]
                }
              ],
              "subdirs": []
            },
            {
              "directory": "datasets",
              "path": "ellama_codebase/minigpt4/datasets/datasets",
              "files": [
                {
                  "file_name": "ellama_codebase/minigpt4/datasets/datasets/base_dataset.py",
                  "type": "python",
                  "imports": [
                    "import json",
                    "from typing import Iterable",
                    "from torch.utils.data import Dataset, ConcatDataset",
                    "from torch.utils.data.dataloader import default_collate"
                  ],
                  "classes": [
                    {
                      "name": "BaseDataset",
                      "decorators": [],
                      "inherits": [
                        "Dataset"
                      ],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "'\\n        vis_root (string): Root directory of images (e.g. coco/images/)\\n        ann_root (string): directory to store the annotation file\\n        '\nself.vis_root = vis_root\nself.annotation = []\nfor ann_path in ann_paths:\n    ann = json.load(open(ann_path, 'r'))\n    if isinstance(ann, dict):\n        self.annotation.extend(json.load(open(ann_path, 'r'))['annotations'])\n    else:\n        self.annotation.extend(json.load(open(ann_path, 'r')))\nself.vis_processor = vis_processor\nself.text_processor = text_processor\nself._add_instance_ids()"
                        },
                        {
                          "name": "__len__",
                          "decorators": [],
                          "body": "return len(self.annotation)"
                        },
                        {
                          "name": "collater",
                          "decorators": [],
                          "body": "return default_collate(samples)"
                        },
                        {
                          "name": "set_processors",
                          "decorators": [],
                          "body": "self.vis_processor = vis_processor\nself.text_processor = text_processor"
                        },
                        {
                          "name": "_add_instance_ids",
                          "decorators": [],
                          "body": "for (idx, ann) in enumerate(self.annotation):\n    ann[key] = str(idx)"
                        }
                      ]
                    },
                    {
                      "name": "ConcatDataset",
                      "decorators": [],
                      "inherits": [
                        "ConcatDataset"
                      ],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "super().__init__(datasets)"
                        },
                        {
                          "name": "collater",
                          "decorators": [],
                          "body": "all_keys = set()\nfor s in samples:\n    all_keys.update(s)\nshared_keys = all_keys\nfor s in samples:\n    shared_keys = shared_keys & set(s.keys())\nsamples_shared_keys = []\nfor s in samples:\n    samples_shared_keys.append({k: s[k] for k in s.keys() if k in shared_keys})\nreturn self.datasets[0].collater(samples_shared_keys)"
                        }
                      ]
                    }
                  ],
                  "other_functions": []
                },
                {
                  "file_name": "ellama_codebase/minigpt4/datasets/datasets/first_face.py",
                  "type": "python",
                  "imports": [
                    "import glob",
                    "import os",
                    "import json",
                    "import pickle",
                    "import random",
                    "import time",
                    "import itertools",
                    "import pandas as pd",
                    "import json",
                    "import torch.nn.functional as F",
                    "import numpy as np",
                    "from PIL import Image",
                    "import skimage.io as io",
                    "import matplotlib.pyplot as plt",
                    "from matplotlib.collections import PatchCollection",
                    "from matplotlib.patches import Polygon, Rectangle",
                    "import torch",
                    "from torch.utils.data import Dataset",
                    "import webdataset as wds",
                    "import cv2",
                    "from minigpt4.datasets.datasets.base_dataset import BaseDataset"
                  ],
                  "classes": [
                    {
                      "name": "FeatureFaceDataset",
                      "decorators": [],
                      "inherits": [
                        "Dataset"
                      ],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "self.vis_root = vis_root\nself.vis_processor = vis_processor\nself.text_processor = text_processor\nself.caption_instruction_pool = ['Please describe the details of the expression and tone the video.', 'Can you provide a description of the facial expression and tone shown by the person in the video?', 'Could you outline the facial expressions and vocal tones displayed in the video?', 'Detail the expressions and tone used in the video.', 'Explain the visual and auditory expressions captured in the video.', 'Provide an analysis of the expressions and tone featured in the video.']\nself.emotion_instruction_pool = ['Please determine which emotion label in the video represents: happy, sad, neutral, angry, worried, surprise, fear, contempt, doubt.']\nself.reason_instruction_pool = ['Please analyze all the clues in the video and reason out the emotional label of the person in the video.', 'What is the emotional state of the person in the video? Please tell me the reason.', 'What are the facial expressions and vocal tone used in the video? What is the intended meaning behind his words? Which emotion does this reflect?', 'Please integrate information from various modalities to infer the emotional category of the person in the video.', 'Could you describe the emotion-related features of the individual in the video? What emotional category do they fall into?']\nself.task_pool = ['emotion']\nprint('ann_path: ', ann_path)\nself.ann_path = ann_path\nself.file_path = os.path.dirname(ann_path)\nself.tmp = [x.strip().split(' ') for x in open(ann_path)]\nprint('video number:%d' % len(self.tmp))\nemos = ['neutral', 'angry', 'happy', 'sad', 'worried', 'surprise', 'fear', 'contempt', 'doubt']\n(self.emo2idx, self.idx2emo) = ({}, {})\nfor (ii, emo) in enumerate(emos):\n    self.emo2idx[emo] = ii\nfor (ii, emo) in enumerate(emos):\n    self.emo2idx[ii] = emo\njson_file_path = '/home/user/selected_face/face_emotion/AU_filter_merge.json'\nwith open(json_file_path, 'r') as json_file:\n    self.AU_filter_json = json.load(json_file)\nreason_json_file_path = '/home/user/selected_face/face_emotion/0512_target_smp_end.json'\nwith open(reason_json_file_path, 'r') as json_file:\n    self.reason_dict = json.load(json_file)\nself.character_lines = pd.read_csv('/home/user/selected_face/face_emotion/transcription_en_all.csv')"
                        },
                        {
                          "name": "__len__",
                          "decorators": [],
                          "body": "return len(self.tmp)"
                        },
                        {
                          "name": "__getitem__",
                          "decorators": [],
                          "body": "t = self.tmp[index]\nvideo_name = t[0]\nimage_file = '{}.jpg'.format(video_name)\nimage_path = os.path.join(self.vis_root, image_file)\nimage = Image.open(image_path).convert('RGB')\nimage = self.vis_processor(image)\n(FaceMAE_feats, VideoMAE_feats, Audio_feats) = self.get(video_name)\nif len(VideoMAE_feats.shape) == 1:\n    VideoMAE_feats = VideoMAE_feats.unsqueeze(0)\nif len(Audio_feats.shape) == 1:\n    Audio_feats = Audio_feats.unsqueeze(0)\nif len(FaceMAE_feats.shape) == 1:\n    FaceMAE_feats = FaceMAE_feats.unsqueeze(0)\nvideo_features = torch.cat((FaceMAE_feats, VideoMAE_feats, Audio_feats), dim=0)\ntask = random.choice(self.task_pool)\nif task == 'emotion':\n    caption = t[2]\n    caption = self.text_processor(caption)\n    instruction_pool = self.emotion_instruction_pool\nelif task == 'reason':\n    caption = self.reason_dict[video_name]['smp_reason_caption']\n    infer_str = ' Therefore, it is inferred that his emotional state is: '\n    caption = caption + infer_str + t[2]\n    caption = self.text_processor(caption)\n    instruction_pool = self.reason_instruction_pool\nelif task == 'infer':\n    infer_str = ' Therefore, it is inferred that his emotional state is: '\n    caption = t[2]\n    instruction_pool = [self.reason_dict[video_name]['reason_caption'] + infer_str]\nelif task == 'caption':\n    caption = self.AU_filter_json[video_name]['caption']\n    caption = self.text_processor(caption)\n    instruction_pool = self.caption_instruction_pool\nemotion = self.emo2idx[t[2]]\nsentence = self.character_lines.loc[self.character_lines['name'] == video_name, 'sentence'].values[0]\ncharacter_line = 'The person in video says: {}. '.format(sentence)\ninstruction = '<video><VideoHere></video> <feature><FeatureHere></feature> {} [{}] {} '.format(character_line, task, random.choice(instruction_pool))\nreturn {'image': image, 'video_features': video_features, 'instruction_input': instruction, 'answer': caption, 'emotion': emotion, 'image_id': video_name}"
                        },
                        {
                          "name": "get",
                          "decorators": [],
                          "body": "FaceMAE_feats_path = os.path.join(self.file_path, 'mae_340_UTT', video_name + '.npy')\nFaceMAE_feats = torch.tensor(np.load(FaceMAE_feats_path))\nVideoMAE_feats_path = os.path.join(self.file_path, 'maeV_399_UTT', video_name + '.npy')\nVideoMAE_feats = torch.tensor(np.load(VideoMAE_feats_path))\nAudio_feats_path = os.path.join(self.file_path, 'HL-UTT', video_name + '.npy')\nAudio_feats = torch.tensor(np.load(Audio_feats_path))\nreturn (FaceMAE_feats, VideoMAE_feats, Audio_feats)"
                        }
                      ]
                    }
                  ],
                  "other_functions": []
                },
                {
                  "file_name": "ellama_codebase/minigpt4/datasets/datasets/dataloader_utils.py",
                  "type": "python",
                  "imports": [
                    "import time",
                    "import random",
                    "import torch",
                    "from minigpt4.datasets.data_utils import move_to_cuda",
                    "from torch.utils.data import DataLoader"
                  ],
                  "classes": [
                    {
                      "name": "MultiIterLoader",
                      "decorators": [],
                      "inherits": [],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "for loader in loaders:\n    assert hasattr(loader, '__next__'), 'Loader {} has no __next__ method.'.format(loader)\nif ratios is None:\n    ratios = [1.0] * len(loaders)\nelse:\n    assert len(ratios) == len(loaders)\n    ratios = [float(ratio) / sum(ratios) for ratio in ratios]\nself.loaders = loaders\nself.ratios = ratios"
                        },
                        {
                          "name": "__next__",
                          "decorators": [],
                          "body": "loader_idx = random.choices(range(len(self.loaders)), self.ratios, k=1)[0]\nreturn next(self.loaders[loader_idx])"
                        }
                      ]
                    },
                    {
                      "name": "PrefetchLoader",
                      "decorators": [],
                      "inherits": [
                        "object"
                      ],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "self.loader = loader\nself.stream = torch.cuda.Stream()"
                        },
                        {
                          "name": "__iter__",
                          "decorators": [],
                          "body": "loader_it = iter(self.loader)\nself.preload(loader_it)\nbatch = self.next(loader_it)\nwhile batch is not None:\n    is_tuple = isinstance(batch, tuple)\n    if is_tuple:\n        (task, batch) = batch\n    if is_tuple:\n        yield (task, batch)\n    else:\n        yield batch\n    batch = self.next(loader_it)"
                        },
                        {
                          "name": "__len__",
                          "decorators": [],
                          "body": "return len(self.loader)"
                        },
                        {
                          "name": "preload",
                          "decorators": [],
                          "body": "try:\n    self.batch = next(it)\nexcept StopIteration:\n    self.batch = None\n    return\nwith torch.cuda.stream(self.stream):\n    self.batch = move_to_cuda(self.batch)"
                        },
                        {
                          "name": "next",
                          "decorators": [],
                          "body": "torch.cuda.current_stream().wait_stream(self.stream)\nbatch = self.batch\nif batch is not None:\n    record_cuda_stream(batch)\nself.preload(it)\nreturn batch"
                        },
                        {
                          "name": "__getattr__",
                          "decorators": [],
                          "body": "method = self.loader.__getattribute__(name)\nreturn method"
                        }
                      ]
                    },
                    {
                      "name": "IterLoader",
                      "decorators": [],
                      "inherits": [],
                      "methods": [
                        {
                          "name": "__init__",
                          "decorators": [],
                          "body": "self._dataloader = dataloader\nself.iter_loader = iter(self._dataloader)\nself._use_distributed = use_distributed\nself._epoch = 0"
                        },
                        {
                          "name": "epoch",
                          "decorators": [
                            "property"
                          ],
                          "body": "return self._epoch"
                        },
                        {
                          "name": "__next__",
                          "decorators": [],
                          "body": "try:\n    data = next(self.iter_loader)\nexcept StopIteration:\n    self._epoch += 1\n    if hasattr(self._dataloader.sampler, 'set_epoch') and self._use_distributed:\n        self._dataloader.sampler.set_epoch(self._epoch)\n    time.sleep(2)\n    self.iter_loader = iter(self._dataloader)\n    data = next(self.iter_loader)\nreturn data"
                        },
                        {
                          "name": "__iter__",
                          "decorators": [],
                          "body": "return self"
                        },
                        {
                          "name": "__len__",
                          "decorators": [],
                          "body": "return len(self._dataloader)"
                        }
                      ]
                    }
                  ],
                  "other_functions": [
                    {
                      "name": "record_cuda_stream",
                      "decorators": [],
                      "body": "if isinstance(batch, torch.Tensor):\n    batch.record_stream(torch.cuda.current_stream())\nelif isinstance(batch, list) or isinstance(batch, tuple):\n    for t in batch:\n        record_cuda_stream(t)\nelif isinstance(batch, dict):\n    for t in batch.values():\n        record_cuda_stream(t)\nelse:\n    pass"
                    }
                  ]
                }
              ],
              "subdirs": []
            }
          ]
        },
        {
          "directory": "processors",
          "path": "ellama_codebase/minigpt4/processors",
          "files": [
            {
              "file_name": "ellama_codebase/minigpt4/processors/base_processor.py",
              "type": "python",
              "imports": [
                "from omegaconf import OmegaConf"
              ],
              "classes": [
                {
                  "name": "BaseProcessor",
                  "decorators": [],
                  "inherits": [],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.transform = lambda x: x\nreturn"
                    },
                    {
                      "name": "__call__",
                      "decorators": [],
                      "body": "return self.transform(item)"
                    },
                    {
                      "name": "from_config",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "return cls()"
                    },
                    {
                      "name": "build",
                      "decorators": [],
                      "body": "cfg = OmegaConf.create(kwargs)\nreturn self.from_config(cfg)"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/processors/__init__.py",
              "type": "python",
              "imports": [
                "from minigpt4.processors.base_processor import BaseProcessor",
                "from minigpt4.processors.blip_processors import Blip2ImageTrainProcessor, Blip2ImageEvalProcessor, BlipCaptionProcessor",
                "from minigpt4.common.registry import registry"
              ],
              "classes": [],
              "other_functions": [
                {
                  "name": "load_processor",
                  "decorators": [],
                  "body": "'\\n    Example\\n\\n    >>> processor = load_processor(\"alpro_video_train\", cfg=None)\\n    '\nprocessor = registry.get_processor_class(name).from_config(cfg)\nreturn processor"
                }
              ]
            },
            {
              "file_name": "ellama_codebase/minigpt4/processors/blip_processors.py",
              "type": "python",
              "imports": [
                "import re",
                "from minigpt4.common.registry import registry",
                "from minigpt4.processors.base_processor import BaseProcessor",
                "from minigpt4.processors.randaugment import RandomAugment",
                "from omegaconf import OmegaConf",
                "from torchvision import transforms",
                "from torchvision.transforms.functional import InterpolationMode"
              ],
              "classes": [
                {
                  "name": "BlipImageBaseProcessor",
                  "decorators": [],
                  "inherits": [
                    "BaseProcessor"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "if mean is None:\n    mean = (0.48145466, 0.4578275, 0.40821073)\nif std is None:\n    std = (0.26862954, 0.26130258, 0.27577711)\nself.normalize = transforms.Normalize(mean, std)"
                    }
                  ]
                },
                {
                  "name": "BlipCaptionProcessor",
                  "decorators": [
                    "registry.register_processor('blip_caption')"
                  ],
                  "inherits": [
                    "BaseProcessor"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.prompt = prompt\nself.max_words = max_words"
                    },
                    {
                      "name": "__call__",
                      "decorators": [],
                      "body": "caption = self.prompt + self.pre_caption(caption)\nreturn caption"
                    },
                    {
                      "name": "from_config",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "if cfg is None:\n    cfg = OmegaConf.create()\nprompt = cfg.get('prompt', '')\nmax_words = cfg.get('max_words', 200)\nreturn cls(prompt=prompt, max_words=max_words)"
                    },
                    {
                      "name": "pre_caption",
                      "decorators": [],
                      "body": "caption = re.sub('([.!\\\\\"()*#:;~])', ' ', caption.lower())\ncaption = re.sub('\\\\s{2,}', ' ', caption)\ncaption = caption.rstrip('\\n')\ncaption = caption.strip(' ')\ncaption_words = caption.split(' ')\nif len(caption_words) > self.max_words:\n    caption = ' '.join(caption_words[:self.max_words])\nreturn caption"
                    }
                  ]
                },
                {
                  "name": "Blip2ImageTrainProcessor",
                  "decorators": [
                    "registry.register_processor('blip2_image_train')"
                  ],
                  "inherits": [
                    "BlipImageBaseProcessor"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__(mean=mean, std=std)\nself.transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), self.normalize])"
                    },
                    {
                      "name": "__call__",
                      "decorators": [],
                      "body": "return self.transform(item)"
                    },
                    {
                      "name": "from_config",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "if cfg is None:\n    cfg = OmegaConf.create()\nimage_size = cfg.get('image_size', 224)\nmean = cfg.get('mean', None)\nstd = cfg.get('std', None)\nmin_scale = cfg.get('min_scale', 0.5)\nmax_scale = cfg.get('max_scale', 1.0)\nreturn cls(image_size=image_size, mean=mean, std=std, min_scale=min_scale, max_scale=max_scale)"
                    }
                  ]
                },
                {
                  "name": "Blip2ImageEvalProcessor",
                  "decorators": [
                    "registry.register_processor('blip2_image_eval')"
                  ],
                  "inherits": [
                    "BlipImageBaseProcessor"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "super().__init__(mean=mean, std=std)\nself.transform = transforms.Compose([transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC), transforms.ToTensor(), self.normalize])"
                    },
                    {
                      "name": "__call__",
                      "decorators": [],
                      "body": "return self.transform(item)"
                    },
                    {
                      "name": "from_config",
                      "decorators": [
                        "classmethod"
                      ],
                      "body": "if cfg is None:\n    cfg = OmegaConf.create()\nimage_size = cfg.get('image_size', 224)\nmean = cfg.get('mean', None)\nstd = cfg.get('std', None)\nreturn cls(image_size=image_size, mean=mean, std=std)"
                    }
                  ]
                }
              ],
              "other_functions": []
            },
            {
              "file_name": "ellama_codebase/minigpt4/processors/randaugment.py",
              "type": "python",
              "imports": [
                "import cv2",
                "import numpy as np",
                "import torch"
              ],
              "classes": [
                {
                  "name": "RandomAugment",
                  "decorators": [],
                  "inherits": [
                    "object"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.N = N\nself.M = M\nself.isPIL = isPIL\nif augs:\n    self.augs = augs\nelse:\n    self.augs = list(arg_dict.keys())"
                    },
                    {
                      "name": "get_random_ops",
                      "decorators": [],
                      "body": "sampled_ops = np.random.choice(self.augs, self.N)\nreturn [(op, 0.5, self.M) for op in sampled_ops]"
                    },
                    {
                      "name": "__call__",
                      "decorators": [],
                      "body": "if self.isPIL:\n    img = np.array(img)\nops = self.get_random_ops()\nfor (name, prob, level) in ops:\n    if np.random.random() > prob:\n        continue\n    args = arg_dict[name](level)\n    img = func_dict[name](img, *args)\nreturn img"
                    }
                  ]
                },
                {
                  "name": "VideoRandomAugment",
                  "decorators": [],
                  "inherits": [
                    "object"
                  ],
                  "methods": [
                    {
                      "name": "__init__",
                      "decorators": [],
                      "body": "self.N = N\nself.M = M\nself.p = p\nself.tensor_in_tensor_out = tensor_in_tensor_out\nif augs:\n    self.augs = augs\nelse:\n    self.augs = list(arg_dict.keys())"
                    },
                    {
                      "name": "get_random_ops",
                      "decorators": [],
                      "body": "sampled_ops = np.random.choice(self.augs, self.N, replace=False)\nreturn [(op, self.M) for op in sampled_ops]"
                    },
                    {
                      "name": "__call__",
                      "decorators": [],
                      "body": "assert frames.shape[-1] == 3, 'Expecting last dimension for 3-channels RGB (b, h, w, c).'\nif self.tensor_in_tensor_out:\n    frames = frames.numpy().astype(np.uint8)\nnum_frames = frames.shape[0]\nops = num_frames * [self.get_random_ops()]\napply_or_not = num_frames * [np.random.random(size=self.N) > self.p]\nframes = torch.stack(list(map(self._aug, frames, ops, apply_or_not)), dim=0).float()\nreturn frames"
                    },
                    {
                      "name": "_aug",
                      "decorators": [],
                      "body": "for (i, (name, level)) in enumerate(ops):\n    if not apply_or_not[i]:\n        continue\n    args = arg_dict[name](level)\n    img = func_dict[name](img, *args)\nreturn torch.from_numpy(img)"
                    }
                  ]
                }
              ],
              "other_functions": [
                {
                  "name": "identity_func",
                  "decorators": [],
                  "body": "return img"
                },
                {
                  "name": "autocontrast_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.ImageOps.autocontrast\\n    '\nn_bins = 256\n\ndef tune_channel(ch):\n    n = ch.size\n    cut = cutoff * n // 100\n    if cut == 0:\n        (high, low) = (ch.max(), ch.min())\n    else:\n        hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n        low = np.argwhere(np.cumsum(hist) > cut)\n        low = 0 if low.shape[0] == 0 else low[0]\n        high = np.argwhere(np.cumsum(hist[::-1]) > cut)\n        high = n_bins - 1 if high.shape[0] == 0 else n_bins - 1 - high[0]\n    if high <= low:\n        table = np.arange(n_bins)\n    else:\n        scale = (n_bins - 1) / (high - low)\n        offset = -low * scale\n        table = np.arange(n_bins) * scale + offset\n        table[table < 0] = 0\n        table[table > n_bins - 1] = n_bins - 1\n    table = table.clip(0, 255).astype(np.uint8)\n    return table[ch]\nchannels = [tune_channel(ch) for ch in cv2.split(img)]\nout = cv2.merge(channels)\nreturn out"
                },
                {
                  "name": "equalize_func",
                  "decorators": [],
                  "body": "\"\\n    same output as PIL.ImageOps.equalize\\n    PIL's implementation is different from cv2.equalize\\n    \"\nn_bins = 256\n\ndef tune_channel(ch):\n    hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n    non_zero_hist = hist[hist != 0].reshape(-1)\n    step = np.sum(non_zero_hist[:-1]) // (n_bins - 1)\n    if step == 0:\n        return ch\n    n = np.empty_like(hist)\n    n[0] = step // 2\n    n[1:] = hist[:-1]\n    table = (np.cumsum(n) // step).clip(0, 255).astype(np.uint8)\n    return table[ch]\nchannels = [tune_channel(ch) for ch in cv2.split(img)]\nout = cv2.merge(channels)\nreturn out"
                },
                {
                  "name": "rotate_func",
                  "decorators": [],
                  "body": "'\\n    like PIL, rotate by degree, not radians\\n    '\n(H, W) = (img.shape[0], img.shape[1])\ncenter = (W / 2, H / 2)\nM = cv2.getRotationMatrix2D(center, degree, 1)\nout = cv2.warpAffine(img, M, (W, H), borderValue=fill)\nreturn out"
                },
                {
                  "name": "solarize_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.ImageOps.posterize\\n    '\ntable = np.array([el if el < thresh else 255 - el for el in range(256)])\ntable = table.clip(0, 255).astype(np.uint8)\nout = table[img]\nreturn out"
                },
                {
                  "name": "color_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.ImageEnhance.Color\\n    '\nM = np.float32([[0.886, -0.114, -0.114], [-0.587, 0.413, -0.587], [-0.299, -0.299, 0.701]]) * factor + np.float32([[0.114], [0.587], [0.299]])\nout = np.matmul(img, M).clip(0, 255).astype(np.uint8)\nreturn out"
                },
                {
                  "name": "contrast_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.ImageEnhance.Contrast\\n    '\nmean = np.sum(np.mean(img, axis=(0, 1)) * np.array([0.114, 0.587, 0.299]))\ntable = np.array([(el - mean) * factor + mean for el in range(256)]).clip(0, 255).astype(np.uint8)\nout = table[img]\nreturn out"
                },
                {
                  "name": "brightness_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.ImageEnhance.Contrast\\n    '\ntable = (np.arange(256, dtype=np.float32) * factor).clip(0, 255).astype(np.uint8)\nout = table[img]\nreturn out"
                },
                {
                  "name": "sharpness_func",
                  "decorators": [],
                  "body": "'\\n    The differences the this result and PIL are all on the 4 boundaries, the center\\n    areas are same\\n    '\nkernel = np.ones((3, 3), dtype=np.float32)\nkernel[1][1] = 5\nkernel /= 13\ndegenerate = cv2.filter2D(img, -1, kernel)\nif factor == 0.0:\n    out = degenerate\nelif factor == 1.0:\n    out = img\nelse:\n    out = img.astype(np.float32)\n    degenerate = degenerate.astype(np.float32)[1:-1, 1:-1, :]\n    out[1:-1, 1:-1, :] = degenerate + factor * (out[1:-1, 1:-1, :] - degenerate)\n    out = out.astype(np.uint8)\nreturn out"
                },
                {
                  "name": "shear_x_func",
                  "decorators": [],
                  "body": "(H, W) = (img.shape[0], img.shape[1])\nM = np.float32([[1, factor, 0], [0, 1, 0]])\nout = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\nreturn out"
                },
                {
                  "name": "translate_x_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.Image.transform\\n    '\n(H, W) = (img.shape[0], img.shape[1])\nM = np.float32([[1, 0, -offset], [0, 1, 0]])\nout = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\nreturn out"
                },
                {
                  "name": "translate_y_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.Image.transform\\n    '\n(H, W) = (img.shape[0], img.shape[1])\nM = np.float32([[1, 0, 0], [0, 1, -offset]])\nout = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\nreturn out"
                },
                {
                  "name": "posterize_func",
                  "decorators": [],
                  "body": "'\\n    same output as PIL.ImageOps.posterize\\n    '\nout = np.bitwise_and(img, np.uint8(255 << 8 - bits))\nreturn out"
                },
                {
                  "name": "shear_y_func",
                  "decorators": [],
                  "body": "(H, W) = (img.shape[0], img.shape[1])\nM = np.float32([[1, 0, 0], [factor, 1, 0]])\nout = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\nreturn out"
                },
                {
                  "name": "cutout_func",
                  "decorators": [],
                  "body": "replace = np.array(replace, dtype=np.uint8)\n(H, W) = (img.shape[0], img.shape[1])\n(rh, rw) = np.random.random(2)\npad_size = pad_size // 2\n(ch, cw) = (int(rh * H), int(rw * W))\n(x1, x2) = (max(ch - pad_size, 0), min(ch + pad_size, H))\n(y1, y2) = (max(cw - pad_size, 0), min(cw + pad_size, W))\nout = img.copy()\nout[x1:x2, y1:y2, :] = replace\nreturn out"
                },
                {
                  "name": "enhance_level_to_args",
                  "decorators": [],
                  "body": "def level_to_args(level):\n    return (level / MAX_LEVEL * 1.8 + 0.1,)\nreturn level_to_args"
                },
                {
                  "name": "shear_level_to_args",
                  "decorators": [],
                  "body": "def level_to_args(level):\n    level = level / MAX_LEVEL * 0.3\n    if np.random.random() > 0.5:\n        level = -level\n    return (level, replace_value)\nreturn level_to_args"
                },
                {
                  "name": "translate_level_to_args",
                  "decorators": [],
                  "body": "def level_to_args(level):\n    level = level / MAX_LEVEL * float(translate_const)\n    if np.random.random() > 0.5:\n        level = -level\n    return (level, replace_value)\nreturn level_to_args"
                },
                {
                  "name": "cutout_level_to_args",
                  "decorators": [],
                  "body": "def level_to_args(level):\n    level = int(level / MAX_LEVEL * cutout_const)\n    return (level, replace_value)\nreturn level_to_args"
                },
                {
                  "name": "solarize_level_to_args",
                  "decorators": [],
                  "body": "def level_to_args(level):\n    level = int(level / MAX_LEVEL * 256)\n    return (level,)\nreturn level_to_args"
                },
                {
                  "name": "none_level_to_args",
                  "decorators": [],
                  "body": "return ()"
                },
                {
                  "name": "posterize_level_to_args",
                  "decorators": [],
                  "body": "def level_to_args(level):\n    level = int(level / MAX_LEVEL * 4)\n    return (level,)\nreturn level_to_args"
                },
                {
                  "name": "rotate_level_to_args",
                  "decorators": [],
                  "body": "def level_to_args(level):\n    level = level / MAX_LEVEL * 30\n    if np.random.random() < 0.5:\n        level = -level\n    return (level, replace_value)\nreturn level_to_args"
                }
              ]
            }
          ],
          "subdirs": []
        }
      ]
    }
  ]
}